{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Text_Normalization_Tutorial.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0DJqotopcyb"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "BRANCH = 'text_normalization'\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH7yR7cSwPKr"
      },
      "source": [
        "import json\n",
        "import os\n",
        "import wget\n",
        "import numpy as np\n",
        "import inspect\n",
        "import regex as re\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXRARM8XtK_g"
      },
      "source": [
        "# Introduction\n",
        "Text normalization for TTS converts text into its verbalized form. That is, tokens belonging to special semiotic classes, e.g. numbers/abbreviations, will be converted into their spoken form. For example, \"10:00\" -> \"ten o'clock\", \"10:00 a.m.\" -> \"ten a m\", \"10kg\" -> \"ten kilograms\".\n",
        "\n",
        "This tutorial shows how to use the NeMo rule-based text normalization system.\n",
        "Similar to [The Google Kestrel TTS text normalization\n",
        "system](https://www.researchgate.net/profile/Richard_Sproat/publication/277932107_The_Kestrel_TTS_text_normalization_system/links/57308b1108aeaae23f5cc8c4/The-Kestrel-TTS-text-normalization-system.pdf), the NeMo rule-based system is devided into a tagger and a verbalizer: the tagger is responsible for detecting and classifying semiotic classes in the underlying text, the verbalizer takes the output of the tagger and carries out the normalization. The system is designed to be easily debuggable and extendable by more rules.\n",
        "We provided the a set of rules that covers the majority of cases as found in the [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish) for the English language. As with every language there is a long tail of special cases.\n",
        "\n",
        "This tutorial will show how to do prediction on regular text data. It also shows how to do evaluation on a labeled text normalization dataset that follows the format of [Google Text normalization dataset](https://www.kaggle.com/richardwilliamsproat/text-normalization-for-english-russian-and-polish)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UD-OuFmEOX3T"
      },
      "source": [
        "# If you're running the notebook locally, update the TOOLS_DIR path below\n",
        "# In Colab, a few required scripts will be downloaded from NeMo github\n",
        "\n",
        "TOOLS_DIR = '<UPDATE_PATH_TO_NeMo_root>/tools/text_normalization/'\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    TOOLS_DIR = 'tools/text_normalization/'\n",
        "    TOOLS_DATA_DIR = TOOLS_DIR + \"data/\"\n",
        "    os.makedirs(TOOLS_DIR, exist_ok=True)\n",
        "    os.makedirs(TOOLS_DATA_DIR, exist_ok=True)\n",
        "\n",
        "    required_files = [\n",
        "      'normalize.py',\n",
        "      'tagger.py',\n",
        "      'utils.py',\n",
        "      'run_evaluate.py',\n",
        "      'run_predict.py',\n",
        "      'verbalizer.py',\n",
        "    ]\n",
        "    required_data_file = [             \n",
        "      'currency.tsv',\n",
        "      'magnitudes.tsv',\n",
        "      'measurements.tsv',\n",
        "      'months.tsv',\n",
        "      'whitelist.tsv'\n",
        "    ]\n",
        "    for file in required_files:\n",
        "        if not os.path.exists(os.path.join(TOOLS_DIR, file)):\n",
        "            file_path = f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/' + TOOLS_DIR + file\n",
        "            print(file_path)\n",
        "            wget.download(file_path, TOOLS_DIR)\n",
        "    for file in required_data_file:\n",
        "        if not os.path.exists(os.path.join(TOOLS_DATA_DIR, file)):\n",
        "            file_path = f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/' + TOOLS_DATA_DIR + file\n",
        "            print(file_path)\n",
        "            wget.download(file_path, TOOLS_DATA_DIR)\n",
        "elif not os.path.exists(TOOLS_DIR):\n",
        "      raise ValueError(f'update path to NeMo root directory')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1DZk-inQGTI"
      },
      "source": [
        "`TOOLS_DIR` should now contain scripts that we are going to need in the next steps, all necessary scripts could be found [here](https://github.com/NVIDIA/NeMo/tree/main/tools/text_normalization)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C9DdMfvRFM-"
      },
      "source": [
        "print(TOOLS_DIR)\n",
        "! ls -l $TOOLS_DIR\n",
        "! ls -l $TOOLS_DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUEncnqTIzF6"
      },
      "source": [
        "# Data Preparation and Download\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDVtdJxhZWju"
      },
      "source": [
        "## Data for Prediction\r\n",
        "For prediction, let's download a text file from [http://www.gutenberg.org/files/48874/48874-0.txt](http://www.gutenberg.org/files/48874/48874-0.txt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkeKX2I_tIgV"
      },
      "source": [
        "## create data directory and download an audio file\n",
        "WORK_DIR = 'WORK_DIR'\n",
        "DATA_DIR = WORK_DIR + '/DATA'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "text_file = '48874-0.txt'\n",
        "if not os.path.exists(os.path.join(DATA_DIR, text_file)):\n",
        "    print('Downloading text file')\n",
        "    wget.download('http://www.gutenberg.org/files/48874/' + text_file, DATA_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyUE_t4vw2et"
      },
      "source": [
        "The `DATA_DIR` should now contain the text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXrTzTyIpzE8"
      },
      "source": [
        "!ls -l $DATA_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWqlbSryw_WL"
      },
      "source": [
        "the first 10 lines of the file :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vC2DHawIGt8"
      },
      "source": [
        "! head -n 10 $DATA_DIR/$text_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMT5lkPYzZHK"
      },
      "source": [
        "## Data for Evaluation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4zjeVVv-UXR"
      },
      "source": [
        "#TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmDTCuTLH7pm"
      },
      "source": [
        "# Prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6R7OKAsYH9p0"
      },
      "source": [
        "# TODO ! ls -l $OUTPUT_DIR/processed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIvKBwRcH_9W"
      },
      "source": [
        "# Evaluation\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GLpMgoICmk"
      },
      "source": [
        "# tODO\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcvT3P2lQ_GS"
      },
      "source": [
        "# Next Steps\n",
        "\n",
        "Check out [NeMo Speech Data Explorer tool](https://github.com/NVIDIA/NeMo/tree/main/tools/speech_data_explorer#speech-data-explorer) to interactively evaluate the aligned segments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYylwvTX2VSF"
      },
      "source": [
        "# References\n",
        "KÃ¼rzinger, Ludwig, et al. [\"CTC-Segmentation of Large Corpora for German End-to-End Speech Recognition.\"](https://arxiv.org/abs/2007.09127) International Conference on Speech and Computer. Springer, Cham, 2020."
      ]
    }
  ]
}