{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Plot of Noam Annealing\n",
    "###\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "d_model = 256\n",
    "warmup = 4000\n",
    "max_steps = 20000\n",
    "lr = 1\n",
    "min_lr = 1e-5\n",
    "\n",
    "def _noam_annealing(initial_lr, step):\n",
    "    normalize = d_model ** (-0.5)\n",
    "    warmup_steps = warmup\n",
    "    mult = normalize * min(step ** (-0.5), step * (warmup_steps ** (-1.5)))\n",
    "    out_lr = initial_lr * mult\n",
    "    if step > warmup_steps:\n",
    "        out_lr = max(out_lr, min_lr)\n",
    "    return out_lr\n",
    "\n",
    "x = range(1, max_steps)\n",
    "y = []\n",
    "for i in x:\n",
    "    y.append(_noam_annealing(lr, i))\n",
    "    if i % 1000 == 0:\n",
    "        print(_noam_annealing(lr, i))\n",
    "plt.plot(x, y)\n",
    "plt.show()\n",
    "\n",
    "class NoamAnnealing(object):\n",
    "    def __init__(\n",
    "        self, d_model, max_steps, warmup_steps=None, min_lr=0.0, last_epoch=-1\n",
    "    ):\n",
    "        self._normalize = d_model ** (-0.5)\n",
    "\n",
    "        self.max_steps = max_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def get_lr(self, step, lr):\n",
    "        if step > self.max_steps:\n",
    "            return [self.min_lr for _ in self.base_lrs]\n",
    "\n",
    "        new_lrs = self._noam_annealing(initial_lr=lr, step=step)\n",
    "        return new_lrs\n",
    "\n",
    "    def _noam_annealing(self, initial_lr, step):\n",
    "        mult = self._normalize * min(step ** (-0.5), step * (self.warmup_steps ** (-1.5)))\n",
    "        out_lr = initial_lr * mult\n",
    "        if step > self.warmup_steps:\n",
    "            out_lr = max(out_lr, self.min_lr)\n",
    "        return out_lr\n",
    "\n",
    "d_model=256\n",
    "max_steps=20000\n",
    "warmup_steps=4000\n",
    "min_lr=1e-5\n",
    "lr = 1\n",
    "\n",
    "myanneal = NoamAnnealing(d_model=d_model, max_steps=max_steps, warmup_steps=warmup_steps, min_lr=min_lr)\n",
    "x = range(1, max_steps)\n",
    "y = []\n",
    "for i in x:\n",
    "    y.append(myanneal.get_lr(i, lr))\n",
    "    if i % 1000 == 0:\n",
    "        print(myanneal.get_lr(i, lr))\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# F0 Checks\n",
    "###\n",
    "\n",
    "from pathlib import Path\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import json\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "audio = []\n",
    "all_data = []\n",
    "total_duration = 0\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    np.random.shuffle(lines)\n",
    "    for lin in lines:\n",
    "        data = json.loads(lin)\n",
    "        total_duration += data['duration']\n",
    "        if total_duration > 45*60:\n",
    "            break\n",
    "        audio.append(data['audio_filepath'])\n",
    "        all_data.append(data)\n",
    "print(f\"Total duration: {total_duration}\")\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_45mins.json\", \"w\") as f:\n",
    "    for i, line in enumerate(all_data):\n",
    "        f.write(json.dumps(line)+'\\n')\n",
    "\n",
    "# audio = []\n",
    "# total_duration = 0\n",
    "# with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_15mins.json\", \"r\") as f:\n",
    "#     lines = f.readlines()\n",
    "#     for lin in lines:\n",
    "#         data = json.loads(lin)\n",
    "#         audio.append(data['audio_filepath'])\n",
    "    \n",
    "\n",
    "# pitch_fmin = 64 # REALLY Should change this\n",
    "# pitch_fmax = 512 # Should change this\n",
    "# n_window_size=2048  # Don't have to change this\n",
    "# # all_f0 = np.array([])\n",
    "# for i, f in enumerate(audio):\n",
    "#     y, _ = librosa.core.load(f, sr=44100)\n",
    "#     stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=2048, window=\"hann\")))\n",
    "#     f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=pitch_fmin, fmax=pitch_fmax, frame_length=n_window_size, sr=44100, fill_na=0.)\n",
    "    \n",
    "#     ## UNCOMMENT THIS SECTION TO EXPERIMENT WITH FMIN, FMAX\n",
    "# #     fig, ax = plt.subplots()\n",
    "# #     librosa.display.specshow(stft_matrix, sr=44100, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "# #     ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "#     # Hack to remove off_pitches\n",
    "#     found_flat_area = False\n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     for i, pitch in enumerate(f0):\n",
    "#         if pitch==0.:\n",
    "#             if found_flat_area:\n",
    "#                 end = i\n",
    "#                 # Decide whether to trim this\n",
    "#                 voice_prob_avg = np.mean(voiced_probs[start:end])\n",
    "#                 if voice_prob_avg < 0.03:\n",
    "#                     # Delete pitch\n",
    "#                     f0[start:end] = 0.\n",
    "#                 else:\n",
    "#                     # Probably started real audio\n",
    "#                     break\n",
    "#                 found_flat_area = False\n",
    "#                 start = 0\n",
    "#             continue\n",
    "#         elif not found_flat_area:\n",
    "#             found_flat_area = True\n",
    "#             start = i\n",
    "#             end = 0\n",
    "#     found_flat_area = False\n",
    "#     start = 0\n",
    "#     end = 0\n",
    "#     for i, pitch in enumerate(reversed(f0)):\n",
    "#         if pitch==0.:\n",
    "#             if found_flat_area:\n",
    "#                 end = i\n",
    "#                 # Decide whether to trim this\n",
    "#                 voice_prob_avg = np.mean(voiced_probs[(len(f0)-end):(len(f0)-start)])\n",
    "#                 if voice_prob_avg < 0.03:\n",
    "#                     # Delete pitch\n",
    "#                     f0[(len(f0)-end):(len(f0)-start)] = 0.\n",
    "#                 else:\n",
    "#                     # Probably started real audio\n",
    "#                     break\n",
    "#                 found_flat_area = False\n",
    "#                 start = 0\n",
    "#             continue\n",
    "#         elif not found_flat_area:\n",
    "#             found_flat_area = True\n",
    "#             start = i\n",
    "#             end = 0\n",
    "\n",
    "# ## UNCOMMENT THIS SECTION TO EXPERIMENT WITH FMIN, FMAX\n",
    "#     all_f0 = np.concatenate((all_f0,f0[f0!=0].flatten()))\n",
    "#     ax.plot(f0, label='f0', color='cyan', linewidth=2)\n",
    "#     ax.plot(voiced_probs*500, label='f0', linewidth=3)\n",
    "# #     plt.show()\n",
    "\n",
    "# # ### UNCOMMENT THIS SECTION WHEN YOU FINALIZE FMIN, FMAX\n",
    "# #     filename = f\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/pitch_prior/{Path(f).stem}_pitch_pyin_fmin{pitch_fmin}_fmax{pitch_fmax}_fl{n_window_size}.npy\"\n",
    "# #     np.save(filename, f0)\n",
    "\n",
    "# ## WE STILL NEED TO FIND MEAN AND STD FOR EACH 15min, 30min, etc. manifest\n",
    "# print(f\"pitch mean: {np.mean(all_f0)}\")\n",
    "# print(f\"pitch std: {np.std(all_f0)}\")\n",
    "    \n",
    "# # import torch\n",
    "\n",
    "# # y, _ = librosa.core.load(\"/data/speech/LJSpeech/wavs/LJ030-0200.wav\", sr=22050)\n",
    "# # stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, window=\"hann\", hop_length=256)))\n",
    "# # %matplotlib inline\n",
    "# # fig, ax = plt.subplots()\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\", ax=ax)\n",
    "# # f0 = torch.load(\"/data/speech/LJSpeech/supplementary/LJ030-0200_melodia_f0min80_f0max800_harm1.0_mps0.0.pt\")[\"f0\"]\n",
    "# # f0 = f0.numpy().squeeze()\n",
    "# # f0[f0==0] = np.nan\n",
    "# # ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "# # import pysptk\n",
    "\n",
    "# # y, _ = librosa.core.load(\"/data/speech/LJSpeech/wavs/LJ030-0200.wav\", sr=22050)\n",
    "# # stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=1024, window=\"hann\", hop_length=256)))\n",
    "# # %matplotlib inline\n",
    "# # fig, ax = plt.subplots()\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\", ax=ax)\n",
    "# # f0 = pysptk.rapt(y.astype(np.float32) * 32768, fs=22050, hopsize=256, otype=\"f0\")\n",
    "# # f0[f0==0] = np.nan\n",
    "# # ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "# # librosa.display.specshow(stft_matrix, sr=22050, hop_length=256, y_axis=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# GL\n",
    "###\n",
    "\n",
    "import librosa\n",
    "def griffin_lim(magnitudes, n_iters=50, n_fft=1024):\n",
    "    \"\"\"\n",
    "    Griffin-Lim algorithm to convert magnitude spectrograms to audio signals\n",
    "    \"\"\"\n",
    "    phase = np.exp(2j * np.pi * np.random.rand(*magnitudes.shape))\n",
    "    complex_spec = magnitudes * phase\n",
    "    signal = librosa.istft(complex_spec)\n",
    "    if not np.isfinite(signal).all():\n",
    "        logging.warning(\"audio was not finite, skipping audio saving\")\n",
    "        return np.array([0])\n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        _, phase = librosa.magphase(librosa.stft(signal, n_fft=n_fft))\n",
    "        complex_spec = magnitudes * phase\n",
    "        signal = librosa.istft(complex_spec)\n",
    "    return signal\n",
    "\n",
    "linear = spec.cpu().numpy().squeeze(0)\n",
    "linear = np.dot(librosa.filters.mel(44100, 2048, n_mels=80, fmin=0.0, fmax=None).T, linear)\n",
    "linear = np.clip(linear, a_min=0, a_max=255)\n",
    "audio = griffin_lim(linear**1.5, n_fft=2048)\n",
    "\n",
    "ipd.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference (T2+HiFiGAN)\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import _AudioTextDataset\n",
    "from nemo.collections.tts.models import Tacotron2Model, FastPitchModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "\n",
    "# t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/examples/tts/Tacotron2.nemo\")\n",
    "# t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/Tacotron2-Helen-char.nemo\")\n",
    "t2 = Tacotron2Model.restore_from(\"/home/jasoli/nemo/NeMo/Tacotron2-large-9017.nemo\")\n",
    "t2 = t2.cuda().eval()\n",
    "t2.decoder.gate_threshold = 0.4\n",
    "labels = t2.cfg.labels\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500.pt\")[\"generator\"]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval().half()\n",
    "\n",
    "dataset = _AudioTextDataset(\n",
    "     \"/data/speech/HiFiTTS/9017_manifest_clean_dev.json\",\n",
    "#     \"/data/speech/Helen/helen_val.json\",\n",
    "#     \"/mnt/ssd1/data/LJSpeech-1.1/nvidia_ljspeech_test.json\",\n",
    "    parser=t2.parser,\n",
    "    sample_rate=44100,\n",
    "    bos_id=len(labels),\n",
    "    eos_id=len(labels) + 1,\n",
    "    pad_id=len(labels) + 2,\n",
    "    max_utts=5,\n",
    "    min_duration=1.,\n",
    "    max_duration=16.,\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset, batch_size=1, collate_fn=dataset.collate_fn, num_workers=0, shuffle=False\n",
    ")\n",
    "device = torch.device(\"cuda\")\n",
    "all_utterances = 0\n",
    "all_samples = 0\n",
    "\n",
    "batches = []\n",
    "for b in dataloader:\n",
    "    batches.append(b)\n",
    "\n",
    "def infer_vocoder(model, spec: torch.Tensor):\n",
    "    audio = model(x=spec).squeeze(1)\n",
    "    return audio\n",
    "\n",
    "audios = []\n",
    "gt_audios = []\n",
    "for batch in batches:\n",
    "    gt_audio, _, text, text_length = batch\n",
    "    gt_audios.append(gt_audio)\n",
    "    text = text.to(device)\n",
    "    text_length = text_length.to(device)\n",
    "    with torch.no_grad():\n",
    "        spectrogram = t2.generate_spectrogram(tokens=text)\n",
    "        audios.append(infer_vocoder(model=vocoder, spec=spectrogram.half()))\n",
    "\n",
    "import IPython.display as ipd\n",
    "for i, (aud, gt_aud) in enumerate(zip(audios, gt_audios)):\n",
    "    print(f\"Sample_{i}\")\n",
    "    ipd.display(ipd.Audio(aud.float().cpu().numpy(), rate=44100))\n",
    "    ipd.display(ipd.Audio(gt_aud.float().cpu().numpy(), rate=44100))\n",
    "    write(f\"sample_{i}.wav\", 44100, aud.float().cpu().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Splicing rap mp3  - did not work\n",
    "###\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "y, sr = librosa.core.load(\"./3899_Eminem - The Real Slim Shady (Acapella).mp3\", sr=None)\n",
    "\n",
    "start = int((60+21.7)*sr)\n",
    "# end = int((60+30.58)*sr)  # end of please stand up x3\n",
    "end = int((60+28.2)*sr)  # end of first please stand up \n",
    "spliced = y[start:end]\n",
    "ipd.display(ipd.Audio(spliced, rate=sr))\n",
    "\n",
    "stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(spliced, n_fft=2048, window=\"hann\")))\n",
    "f0, voiced_flag, voiced_probs = librosa.pyin(spliced, fmin=120, fmax=512, frame_length=2048, sr=sr)\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "ax.plot(f0, label='f0', color='cyan', linewidth=3)\n",
    "\n",
    "sf.write('chorus.wav', spliced, sr)\n",
    "\n",
    "f0_mean = np.mean(f0[~np.isnan(f0)])\n",
    "f0_std = np.std(f0[~np.isnan(f0)])\n",
    "\n",
    "f0_stand = (f0 - f0_mean)/f0_std\n",
    "f0_stand[np.isnan(f0_stand)] = 0.\n",
    "\n",
    "# Load fastpitch model\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "import torch\n",
    "from nemo.collections.tts.torch.helpers import beta_binomial_prior_distribution\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "fastpitch = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FastPitch-Align-8051.nemo\").eval()\n",
    "\n",
    "# inp = (\"I'm Slim Shady, yes, I'm the real Shady. \"\n",
    "#        \"All you other Slim Shadys are just imitating. \"\n",
    "#        \"So won't the real Slim Shady please stand up?\")\n",
    "# tokens = fastpitch.parse(inp)\n",
    "\n",
    "# mels, spec_len = fastpitch.preprocessor(input_signal=torch.tensor(spliced).unsqueeze(0).cuda(), length=torch.tensor([len(spliced)]).cuda())\n",
    "\n",
    "# mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=None,\n",
    "#     pitch=None,\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=mels,\n",
    "#     attn_prior=torch.tensor(beta_binomial_prior_distribution(tokens.shape[1],mels.shape[2])).unsqueeze(0).cuda(),\n",
    "#     mel_lens=spec_len,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "# spec, *_ = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=new_dur.cuda(),\n",
    "#     pitch=torch.tensor(f0_stand).cuda().unsqueeze(0),\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=None,\n",
    "#     attn_prior=None,\n",
    "#     mel_lens=None,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "# spec_2, *_ = fastpitch(\n",
    "#     text=tokens,\n",
    "#     durs=attn_hard_dur,\n",
    "#     pitch=None,\n",
    "#     speaker=None,\n",
    "#     pace=1.0,\n",
    "#     spec=None,\n",
    "#     attn_prior=None,\n",
    "#     mel_lens=None,\n",
    "#     input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    "# )\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/hifigan_gen_nemo_8051_00524000.pt\")\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval()\n",
    "\n",
    "\n",
    "audio = vocoder(x=spec)\n",
    "audio2 = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))\n",
    "ipd.display(ipd.Audio(audio2.cpu().detach().numpy()[0], rate=44100))\n",
    "\n",
    "new_dur = torch.zeros(attn_hard_dur.shape)\n",
    "\n",
    "for i in range(attn_hard_dur.shape[1]):\n",
    "    if tokens[0,i] == 0:\n",
    "        new_dur[0,i] = int(attn_hard_dur[0,i] / 1.3)\n",
    "    else:\n",
    "        new_dur[0,i] = int(attn_hard_dur[0,i] / 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Fastpitch female 2 male attempt 1 - did not work\n",
    "###\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "import torch\n",
    "from nemo.collections.tts.torch.helpers import beta_binomial_prior_distribution\n",
    "from nemo.utils import logging\n",
    "logging.setLevel(logging.ERROR)\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "\n",
    "fastpitch = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FastPitch-Align-8051.nemo\").eval().cuda()\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "vocoder = Generator(\n",
    "    resblock=1,\n",
    "    upsample_rates=[8, 8, 4, 2],\n",
    "    upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "    upsample_initial_channel=512,\n",
    "    resblock_kernel_sizes=[3, 7, 11],\n",
    "    resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    ")\n",
    "nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/hifigan_gen_nemo_8051_00524000.pt\")\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "vocoder.load_state_dict(new_nemo_ckpt)\n",
    "vocoder = vocoder.cuda().eval()\n",
    "\n",
    "inp = (\"Who do I sound like?\")\n",
    "tokens = fastpitch.parse(inp)\n",
    "\n",
    "mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "    text=tokens,\n",
    "    durs=None,\n",
    "    pitch=None,\n",
    "    speaker=None,\n",
    "    pace=1.0,\n",
    "    spec=None,\n",
    "    attn_prior=None,\n",
    "    mel_lens=None,\n",
    "    input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0),\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels_pred.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "audio = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))\n",
    "\n",
    "print(durs_predicted)\n",
    "mels_pred, _, durs_predicted, log_durs_pred, pitch_pred, attn_soft, attn_logprob, attn_hard, attn_hard_dur, pitch = fastpitch(\n",
    "    text=tokens,\n",
    "    durs=None,\n",
    "    pitch=None,\n",
    "    speaker=None,\n",
    "    pace=0.4,\n",
    "    spec=None,\n",
    "    attn_prior=None,\n",
    "    mel_lens=None,\n",
    "    input_lens=torch.tensor(tokens.shape[1]).unsqueeze(0).cuda(),\n",
    "    pitch_transform=-50,\n",
    ")\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "ax.imshow(mels_pred.cpu().detach().numpy()[0], origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "audio = vocoder(x=mels_pred)\n",
    "\n",
    "ipd.display(ipd.Audio(audio.cpu().detach().numpy()[0], rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# HiFigan cycle stationary consistency\n",
    "###\n",
    "\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "\n",
    "y, sr = librosa.core.load(\"/home/jasoli/example.wav\", sr=None)\n",
    "stft_matrix = librosa.amplitude_to_db(np.abs(librosa.stft(y, n_fft=2048, window=\"hann\")))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=[15,8])\n",
    "librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "ipd.display(ipd.Audio(y, rate=44100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# TorchScript export\n",
    "###\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "fp = FastPitchModel.from_pretrained(\"tts_en_fastpitch\").cpu()\n",
    "# fp.export(\"fastpitch_ngc_ljs_pitch.ts\")\n",
    "# hf = HifiGanModel.from_pretrained(\"tts_hifigan\").cpu()\n",
    "# hf.export(\"tts_hifigan.ts\")\n",
    "\n",
    "# import torch\n",
    "\n",
    "# fp_ts = torch.jit.load('fastpitch_ngc_ljs_pitch.ts')\n",
    "\n",
    "# tokens = fp.parse(\"Hello.\")\n",
    "# # spec = fp_ts(tokens)\n",
    "# print(fp._parser._labels_map)\n",
    "# with open(\"fastpitch_ngc_ljs_mappings.txt\", \"w\") as f:\n",
    "#     for key, value in fp._parser._labels_map.items():\n",
    "#         f.write(f\"{value} {key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# ADLR checkpoint to .NeMo (only generator)\n",
    "###\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "hf_conf = {}\n",
    "hf_conf[\"generator\"] = {\n",
    "    \"_target_\":\"nemo.collections.tts.modules.hifigan_modules.Generator\",\n",
    "    \"resblock\":1,\n",
    "    \"upsample_rates\":[8, 8, 4, 2],\n",
    "    \"upsample_kernel_sizes\":[16, 16, 4, 4],\n",
    "    \"upsample_initial_channel\":512,\n",
    "    \"resblock_kernel_sizes\":[3, 7, 11],\n",
    "    \"resblock_dilation_sizes\":[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "}\n",
    "hf = HifiGanModel(hf_conf)\n",
    "\n",
    "# vocoder = Generator(\n",
    "#     resblock=1,\n",
    "#     upsample_rates=[8, 8, 4, 2],\n",
    "#     upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "#     upsample_initial_channel=512,\n",
    "#     resblock_kernel_sizes=[3, 7, 11],\n",
    "#     resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "# )\n",
    "nemo_gen_keys = [k for k in hf.state_dict().keys()]\n",
    "adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "hf.load_state_dict(new_nemo_ckpt)\n",
    "hf.save_to(\"Hifigan_ADLR_Helen.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# ADLR checkpoint to .NeMo (generator+disc)\n",
    "###\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "\n",
    "adlr_gen_ckpt = torch.load(\n",
    "    \"/home/jasoli/nemo/NeMo/g_00587500\"\n",
    ")\n",
    "adlr_dis_ckpt = torch.load(\n",
    "    \"/home/jasoli/nemo/NeMo/do_00587500\"\n",
    ")\n",
    "save_path = \"/home/jasoli/nemo/NeMo/checkpoints/hifigan_from_adlr_in_nemo_helen_00587500.nemo\"\n",
    "model_config = OmegaConf.load('/home/jasoli/nemo/NeMo/examples/tts/my-hifigan-44k.yaml')\n",
    "\n",
    "del model_config[\"model\"][\"train_ds\"]\n",
    "del model_config[\"model\"][\"validation_ds\"]\n",
    "del model_config[\"exp_manager\"]\n",
    "\n",
    "model = HifiGanModel(cfg=model_config.model)\n",
    "nemo_gen_keys = [k for k in model.state_dict().keys() if \"generator\" in k]\n",
    "adlr_gen_keys = adlr_gen_ckpt[\"generator\"].keys()\n",
    "\n",
    "new_nemo_ckpt = {\n",
    "    nemo_key: adlr_gen_ckpt[\"generator\"][adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)\n",
    "}\n",
    "\n",
    "for k in model.state_dict().keys():\n",
    "    if \"msd.discriminators\" in k:\n",
    "        new_nemo_ckpt[k] = adlr_dis_ckpt[\"msd\"][k.replace(\"msd.\", \"\")]\n",
    "    elif \"mpd.discriminators\" in k:\n",
    "        new_nemo_ckpt[k] = adlr_dis_ckpt[\"mpd\"][k.replace(\"mpd.\", \"\")]\n",
    "\n",
    "model.load_state_dict(OrderedDict(new_nemo_ckpt), strict=False)\n",
    "model.save_to(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference with FastPitch over dataset to prepare for hifigan finetuning\n",
    "###\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "from operator import ne\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import AudioToCharWithPriorAndPitchDataset\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "\n",
    "def generate_samples():\n",
    "    prior_folder = \"/data/speech/Helen/FasPitch_align/\"\n",
    "    nemo_checkpoint = \"/home/jasoli/nemo/NeMo/checkpoints/FastPitch-Align-Helen.nemo\"\n",
    "    manifest_filepath = \"/data/speech/Helen/helen_train.json\"\n",
    "#     manifest_filepath = \"/data/speech/Helen/helen_val.json\"\n",
    "    syn_mel_folder = Path(\"/data/speech/Helen/FastPitch_align_syn_mel\")\n",
    "    \n",
    "    model = FastPitchModel.restore_from(nemo_checkpoint, map_location=\"cpu\").cuda(1).eval()\n",
    "    vocab_dict = {\n",
    "        \"notation\": \"phonemes\",\n",
    "        \"punct\": True,\n",
    "        \"spaces\": True,\n",
    "        \"stresses\": True,\n",
    "        \"add_blank_at\": None,\n",
    "        \"pad_with_space\": True,\n",
    "        \"chars\": True,\n",
    "        \"improved_version_g2p\": True,\n",
    "    }\n",
    "    dataset = AudioToCharWithPriorAndPitchDataset(\n",
    "        sup_data_path=prior_folder,\n",
    "        n_window_stride=512,\n",
    "        n_window_size=2048,\n",
    "        manifest_filepath=manifest_filepath,\n",
    "        sample_rate=44100,\n",
    "        pitch_fmin=model._cfg.pitch_fmin,\n",
    "        pitch_fmax=model._cfg.pitch_fmax,\n",
    "        pitch_avg=model._cfg.pitch_avg,\n",
    "        pitch_std=model._cfg.pitch_std,\n",
    "        vocab=vocab_dict,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset, batch_size=32, collate_fn=dataset.collate_fn, num_workers=8, shuffle=False\n",
    "    )\n",
    "\n",
    "    if not syn_mel_folder.exists():\n",
    "        syn_mel_folder.mkdir()\n",
    "\n",
    "    i = 0\n",
    "    for batch in dataloader:\n",
    "        audio, audio_lens, text, text_lens, attn_prior, _, _ = batch\n",
    "        with torch.no_grad():\n",
    "            mels, spec_len = model.preprocessor(input_signal=audio.cuda(1), length=audio_lens.cuda(1))\n",
    "            mels_pred, _, _, _, _, _, _, _, attn_hard_dur, _ = model(\n",
    "                text=text.cuda(1),\n",
    "                durs=None,\n",
    "                pitch=None,\n",
    "                speaker=None,\n",
    "                pace=1.0,\n",
    "                spec=mels,\n",
    "                attn_prior=attn_prior.cuda(1),\n",
    "                mel_lens=spec_len,\n",
    "                input_lens=text_lens.cuda(1),\n",
    "            )\n",
    "        mels = mels_pred.cpu().numpy()\n",
    "        for j, mel in enumerate(mels):\n",
    "            mel_length = int(torch.sum(attn_hard_dur[j]))\n",
    "            second_half = \"train_{}\" if \"train\" in manifest_filepath else \"val_{}\"\n",
    "            save_path = syn_mel_folder / second_half.format(i)\n",
    "            np.save(save_path, mel[:, :mel_length])\n",
    "            if i < 10:\n",
    "                print(save_path)\n",
    "            i += 1\n",
    "\n",
    "\n",
    "def make_new_json():\n",
    "    manifest_filepath = \"/data/speech/Helen/helen_train.json\"\n",
    "    new_json_filepath = \"/data/speech/Helen/helen_train_synmel.json\"\n",
    "#     manifest_filepath = \"/data/speech/Helen/helen_val.json\"\n",
    "#     new_json_filepath = \"/data/speech/Helen/helen_val_synmel.json\"\n",
    "    syn_mel_folder = Path(\"/data/speech/Helen/FastPitch_align_syn_mel\")\n",
    "\n",
    "    with open(manifest_filepath, \"r\") as data_in:\n",
    "        lines = data_in.readlines()\n",
    "\n",
    "    with open(new_json_filepath, \"w\") as data_out:\n",
    "        for i, line in enumerate(lines):\n",
    "            in_line = json.loads(line)\n",
    "            second_half = \"train_{}.npy\" if \"train\" in manifest_filepath else \"val_{}.npy\"\n",
    "            save_path = syn_mel_folder / second_half.format(i)\n",
    "            in_line[\"mel_filepath\"] = str(save_path)\n",
    "            data_out.write(json.dumps(in_line) + '\\n')\n",
    "\n",
    "generate_samples()\n",
    "make_new_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hifigan finetuning\n",
    "\n",
    "import json\n",
    "\n",
    "# with open(\"/data/speech/Helen/helen_train.json\", \"r\") as data_in:\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_15mins.json\", \"r\") as data_in:\n",
    "    lines = data_in.readlines()\n",
    "\n",
    "# with open(\"/data/speech/Helen/helen_train_synmel.json\", \"w\") as data_out:\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35_syn_15mins.json\", \"w\") as data_out:\n",
    "    for i, line in enumerate(lines):\n",
    "        in_line = json.loads(line)\n",
    "        # in_line[\"mel_filepath\"] = f\"/data/speech/Helen/FastPitch_align_syn_mel/train_{i}.npy\"\n",
    "        in_line[\"mel_filepath\"] = f\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/audio_syn_15min/train_{i}.npy\"\n",
    "        data_out.write(json.dumps(in_line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Inference (FP+HiFiGAN)\n",
    "###\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import _AudioTextDataset\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "from nemo.collections.tts.modules.hifigan_modules import Generator\n",
    "\n",
    "\n",
    "fp = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/FastPitch-Align-Helen.nemo\", map_location=\"cpu\")\n",
    "# fp = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/FinetuningDemo/Sirisha_15_mins/FastPitch/2021-09-28_22-20-50/checkpoints/FastPitch.nemo\")\n",
    "# fp = FastPitchModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/checkpoints/JonCkpts/FastPitchNoMixJon.ckpt\")\n",
    "fp = fp.cuda(1).eval()\n",
    "\n",
    "# vocoder = Generator(\n",
    "#     resblock=1,\n",
    "#     upsample_rates=[8, 8, 4, 2],\n",
    "#     upsample_kernel_sizes=[16, 16, 4, 4],\n",
    "#     upsample_initial_channel=512,\n",
    "#     resblock_kernel_sizes=[3, 7, 11],\n",
    "#     resblock_dilation_sizes=[[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "# )\n",
    "# nemo_gen_keys = [k for k in vocoder.state_dict().keys()]\n",
    "# # adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")\n",
    "# # adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_male_44k_00632500\")[\"generator\"]\n",
    "# adlr_gen_ckpt = torch.load(\"/home/jasoli/nemo/NeMo/Hifigan_ADLR_Helen_44k_00570000.pt\")[\"generator\"]\n",
    "# adlr_gen_keys = adlr_gen_ckpt.keys()\n",
    "\n",
    "# new_nemo_ckpt = {nemo_key: adlr_gen_ckpt[adlr_key] for adlr_key, nemo_key in zip(adlr_gen_keys, nemo_gen_keys)}\n",
    "# vocoder.load_state_dict(new_nemo_ckpt)\n",
    "\n",
    "# vocoder = HifiGanModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/checkpoints/JonCkpts/HifiGanMix.ckpt\")\n",
    "vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifiganfinetune/HifiGan/2021-09-17_21-15-08/checkpoints/HifiGan.nemo\", map_location=\"cpu\")\n",
    "# whatever = torch.load(\"/home/jasoli/nemo/NeMo/HifiGan--val_loss=0.26-epoch=246-last.ckpt\")\n",
    "# vocoder = HifiGanModel(cfg=whatever[\"hyper_parameters\"])\n",
    "# vocoder.load_state_dict(whatever[\"state_dict\"])\n",
    "vocoder = vocoder.cuda(1).eval().half()\n",
    "\n",
    "vocoder2 = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifiganfinetune_try2/HifiGan/2021-09-29_20-54-05/checkpoints/HifiGan.nemo\", map_location=\"cpu\")\n",
    "vocoder2 = vocoder.cuda(1).eval().half()\n",
    "\n",
    "\n",
    "import json\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "with open(\"/data/speech/LJSpeech/nvidia_ljspeech_test.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "lines = [json.loads(l) for l in lines]\n",
    "\n",
    "for i, l in enumerate(lines):\n",
    "    tokens = fp.parse(l['text']).cuda(1)\n",
    "    with torch.no_grad():\n",
    "        spectrogram = fp.generate_spectrogram(tokens=tokens)\n",
    "    #     audio = vocoder(x=spectrogram.half()).squeeze(1)  # x for generator\n",
    "        audio = vocoder(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "        audio2 = vocoder2(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "    print(i)\n",
    "    print((audio==audio2).all())\n",
    "    ipd.display(ipd.Audio(audio.float().cpu().numpy(), rate=44100))\n",
    "    ipd.display(ipd.Audio(audio2.float().cpu().numpy(), rate=44100))\n",
    "#     sf.write(f'sample_{i}.wav', audio.float().cpu().numpy().squeeze(0), 44100)\n",
    "    if i > 10:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Mix Sally duration with Helen model\n",
    "###\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "from nemo.core.config import hydra_runner\n",
    "import IPython.display as ipd\n",
    "from nemo.core.classes import typecheck\n",
    "\n",
    "typecheck.set_typecheck_enabled(enabled=False)\n",
    "\n",
    "\n",
    "config = FastPitchModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/FastPitch-Align-Helen.nemo\", return_config=True)\n",
    "config.vocab = config.train_ds.dataset.vocab\n",
    "del config.train_ds\n",
    "del config.validation_ds\n",
    "fastpitch = FastPitchModel(cfg=config)\n",
    "sally_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/checkpoints/Sally/model_weights.ckpt\")\n",
    "helen_checkpoint = torch.load(\"/home/jasoli/nemo/NeMo/checkpoints/Helen/model_weights.ckpt\")\n",
    "\n",
    "# print(config.train_ds.dataset)\n",
    "model_params = {}\n",
    "for n, p in fastpitch.named_parameters():\n",
    "    model_params[n] = ''\n",
    "for key, value in sally_checkpoint.items():\n",
    "    if 'encoder' in key:\n",
    "        new_key_pos = key.find(\"encoder\")\n",
    "        new_key = key[:new_key_pos+len(\"encoder\")]+\"_sally\"+key[new_key_pos+len(\"encoder\"):]\n",
    "        model_params[new_key] = value\n",
    "    elif 'fastpitch.duration_predictor' in key:\n",
    "        new_key_pos = key.find(\"duration_predictor\")\n",
    "        new_key = key[:new_key_pos+len(\"duration_predictor\")]+\"_sally\"+key[new_key_pos+len(\"duration_predictor\"):]\n",
    "        model_params[new_key] = value\n",
    "for key, value in helen_checkpoint.items():\n",
    "    if 'encoder' in key:\n",
    "        new_key_pos = key.find(\"encoder\")\n",
    "        new_key = key[:new_key_pos+len(\"encoder\")]+\"_helen\"+key[new_key_pos+len(\"encoder\"):]\n",
    "        model_params[new_key] = value\n",
    "    elif 'fastpitch.duration_predictor' in key:\n",
    "        new_key_pos = key.find(\"duration_predictor\")\n",
    "        new_key = key[:new_key_pos+len(\"duration_predictor\")]+\"_helen\"+key[new_key_pos+len(\"duration_predictor\"):]\n",
    "        model_params[new_key] = value\n",
    "    else:\n",
    "        model_params[key] = value\n",
    "\n",
    "fastpitch.load_state_dict(model_params)\n",
    "fastpitch.save_to(\"/home/jasoli/nemo/NeMo/checkpoints/Fastpitch-Align-SallySpeed-HelenVoice.nemo\")\n",
    "\n",
    "## INFERENCE\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifigan_from_adlr_in_nemo_helen_00587500.nemo\")\n",
    "# vocoder = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/checkpoints/hifigan_adlr_universal_44k.nemo\")\n",
    "# whatever = torch.load(\"/home/jasoli/nemo/NeMo/HifiGan--val_loss=0.26-epoch=246-last.ckpt\")\n",
    "# vocoder = HifiGanModel(cfg=whatever[\"hyper_parameters\"])\n",
    "# vocoder.load_state_dict(whatever[\"state_dict\"])\n",
    "\n",
    "vocoder = vocoder.cuda().eval().half()\n",
    "fastpitch = fastpitch.cuda().eval()\n",
    "\n",
    "tokens = fastpitch.parse(\"Why do you think I would say something like that?\").cuda()\n",
    "with torch.no_grad():\n",
    "    for i in [0., 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "        spectrogram = fastpitch.generate_spectrogram(tokens=tokens, helen_mix=i)\n",
    "    #     audio = vocoder(x=spectrogram.half()).squeeze(1)  # x for generator\n",
    "        audio = vocoder(spec=spectrogram.half()).squeeze(1)  # spec for hifiganmodel\n",
    "        print(i)\n",
    "        ipd.display(ipd.Audio(audio.float().cpu().numpy(), rate=44100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TTS Finetuning\n",
    "\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import IPython.display as ipd\n",
    "import random\n",
    "from matplotlib.pyplot import imshow\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "\n",
    "filelist = Path(\"/data/speech/NvidiaCustomTTSFinetuning/Sirisha\").glob(\"*.wav\")\n",
    "filelist = [t for t in filelist]\n",
    "random.shuffle(filelist)\n",
    "for f in filelist:\n",
    "    y, sr = librosa.load(f, sr=None)\n",
    "    trimmed, _ = librosa.effects.trim(y, top_db=60, frame_length=1024, hop_length=256)\n",
    "    print(len(y)-len(trimmed))\n",
    "    assert sr == 44100\n",
    "    ipd.display(ipd.Audio(trimmed, rate=sr))\n",
    "    mag = np.abs(librosa.stft(trimmed, n_fft=2048, window=\"hann\"))\n",
    "    stft_matrix = librosa.amplitude_to_db(mag)\n",
    "    fig, ax = plt.subplots(figsize=[8,6])\n",
    "    librosa.display.specshow(stft_matrix, sr=sr, hop_length=512, y_axis=\"log\", ax=ax)\n",
    "    plt.show()\n",
    "    energy = np.sum(stft_matrix, axis=1)\n",
    "    np.sum(mag, axis=0)\n",
    "    plt.plot(np.sum(mag, axis=0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### VCTK reading\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "harvard_sentences = set()\n",
    "with open(\"/home/jasoli/harvard.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        harvard_sentences.add(line)\n",
    "        \n",
    "VCTK_sentences = set()\n",
    "for file in Path(\"/data/speech/VCTK/txt\").glob(\"*/*.txt\"):\n",
    "    with open(file) as f:\n",
    "        text = f.read()\n",
    "\n",
    "#     file_number = file.stem.split(\"_\")[1]\n",
    "#     if file_number in VCTK_sentences:\n",
    "#         assert text == VCTK_sentences[file_number][\"data\"], f\"{text}!={VCTK_sentences[file_number]['data']}|{file}|{VCTK_sentences[file_number]['file']}\"\n",
    "#     else:\n",
    "#         VCTK_sentences[file_number] = {'data': text, 'file': file}\n",
    "    VCTK_sentences.add(text.strip())\n",
    "    assert text not in harvard_sentences, f\"{file}|{text}| was in harvard\"\n",
    "\n",
    "sentences = sorted(VCTK_sentences)\n",
    "# sentence_length = [len(s) for s in sentences]\n",
    "# hist = {}\n",
    "# for l in sentence_length:\n",
    "#     if l not in hist:\n",
    "#         hist[l] = 1\n",
    "#     else:\n",
    "#         hist[l] += 1\n",
    "filtered_sentences = []\n",
    "for s in sentences:\n",
    "    if len(s) > 130 or len(s) < 20:\n",
    "        continue\n",
    "    if any([c.isupper() for c in s[1:]]):\n",
    "        continue\n",
    "    filtered_sentences.append(s)\n",
    "\n",
    "final_sentences = []\n",
    "for i, s in enumerate(filtered_sentences):\n",
    "    if i % 20 == 0:\n",
    "        final_sentences.append(s)\n",
    "        \n",
    "for s in final_sentences:\n",
    "    print(s)\n",
    "print(len(final_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hifigan checkpoint -> nemo\n",
    "\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "\n",
    "model = HifiGanModel.load_from_checkpoint(\"/home/jasoli/nemo/NeMo/examples/tts/nemottsmodels/1.3.1/HifiGan-UniversalHiFi.ckpt\")\n",
    "model.save_to(\"/home/jasoli/nemo/NeMo/examples/tts/nemottsmodels/1.3.1/HifiGan-UniversalHiFi.nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GTC Finetuning Mixing filelists\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "baseline_speaker_files = []\n",
    "new_speaker_files = []\n",
    "\n",
    "stems = set()\n",
    "\n",
    "with open(\"/data/speech/HiFiTTS/8051_manifest_clean_train.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        baseline_speaker_files.append(json.loads(f))\n",
    "        baseline_speaker_files[-1][\"text\"] = baseline_speaker_files[-1][\"text_normalized\"]\n",
    "        del baseline_speaker_files[-1][\"text_no_preprocessing\"]\n",
    "        del baseline_speaker_files[-1][\"text_normalized\"]\n",
    "        baseline_speaker_files[-1][\"speaker\"] = 0\n",
    "        \n",
    "#         stem = Path(baseline_speaker_files[-1][\"audio_filepath\"]).stem\n",
    "#         assert stem not in stems\n",
    "#         stems.add(stem)\n",
    "\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest35.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        new_speaker_files.append(json.loads(f))\n",
    "        new_speaker_files[-1][\"speaker\"] = 1\n",
    "        new_speaker_files[-1][\"text\"]+=(\".\")\n",
    "        \n",
    "#         stem = Path(new_speaker_files[-1][\"audio_filepath\"]).stem\n",
    "#         assert stem not in stems\n",
    "#         stems.add(stem)\n",
    "\n",
    "np.random.shuffle(new_speaker_files)\n",
    "np.random.shuffle(baseline_speaker_files)\n",
    "baseline_speaker_files = baseline_speaker_files[:5000]  # Take first 5k\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_sirisha35/manifest_8051mix_44mins.json\", \"w\") as f:\n",
    "    for ridx, original_record in enumerate(baseline_speaker_files):\n",
    "        new_speaker_record = new_speaker_files[ridx % len(new_speaker_files)]\n",
    "        f.write(json.dumps(original_record) + \"\\n\")\n",
    "        f.write(json.dumps(new_speaker_record) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GTC Finetuning check dataset\n",
    "\n",
    "from nemo.collections.asr.data.audio_to_text import AudioToCharWithPriorAndPitchDataset\n",
    "params = {\n",
    "    \"manifest_filepath\": \"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/manifest_8051mix_40mins.json\",\n",
    "    \"int_values\": False,\n",
    "    \"normalize\": True,\n",
    "    \"sample_rate\": 44100,\n",
    "    \"trim\": False,\n",
    "    \"sup_data_path\": \"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/pitch_prior\",\n",
    "    \"n_window_stride\": 512,\n",
    "    \"n_window_size\": 2048,\n",
    "    \"pitch_fmin\": 80,\n",
    "    \"pitch_fmax\": 256,\n",
    "    \"pitch_avg\": 107.3371734925776,\n",
    "    \"pitch_std\": 19.38637136174705,\n",
    "    \"vocab\":{\n",
    "        \"notation\": \"phonemes\",\n",
    "        \"punct\": True,\n",
    "        \"spaces\": True,\n",
    "        \"stresses\": True,\n",
    "        \"add_blank_at\": None,\n",
    "        \"pad_with_space\": True,\n",
    "        \"chars\": True,\n",
    "        \"improved_version_g2p\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "dataset = AudioToCharWithPriorAndPitchDataset(**params)\n",
    "\n",
    "filelist = []\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/trimmed_oleksii35/manifest_8051mix_40mins.json\", \"r\") as f:\n",
    "    for f in f.readlines():\n",
    "        filelist.append(json.loads(f))\n",
    "\n",
    "audio, audio_len, text, text_len, attn_prior, pitch, speaker = dataset[0]\n",
    "\n",
    "print(text)\n",
    "print(text_len)\n",
    "print(filelist[0][\"text\"])\n",
    "print(len(filelist[0][\"text\"]))\n",
    "\n",
    "print(dataset.vocab._label2id)\n",
    "print(dataset.vocab._id2label)\n",
    "[\"/\".join([dataset.vocab._id2label[i] for i in text])]\n",
    "\n",
    "def id_to_chars(arr):\n",
    "    return [\"/\".join([dataset.vocab._id2label[i] for i in arr])]\n",
    "\n",
    "id_to_chars(dataset.vocab.encode(\"nemo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### HiFiGAN check real audio\n",
    "\n",
    "import torch\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "import IPython.display as ipd\n",
    "\n",
    "\n",
    "config = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/FinetuningDemo/Nick_real/HifiGan/2021-10-25_20-25-55/checkpoints/HifiGan.nemo\", return_config=True)\n",
    "model = HifiGanModel.restore_from(\"/home/jasoli/nemo/NeMo/examples/tts/nemottsmodels/1.3.1/HifiGan-UniversalHiFi.nemo\")\n",
    "model.setup_validation_data(config.validation_ds)\n",
    "\n",
    "\n",
    "model = model.cpu()\n",
    "model._validation_dl\n",
    "for batch in model._validation_dl:\n",
    "    with torch.no_grad():\n",
    "        audio, audio_len = batch\n",
    "        audio_mel, audio_mel_len = model.audio_to_melspec_precessor(audio, audio_len)\n",
    "        audio_pred = model(spec=audio_mel)\n",
    "    ipd.display(ipd.Audio(audio[0].numpy().squeeze(), rate=44100))\n",
    "    ipd.display(ipd.Audio(audio_pred[0].numpy().squeeze(), rate=44100))\n",
    "    print(audio[0].shape)\n",
    "    print(audio_pred[0].shape)\n",
    "    print(\"Sample\")\n",
    "    ipd.display(ipd.Audio(audio[1].numpy().squeeze(), rate=44100))\n",
    "    ipd.display(ipd.Audio(audio_pred[1].numpy().squeeze(), rate=44100))\n",
    "    print(audio[1].shape)\n",
    "    print(audio_pred[1].shape)\n",
    "print(audio_len)\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show(y1, y2):\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True)\n",
    "    hop_length = 512\n",
    "    D = librosa.feature.melspectrogram(y=y1, sr=44100, n_fft=2048, hop_length=512, n_mels=80)\n",
    "    img = librosa.display.specshow(np.log(D), sr=44100, hop_length=hop_length,\n",
    "                             x_axis='time', ax=ax[0])\n",
    "    D = librosa.feature.melspectrogram(y=y2, sr=44100, n_fft=2048, hop_length=512, n_mels=80)\n",
    "    img = librosa.display.specshow(np.log(D), sr=44100, hop_length=hop_length,\n",
    "                             x_axis='time', ax=ax[1])\n",
    "\n",
    "show(audio[0].numpy().squeeze(), audio_pred[0].numpy().squeeze())\n",
    "show(audio[1].numpy().squeeze(), audio_pred[1].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FastPitch vocab check\n",
    "\n",
    "import json\n",
    "from nemo.collections.asr.data import vocabs\n",
    "from nemo.collections.tts.models import FastPitchModel, HifiGanModel\n",
    "\n",
    "fastpitch = FastPitchModel.restore_from(\"/data/speech/NvidiaCustomTTSFinetuning/Nick/FastPitch.nemo\").cuda()\n",
    "hifigan = HifiGanModel.restore_from(\"/data/speech/NvidiaCustomTTSFinetuning/Nick/HifiGan.nemo\").cuda()\n",
    "\n",
    "vocab = vocabs.Phonemes(\n",
    "    punct=True,\n",
    "    stresses=True,\n",
    "    spaces=True,\n",
    "    chars=True,\n",
    "    add_blank_at=None,\n",
    "    pad_with_space=True,\n",
    "    improved_version_g2p=True,\n",
    "    phoneme_dict_path=None,\n",
    ")\n",
    "\n",
    "with open(\"/data/speech/NvidiaCustomTTSFinetuning/Nick/manifest_normalized.json\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "text = []\n",
    "for i in lines:\n",
    "    line = json.loads(i)\n",
    "    text.append((line[\"text\"], vocab.encode(line[\"text\"])))\n",
    "\n",
    "def id_to_chars(arr):\n",
    "    return [vocab._id2label[i] for i in arr]\n",
    "\n",
    "words = set()\n",
    "tokens = set()\n",
    "\n",
    "for i, t in enumerate(text):\n",
    "    for word in t[0].split(\" \"):\n",
    "        words.add(word.lower())\n",
    "    for token in id_to_chars(t[1]):\n",
    "        tokens.add(token)\n",
    "\n",
    "token_count = {}\n",
    "for t in tokens:\n",
    "    token_count[t] = 0\n",
    "\n",
    "for i, t in enumerate(text):\n",
    "    for token in id_to_chars(t[1]):\n",
    "        token_count[token] += 1\n",
    "\n",
    "samples = [\n",
    "    \"In nine teen ninety three; the three co-founders of nvidia had a collective epiphany; that the proper direction for the next wave of computing was accelerated or graphics-based computing because it could solve problems that general-purpose computing could not.\",\n",
    "    \"The release of the Riva tee en tee in nineteen ninety eight solidified nvidia's reputation for developing capable graphics adapters.\",\n",
    "    \"In late nine teen ninety nine; nvidia released the G force two fifty six; en vee ten; most notably introducing on-board transformation and lighting; tee and ell; to consumer-level three dee hardware.\",\n",
    "    \"In October; twenty-twenty; nvidia announced its plan to build Cambridge one; the most powerful computer in the you kay.\",\n",
    "    \"It will leverage artificial intelligence and deep learning to further advance healthcare research; and serve as a hub of innovation for the United Kingdom.\",\n",
    "]\n",
    "vocab.g2p.g2p_dict['nvidia'] = [['EH0','N','V','IH1','D','IY0','AH0']]\n",
    "vocab.g2p.g2p_dict['geforce'] = [['JH','IY1','F','AO2','R','S']]\n",
    "vocab.g2p.g2p_dict['two'] = [['T', 'UW1', 'UW0']]\n",
    "\n",
    "sample_tokens = set()\n",
    "sample_words = set()\n",
    "\n",
    "for i in samples:\n",
    "    for word in i.split(\" \"):\n",
    "        sample_words.add(word.lower())\n",
    "    for token in id_to_chars(vocab.encode(i)):\n",
    "        sample_tokens.add(token)\n",
    "    print(\"/\".join(id_to_chars(vocab.encode(i))))\n",
    "    \n",
    "import IPython.display as ipd\n",
    "import torch\n",
    "from nemo.core.classes import typecheck\n",
    "\n",
    "typecheck.set_typecheck_enabled(enabled=False)\n",
    "\n",
    "samples = [\n",
    "    \"In nine teen ninety three; the three co-founders of nvidia had a collective epiphany; that the proper direction for the next wave of computing was accelerated or graphics-based computing because it could solve problems that general-purpose computing could not.\",\n",
    "    \"The release of the Riva tee en tee in nineteen ninety eight solidified nvidia's reputation for developing capable graphics adapters.\",\n",
    "    \"In late nine teen ninety nine; nvidia released the G force two fifty six; en vee ten; most notably introducing on-board transformation and lighting; tee and ell; to consumer-level three dee hardware.\",\n",
    "    \"In October; twenty-twenty; nvidia announced its plan to build Cambridge one; the most powerful computer in the you kay.\",\n",
    "    \"It will leverage artificial intelligence and deep learning to further advance healthcare research; and serve as a hub of innovation for the United Kingdom.\",\n",
    "]\n",
    "\n",
    "fastpitch = fastpitch.cuda()\n",
    "hifigan = hifigan.cuda()\n",
    "\n",
    "for i in samples:\n",
    "    with torch.no_grad():\n",
    "        tokens = torch.tensor(vocab.encode(i)).cuda()\n",
    "        spectrogram = fastpitch.generate_spectrogram(tokens=tokens.unsqueeze(0), speaker=torch.tensor([1]).cuda())\n",
    "        audio = hifigan(spec=spectrogram).squeeze(1)  # spec for hifiganmodel\n",
    "    ipd.display(ipd.Audio(audio.cpu(), rate=44100))\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokens = torch.tensor(vocab.encode(\"\".join(samples))).cuda()\n",
    "    spectrogram = fastpitch.generate_spectrogram(tokens=tokens.unsqueeze(0), speaker=torch.tensor([1]).cuda())\n",
    "    audio = hifigan(spec=spectrogram).squeeze(1)  # spec for hifiganmodel\n",
    "ipd.display(ipd.Audio(audio.cpu(), rate=44100))\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mel = librosa.feature.melspectrogram(y=audio.cpu().numpy().squeeze(), sr=44100, n_fft=2048, hop_length=512, n_mels=80)\n",
    "fig, ax = plt.subplots(figsize=[150,8])\n",
    "ax.imshow(np.log(mel), origin=\"lower\")\n",
    "plt.show()\n",
    "\n",
    "tokens = set()\n",
    "\n",
    "for i, t in enumerate(text):\n",
    "    for word in t[0].split(\" \"):\n",
    "        words.add(word.lower())\n",
    "    for token in id_to_chars(t[1]):\n",
    "        tokens.add(token)\n",
    "\n",
    "token_count = {}\n",
    "for t in tokens:\n",
    "    token_count[t] = 0\n",
    "\n",
    "for i, t in enumerate(text):\n",
    "    for token in id_to_chars(t[1]):\n",
    "        token_count[token] += 1\n",
    "print(sorted(token_count.items(), key=lambda x: x[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
