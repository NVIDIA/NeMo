{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Megatron-LM_for_Downstream_Tasks.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "74e9cc47740045fe99b449132a964baf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3693a5d1dc9247f69620a6d360274d5d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_69010de9dfcc41a9b8d52dc837103096",
              "IPY_MODEL_5787a34a4bef4342b81011b88f432fb7"
            ]
          }
        },
        "3693a5d1dc9247f69620a6d360274d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "69010de9dfcc41a9b8d52dc837103096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2137573882df4f1aafa496c412db1334",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 442,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 442,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e7884b1b96b4ef1a890a1612ee6a397"
          }
        },
        "5787a34a4bef4342b81011b88f432fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f7698a3ebafd4e649b3aed4a7026759a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 442/442 [00:00&lt;00:00, 3.24kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_df221f56c9dd414a8d7541bc21844f02"
          }
        },
        "2137573882df4f1aafa496c412db1334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e7884b1b96b4ef1a890a1612ee6a397": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f7698a3ebafd4e649b3aed4a7026759a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "df221f56c9dd414a8d7541bc21844f02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b62e1a921a340329c57717ba90b7182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e5889797dd644a788733080ca8033d74",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d6956f0522ae48669f72a682d70efb8b",
              "IPY_MODEL_79086d4f5aa1444e80d25b8a39267f66"
            ]
          }
        },
        "e5889797dd644a788733080ca8033d74": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d6956f0522ae48669f72a682d70efb8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_680fcb90bfbd48ddbe50060a79cab21d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 267967963,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 267967963,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_27af7cda51e543b69c7c7234e7ec75d2"
          }
        },
        "79086d4f5aa1444e80d25b8a39267f66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_852f399e5750441c81a2c1603af6b6db",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 268M/268M [00:06&lt;00:00, 43.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_26058d8fb43d4d7b9f2d6b91e1cbb018"
          }
        },
        "680fcb90bfbd48ddbe50060a79cab21d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "27af7cda51e543b69c7c7234e7ec75d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "852f399e5750441c81a2c1603af6b6db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "26058d8fb43d4d7b9f2d6b91e1cbb018": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "X87q4jmW4Rmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BRANCH = 'main'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djqHSONJ20X8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4fef2981-0136-4437-c645-38ca880f88d8"
      },
      "source": [
        "\"\"\"\n",
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run this cell to set up dependencies.\n",
        "\"\"\"\n",
        "# If you're using Google Colab and not running locally, run this cell\n",
        "\n",
        "# install NeMo\n",
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@{BRANCH}#egg=nemo_toolkit[nlp]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nemo_toolkit[nlp]\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision megatron_docs) to /tmp/pip-install-frr55wvw/nemo-toolkit\n",
            "  Running command git clone -q https://github.com/NVIDIA/NeMo.git /tmp/pip-install-frr55wvw/nemo-toolkit\n",
            "  Running command git checkout -b megatron_docs --track origin/megatron_docs\n",
            "  Switched to a new branch 'megatron_docs'\n",
            "  Branch 'megatron_docs' set up to track remote branch 'megatron_docs' from 'origin'.\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.18.5)\n",
            "Collecting onnx>=1.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/ee/bc7bc88fc8449266add978627e90c363069211584b937fd867b0ccc59f09/onnx-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 6.9MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/af/2f10c8ee22d7a05fe8c9be58ad5c55b71ab4dd895b44f0156bfd5535a708/pytorch_lightning-0.9.0-py3-none-any.whl (408kB)\n",
            "\u001b[K     |████████████████████████████████| 409kB 33.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.8.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.6.0+cu101)\n",
            "Collecting wget\n",
            "  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.12.1)\n",
            "Collecting ruamel.yaml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/39/186f14f3836ac5d2a6a042c8de69988770e8b9abb537610edc429e4914aa/ruamel.yaml-0.16.12-py2.py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (0.22.2.post1)\n",
            "Collecting omegaconf==2.0.1rc12\n",
            "  Downloading https://files.pythonhosted.org/packages/93/44/ec491bba178fea12f87778eca6661e63bfa4baec1bf10461937234f11271/omegaconf-2.0.1rc12-py3-none-any.whl\n",
            "Collecting hydra-core==1.0.0rc4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b9/af/90d053fd14f52a7b86eb8623b45b844e67676f20db26e5e1b666f77d8faf/hydra_core-1.0.0rc4-py3-none-any.whl (117kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 43.6MB/s \n",
            "\u001b[?25hCollecting transformers>=2.11.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/05/c8c55b600308dc04e95100dc8ad8a244dd800fe75dfafcf1d6348c6f6209/transformers-3.1.0-py3-none-any.whl (884kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (1.14.48)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.10.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (3.2.2)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 37.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (0.3.1)\n",
            "Collecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 41.7MB/s \n",
            "\u001b[?25hCollecting youtokentome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/65/4a86cf99da3f680497ae132329025b291e2fda22327e8da6a9476e51acb1/youtokentome-1.0.6-cp36-cp36m-manylinux2010_x86_64.whl (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (4.41.1)\n",
            "Collecting rapidfuzz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/00/d1d68b3600395557ece71ef4a119c65a09ea8cd1f27490953d99b0253885/rapidfuzz-0.11.1-cp36-cp36m-manylinux2010_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 38.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (3.6.4)\n",
            "Collecting megatron-lm>=1.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/a5/29e0a4edd782af828c6a22b4206b606e56816c3d0c5734a7cc593961ecf5/megatron_lm-1.1.4-py3-none-any.whl (191kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: inflect in /usr/local/lib/python3.6/dist-packages (from nemo_toolkit[nlp]) (2.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.7.4.3)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.12.4)\n",
            "Collecting PyYAML>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (20.4)\n",
            "Collecting tensorboard==2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/f5/d75a6f7935e4a4870d85770bc9976b12e7024fbceb83a1a6bc50e6deb7c4/tensorboard-2.2.0-py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 33.9MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 38.2MB/s \n",
            "\u001b[?25hCollecting ruamel.yaml.clib>=0.1.2; platform_python_implementation == \"CPython\" and python_version < \"3.9\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/94/cbf462e95ffaf8af5d07b0e55cfedd74b05e2831ed3d19fc946bee8465b9/ruamel.yaml.clib-0.2.1-cp36-cp36m-manylinux1_x86_64.whl (549kB)\n",
            "\u001b[K     |████████████████████████████████| 552kB 34.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (1.4.1)\n",
            "Requirement already satisfied: dataclasses; python_version == \"3.6\" in /usr/local/lib/python3.6/dist-packages (from omegaconf==2.0.1rc12->nemo_toolkit[nlp]) (0.7)\n",
            "Collecting importlib-resources; python_version < \"3.9\"\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/03/0f9595c0c2ef12590877f3c47e5f579759ce5caf817f8256d5dcbd8a1177/importlib_resources-3.0.0-py2.py3-none-any.whl\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/02/789a0bddf9c9b31b14c3e79ec22b9656185a803dc31c15f006f9855ece0d/antlr4-python3-runtime-4.8.tar.gz (112kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 43.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (3.0.12)\n",
            "Collecting tokenizers==0.8.1.rc2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/83/8b9fccb9e48eeb575ee19179e2bdde0ee9a1904f97de5f02d19016b8804f/tokenizers-0.8.1rc2-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 36.5MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 30.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=2.11.0->nemo_toolkit[nlp]) (2.23.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.48 in /usr/local/lib/python3.6/dist-packages (from boto3->nemo_toolkit[nlp]) (1.17.48)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->nemo_toolkit[nlp]) (0.10.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from youtokentome->nemo_toolkit[nlp]) (7.1.2)\n",
            "Collecting pybind11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/89/e3/d576f6f02bc75bacbc3d42494e8f1d063c95617d86648dba243c2cb3963e/pybind11-2.5.0-py2.py3-none-any.whl (296kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 40.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->onnx>=1.7.0->nemo_toolkit[nlp]) (49.6.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (3.2.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.4.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.35.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.31.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.17.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-resources; python_version < \"3.9\"->hydra-core==1.0.0rc4->nemo_toolkit[nlp]) (3.1.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=2.11.0->nemo_toolkit[nlp]) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.48->boto3->nemo_toolkit[nlp]) (0.15.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.7.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (4.1.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (4.6)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard==2.2.0->pytorch-lightning==0.9.0->nemo_toolkit[nlp]) (0.4.8)\n",
            "Building wheels for collected packages: nemo-toolkit, wget, PyYAML, future, antlr4-python3-runtime, sacremoses\n",
            "  Building wheel for nemo-toolkit (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo-toolkit: filename=nemo_toolkit-1.0.0a0-cp36-none-any.whl size=384001 sha256=1aed584f813d9403a8ef26bf95e03679b29f8c3d9c17514ff93f8f424a404beb\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-h8nq7tqw/wheels/53/0d/1a/5ec64716c0751ea1f1000c1131cb5ed8d573af06685aa69ec0\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=a4724de56213bc7b390c0657217b71fda4c090b20295816ac0883f3167a593a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=34dde048580e3d8f52f7054bebc846e6b1949e34906a923273bdc2f7ad083970\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp36-none-any.whl size=491057 sha256=70ec1a3e95af256ac10c037e42c679ea647a199a0306d33a226655391fecb532\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-cp36-none-any.whl size=141230 sha256=74ef20dac9a03ec543db345eb09716fc273c4d50d7a6be602f2f37c78b4973d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/e2/fa/b78480b448b8579ddf393bebd3f47ee23aa84c89b6a78285c8\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=b7779f3e97a9a28b6212711a2cac1765260f96504b5e3f773a10ae5cb2736d7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built nemo-toolkit wget PyYAML future antlr4-python3-runtime sacremoses\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 2.2.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: onnx, PyYAML, tensorboard, future, pytorch-lightning, wget, ruamel.yaml.clib, ruamel.yaml, omegaconf, importlib-resources, antlr4-python3-runtime, hydra-core, tokenizers, sentencepiece, sacremoses, transformers, unidecode, youtokentome, rapidfuzz, pybind11, megatron-lm, nemo-toolkit\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: tensorboard 2.3.0\n",
            "    Uninstalling tensorboard-2.3.0:\n",
            "      Successfully uninstalled tensorboard-2.3.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 antlr4-python3-runtime-4.8 future-0.18.2 hydra-core-1.0.0rc4 importlib-resources-3.0.0 megatron-lm-1.1.4 nemo-toolkit-1.0.0a0 omegaconf-2.0.1rc12 onnx-1.7.0 pybind11-2.5.0 pytorch-lightning-0.9.0 rapidfuzz-0.11.1 ruamel.yaml-0.16.12 ruamel.yaml.clib-0.2.1 sacremoses-0.0.43 sentencepiece-0.1.91 tensorboard-2.2.0 tokenizers-0.8.1rc2 transformers-3.1.0 unidecode-1.1.1 wget-3.2 youtokentome-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmvtH0pxHDQC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "574c3578-9e70-4906-9fd0-dcf82c66472c"
      },
      "source": [
        "import os\n",
        "import wget\n",
        "from nemo.collections import nlp as nemo_nlp\n",
        "from omegaconf import OmegaConf"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[NeMo W 2020-09-04 15:09:59 experimental:28] Module <class 'nemo.collections.nlp.modules.common.megatron.megatron_bert.MegatronBertEncoder'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
            "[NeMo W 2020-09-04 15:09:59 experimental:28] Module <class 'nemo.collections.nlp.modules.common.sequence_token_classifier.SequenceTokenClassifier'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: APEX is not installed, multi_tensor_applier will not be available.\n",
            "WARNING: APEX is not installed, using torch.nn.LayerNorm instead of apex.normalization.FusedLayerNorm!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6AARcXXUEbs",
        "colab_type": "text"
      },
      "source": [
        "# Language models\n",
        "\n",
        "Natural Language Processing (NLP) field experienced a huge leap in recent years due to the concept of transfer learning enabled through pretrained language models.\n",
        "\n",
        "[BERT](https://arxiv.org/abs/1810.04805), [RoBERTa](https://arxiv.org/abs/1907.11692), [Megatron-LM](https://arxiv.org/abs/1909.08053), and many other proposed language models achieve state-of-the-art results on many NLP tasks, such as:\n",
        "* question answering\n",
        "* sentiment analysis\n",
        "* named entity recognition and many others.\n",
        "\n",
        "In NeMo, most of the NLP models represent a pretrained language model followed by a Token Classification layer or a Sequence Classification layer or a combination of both. By changing the language model, you can improve the performance of your final model for the specific downstream task you are solving.\n",
        "\n",
        "With NeMo you can use either pretrain a BERT model from your data or use a pretrained language model from [HuggingFace transformers](https://github.com/huggingface/transformers) or [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) libraries.\n",
        "\n",
        "Let's take a look at the list of available pretrained language models:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp7F45bgX7SU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "0262a3bc-1d20-4e28-a77d-544a5fd76dcf"
      },
      "source": [
        "nemo_nlp.modules.get_pretrained_lm_models_list()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['megatron-bert-345m-uncased',\n",
              " 'megatron-bert-345m-cased',\n",
              " 'biomegatron-bert-345m-uncased',\n",
              " 'biomegatron-bert-345m-cased',\n",
              " 'megatron-bert-uncased',\n",
              " 'megatron-bert-cased',\n",
              " 'bert-base-uncased',\n",
              " 'bert-large-uncased',\n",
              " 'bert-base-cased',\n",
              " 'bert-large-cased',\n",
              " 'bert-base-multilingual-uncased',\n",
              " 'bert-base-multilingual-cased',\n",
              " 'bert-base-chinese',\n",
              " 'bert-base-german-cased',\n",
              " 'bert-large-uncased-whole-word-masking',\n",
              " 'bert-large-cased-whole-word-masking',\n",
              " 'bert-large-uncased-whole-word-masking-finetuned-squad',\n",
              " 'bert-large-cased-whole-word-masking-finetuned-squad',\n",
              " 'bert-base-cased-finetuned-mrpc',\n",
              " 'bert-base-german-dbmdz-cased',\n",
              " 'bert-base-german-dbmdz-uncased',\n",
              " 'cl-tohoku/bert-base-japanese',\n",
              " 'cl-tohoku/bert-base-japanese-whole-word-masking',\n",
              " 'cl-tohoku/bert-base-japanese-char',\n",
              " 'cl-tohoku/bert-base-japanese-char-whole-word-masking',\n",
              " 'TurkuNLP/bert-base-finnish-cased-v1',\n",
              " 'TurkuNLP/bert-base-finnish-uncased-v1',\n",
              " 'wietsedv/bert-base-dutch-cased',\n",
              " 'distilbert-base-uncased',\n",
              " 'distilbert-base-uncased-distilled-squad',\n",
              " 'distilbert-base-cased',\n",
              " 'distilbert-base-cased-distilled-squad',\n",
              " 'distilbert-base-german-cased',\n",
              " 'distilbert-base-multilingual-cased',\n",
              " 'distilbert-base-uncased-finetuned-sst-2-english',\n",
              " 'roberta-base',\n",
              " 'roberta-large',\n",
              " 'roberta-large-mnli',\n",
              " 'distilroberta-base',\n",
              " 'roberta-base-openai-detector',\n",
              " 'roberta-large-openai-detector',\n",
              " 'albert-base-v1',\n",
              " 'albert-large-v1',\n",
              " 'albert-xlarge-v1',\n",
              " 'albert-xxlarge-v1',\n",
              " 'albert-base-v2',\n",
              " 'albert-large-v2',\n",
              " 'albert-xlarge-v2',\n",
              " 'albert-xxlarge-v2']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdpxEiirX8F-",
        "colab_type": "text"
      },
      "source": [
        "NLP models for downstream tasks use `get_pretrained_lm_model` helper function to easily switch between language models from the list above to another:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zllsvBojxuJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476,
          "referenced_widgets": [
            "74e9cc47740045fe99b449132a964baf",
            "3693a5d1dc9247f69620a6d360274d5d",
            "69010de9dfcc41a9b8d52dc837103096",
            "5787a34a4bef4342b81011b88f432fb7",
            "2137573882df4f1aafa496c412db1334",
            "8e7884b1b96b4ef1a890a1612ee6a397",
            "f7698a3ebafd4e649b3aed4a7026759a",
            "df221f56c9dd414a8d7541bc21844f02",
            "3b62e1a921a340329c57717ba90b7182",
            "e5889797dd644a788733080ca8033d74",
            "d6956f0522ae48669f72a682d70efb8b",
            "79086d4f5aa1444e80d25b8a39267f66",
            "680fcb90bfbd48ddbe50060a79cab21d",
            "27af7cda51e543b69c7c7234e7ec75d2",
            "852f399e5750441c81a2c1603af6b6db",
            "26058d8fb43d4d7b9f2d6b91e1cbb018"
          ]
        },
        "outputId": "25bb4304-8d62-4c3e-c32f-a64bcf35967d"
      },
      "source": [
        "# use any pretrained model name from the list above\n",
        "nemo_nlp.modules.get_pretrained_lm_model(pretrained_model_name='distilbert-base-uncased')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74e9cc47740045fe99b449132a964baf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=442.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b62e1a921a340329c57717ba90b7182",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=267967963.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[NeMo I 2020-09-04 15:10:13 megatron_utils:196] Downloading from https://api.ngc.nvidia.com/v2/models/nvidia/megatron_bert_345m/versions/v0.0/files/release/mp_rank_00/model_optim_rng.pt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e57d6ed70318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# or any other supported models from the list above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnemo_nlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pretrained_lm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'megatron-bert-345m-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nemo/collections/nlp/modules/common/common_utils.py\u001b[0m in \u001b[0;36mget_pretrained_lm_model\u001b[0;34m(pretrained_model_name, config_dict, config_file, checkpoint_file)\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mconfig_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m                 \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             )\n\u001b[1;32m     70\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nemo/collections/nlp/modules/common/megatron/megatron_utils.py\u001b[0m in \u001b[0;36mget_megatron_lm_model\u001b[0;34m(pretrained_model_name, config_dict, config_file, checkpoint_file)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mcheckpoint_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_megatron_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_megatron_vocab_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nemo/collections/nlp/modules/common/megatron/megatron_utils.py\u001b[0m in \u001b[0;36mget_megatron_checkpoint\u001b[0;34m(pretrained_model_name)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMEGATRON_CONFIG_MAP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"checkpoint\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEGATRON_CACHE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nemo/collections/nlp/modules/common/megatron/megatron_utils.py\u001b[0m in \u001b[0;36m_download\u001b[0;34m(path, url)\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMEGATRON_CACHE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Downloading from {url}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mwget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m             \u001b[0;31m# wait until the master process downloads the file and writes it to the cache dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wget.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, out, bar)\u001b[0m\n\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mbinurl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mulib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinurl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1012\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEgEo0aPj3Ws",
        "colab_type": "text"
      },
      "source": [
        "All NeMo [NLP models](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp) have an associated config file. As an example, let's examine the config file for the Named Entity Recognition (NER) model (more details about the model and the NER task, could be found [here](https://github.com/NVIDIA/NeMo/blob/main/tutorials/nlp/Punctuation_and_Capitalization.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1gA8PsJ13MJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a807e471-f3d2-49fe-dd63-ccdc81ea05a4"
      },
      "source": [
        "MODEL_CONFIG = \"token_classification_config.yaml\"\n",
        "\n",
        "# download the model's configuration file \n",
        "if not os.path.exists(MODEL_CONFIG):\n",
        "    print('Downloading config file...')\n",
        "    wget.download('https://raw.githubusercontent.com/NVIDIA/NeMo/' + BRANCH + '/examples/nlp/token_classification/conf/' + MODEL_CONFIG)\n",
        "else:\n",
        "    print ('Config file already exists')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading config file...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX3KmWMvSUQw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e8643e58-3c10-406f-ac0c-17d48bd12dfd"
      },
      "source": [
        "# this line will print the entire config of the model\n",
        "config = OmegaConf.load(MODEL_CONFIG)\n",
        "print(OmegaConf.to_yaml(config))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainer:\n",
            "  gpus: 1\n",
            "  num_nodes: 1\n",
            "  max_epochs: 5\n",
            "  max_steps: null\n",
            "  accumulate_grad_batches: 1\n",
            "  gradient_clip_val: 0.0\n",
            "  amp_level: O0\n",
            "  precision: 16\n",
            "  distributed_backend: ddp\n",
            "  checkpoint_callback: false\n",
            "  logger: false\n",
            "  row_log_interval: 1\n",
            "  val_check_interval: 1.0\n",
            "  resume_from_checkpoint: null\n",
            "exp_manager:\n",
            "  exp_dir: null\n",
            "  name: token_classification_model\n",
            "  create_tensorboard_logger: true\n",
            "  create_checkpoint_callback: true\n",
            "model:\n",
            "  nemo_path: null\n",
            "  label_ids: null\n",
            "  dataset:\n",
            "    data_dir: ???\n",
            "    class_balancing: null\n",
            "    max_seq_length: 128\n",
            "    pad_label: O\n",
            "    ignore_extra_tokens: false\n",
            "    ignore_start_end: false\n",
            "    use_cache: true\n",
            "    num_workers: 2\n",
            "    pin_memory: false\n",
            "    drop_last: false\n",
            "  train_ds:\n",
            "    text_file: text_train.txt\n",
            "    labels_file: labels_train.txt\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "  validation_ds:\n",
            "    text_file: text_dev.txt\n",
            "    labels_file: labels_dev.txt\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    batch_size: 64\n",
            "  language_model:\n",
            "    pretrained_model_name: bert-base-uncased\n",
            "    bert_checkpoint: null\n",
            "    bert_config: null\n",
            "    tokenizer: nemobert\n",
            "    vocab_file: null\n",
            "    tokenizer_model: null\n",
            "    do_lower_case: false\n",
            "  head:\n",
            "    num_fc_layers: 2\n",
            "    fc_dropout: 0.5\n",
            "    activation: relu\n",
            "    log_softmax: true\n",
            "    use_transformer_init: true\n",
            "  optim:\n",
            "    name: adam\n",
            "    lr: 5.0e-05\n",
            "    weight_decay: 0.0\n",
            "    sched:\n",
            "      name: WarmupAnnealing\n",
            "      warmup_steps: null\n",
            "      warmup_ratio: 0.1\n",
            "      last_epoch: -1\n",
            "      monitor: val_loss\n",
            "      reduce_on_plateau: false\n",
            "hydra:\n",
            "  run:\n",
            "    dir: .\n",
            "  job_logging:\n",
            "    root:\n",
            "      handlers: null\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKFbFDwzKUZB",
        "colab_type": "text"
      },
      "source": [
        "For the purposes of this tutorial, we are interested in the language_model part of the Named Entity Recognition Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6hlcCYyKFiY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "a6f7e18c-46c1-4e78-a894-bc571a8f70e6"
      },
      "source": [
        "print(OmegaConf.to_yaml(config.model.language_model))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pretrained_model_name: bert-base-uncased\n",
            "bert_checkpoint: null\n",
            "bert_config: null\n",
            "tokenizer: nemobert\n",
            "vocab_file: null\n",
            "tokenizer_model: null\n",
            "do_lower_case: false\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xldsDiH9ZY2h",
        "colab_type": "text"
      },
      "source": [
        "There are might be slight differences from one model to another, but most of them have the following important parameters associated with a language model:\n",
        "* `pretrained_model_name` - a name of the pretrained model from either HuggingFace or Megatron-LM libraries\n",
        "* `bert_checkpoint` - a path to the pretrained language model checkpoint if, for example, you trained a BERT model with your own data\n",
        "* `bert_config` - a path the model config file if a language you want to use differs from the model's default configuration\n",
        "\n",
        "To modify the default language model, specify the desired language model name with `model.language_model.pretrained_model_name` argument, like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r_1gIXwashY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.model.language_model = 'roberta-base'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVp4zvxPatga",
        "colab_type": "text"
      },
      "source": [
        "and then start the training as usual (please see [tutorials/nlp](https://github.com/NVIDIA/NeMo/tree/main/tutorials/nlp) for more details about training a particular model). \n",
        "\n",
        "You can also provide a pretrained language model checkpoint and a configuration file if available.\n",
        "\n",
        "All the above holds for both HuggingFace and Megatron-LM pretrained language models. Let's now take a closer look on how to finetune Megatron-LM and how to use a HuggingFace model not included in `nemo_nlp.modules.get_pretrained_lm_models_list()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi_LBdSA4U6Q",
        "colab_type": "text"
      },
      "source": [
        "# Downstream tasks with Megatron and BioMegatron LM\n",
        "\n",
        "All the above holds for both HuggingFace and Megatron-LM pretrained language models, but let's closely examine the Megatron-LM.\n",
        "\n",
        "[Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) is a large, powerful transformer developed by the Applied Deep Learning Research team at NVIDIA. More details could be found at [Megatron-LM github repo](https://github.com/NVIDIA/Megatron-LM).\n",
        "\n",
        "To see the list of available Megatron-LM models in NeMo, run:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o-XPMrIQBmm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "85f53c28-6713-44ee-d1b0-2478a6633498"
      },
      "source": [
        "nemo_nlp.modules.get_megatron_lm_models_list()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['megatron-bert-345m-uncased',\n",
              " 'megatron-bert-345m-cased',\n",
              " 'biomegatron-bert-345m-uncased',\n",
              " 'biomegatron-bert-345m-cased',\n",
              " 'megatron-bert-uncased',\n",
              " 'megatron-bert-cased']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM_ei7OSLn1X",
        "colab_type": "text"
      },
      "source": [
        "If you want to use one of the available Megatron-LM models, specify its name with `model.language_model.pretrained_model_name` argument, for example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcQLoouME8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config.model.language_model = 'megatron-bert-345m-uncased'"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mla-BZexMM2u",
        "colab_type": "text"
      },
      "source": [
        "If you have a different checkpoint or a model configuration file, use these general Megatron-LM model names:\n",
        "* `megatron-bert-uncased` or \n",
        "* `megatron-bert-cased` \n",
        "\n",
        "and provide associated bert_config and bert_checkpoint files, as follows:\n",
        "\n",
        "`model.language_model.pretrained_model_name=megatron-bert-uncased \\\n",
        "model.language_model.bert_checkpoint=<PATH_TO_CHECKPOINT> \\\n",
        "model.language_model.bert_config=<PAHT_TO_CONFIG>`\n",
        " \n",
        " or \n",
        " \n",
        "`model.language_model.pretrained_model_name=megatron-bert-cased \\\n",
        "model.language_model.bert_checkpoint=<PATH_TO_CHECKPOINT> \\\n",
        "model.language_model.bert_config=<PAHT_TO_CONFIG>`\n",
        "\n",
        "The general Megatron-LM model names are used to download the correct vocabulary file needed to setup the model correctly. Note, the data preprocessing and model training is done in NeMo. Megatron-LM has its own set of training arguments (including tokenizer) that are ignored during finetuning in NeMo. Please see downstream task [config files and training scripts](https://github.com/NVIDIA/NeMo/tree/main/examples/nlp) for all NeMo supported arguments.\n",
        "\n",
        "## Download pretrained model\n",
        "\n",
        "With NeMo, the original and domain-specific Megatron-LM BERT models and model configuration files will be downloaded automatically, but they also could be downloaded with the links below:\n",
        "\n",
        "[Megatron-LM BERT Uncased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/files?version=v0.1_uncased)\n",
        "\n",
        "[Megatron-LM BERT Cased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m/files?version=v0.1_cased)\n",
        "\n",
        "[BioMegatron-LM BERT Cased 345M (~345M parameters): https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345mcased](https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345mcased)\n",
        "\n",
        "[BioMegatron-LM BERT Uncased 345M (~345M parameters)](https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345muncased): https://ngc.nvidia.com/catalog/models/nvidia:biomegatron345muncased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caafJmVLmEVD",
        "colab_type": "text"
      },
      "source": [
        "# Using any HiggingFace Pretrained Model\n",
        "\n",
        "If a pretrained HuggingFace model you want to use is missing in `nemo_nlp.modules.get_pretrained_lm_models_list()`, remove line `self.bert_model = nemo_nlp.modules.get_pretrained_lm_model(...)` inside your PyTorch Lightning Model and directly use HuggingFace API. For example, if you want to use a [T5](https://arxiv.org/abs/1910.10683) in [TokenClassificationModel](https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/nlp/models/token_classification/token_classification_model.py) to solve Named Entity Recognition task, define `self.bert_model` as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms4DAC4BpvqS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d36be105-d3d1-4bbf-8903-fe4c21839f76"
      },
      "source": [
        "from transformers import T5Model\n",
        "\n",
        "# use self.bert_model inside the TokenClassificationModel Class\n",
        "bert_model = T5Model.from_pretrained('t5-small')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of T5Model were not initialized from the model checkpoint at t5-small and are newly initialized: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2Joti9XrJEp",
        "colab_type": "text"
      },
      "source": [
        "Then continue training your PyTorch Lightning Model, as usual, more details on model training could be found at [tutorials](https://github.com/NVIDIA/NeMo/tree/main/tutorials)."
      ]
    }
  ]
}