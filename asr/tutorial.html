

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; nemo 0.11.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Datasets" href="datasets.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Speech Recognition</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="#get-data">Get data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#mixed-precision-training">Mixed Precision training</a></li>
<li class="toctree-l3"><a class="reference internal" href="#multi-gpu-training">Multi-GPU training</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#large-training-example">Large Training Example</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#fine-tuning">Fine-tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation">Evaluation</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-with-language-model">Evaluation with Language Model</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#using-kenlm">Using KenLM</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#using-and-converting-to-tarred-datasets">Using and Converting to Tarred Datasets</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#conversion-from-an-existing-dataset-to-tarred-dataset">Conversion from an Existing Dataset to Tarred Dataset</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#kaldi-compatibility">Kaldi Compatibility</a></li>
<li class="toctree-l3"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="datasets.html">Datasets</a></li>
<li class="toctree-l2"><a class="reference internal" href="models.html">Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="8kHz_models.html">8kHz Models</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../nlp/intro.html">Natural Language Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Speech Recognition</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/asr/tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>Make sure you have installed <code class="docutils literal notranslate"><span class="pre">nemo</span></code> and the <code class="docutils literal notranslate"><span class="pre">nemo_asr</span></code> collection.
See the <a class="reference internal" href="../index.html#installation"><span class="std std-ref">Getting started</span></a> section.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You only need to have <code class="docutils literal notranslate"><span class="pre">nemo</span></code> and the <code class="docutils literal notranslate"><span class="pre">nemo_asr</span></code> collection for this tutorial.</p>
</div>
<p>A more introductory, Jupyter notebook ASR tutorial can be found <a class="reference external" href="https://github.com/NVIDIA/NeMo/tree/master/examples/asr/notebooks">on GitHub</a>.</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This Automatic Speech Recognition (ASR) tutorial is focused on QuartzNet <a class="bibtex reference internal" href="#asr-tut-kriman2019quartznet" id="id1">[ASR-TUT2]</a> model.
QuartzNet is a CTC-based <a class="bibtex reference internal" href="#asr-tut-graves2006" id="id2">[ASR-TUT1]</a> end-to-end model. The model is called “end-to-end” because it
transcribes speech samples without any additional alignment information. CTC allows for finding an alignment between
audio and text.</p>
<p>The CTC-ASR training pipeline consists of the following blocks:</p>
<ol class="arabic simple">
<li><p>Audio preprocessing (feature extraction): signal normalization, windowing, (log) spectrogram (or mel scale spectrogram, or MFCC)</p></li>
<li><p>Neural acoustic model (which predicts a probability distribution P_t(c) over vocabulary characters c per each time step t given input features per each timestep)</p></li>
<li><p>CTC loss function</p></li>
</ol>
<img alt="CTC-based ASR" class="align-center" src="../_images/ctc_asr.png" />
</div>
<div class="section" id="get-data">
<h2>Get data<a class="headerlink" href="#get-data" title="Permalink to this headline">¶</a></h2>
<p>We will be using an open-source LibriSpeech <a class="bibtex reference internal" href="#asr-tut-panayotov2015librispeech" id="id3">[ASR-TUT3]</a> dataset. These scripts will download and convert LibriSpeech into format expected by <cite>nemo_asr</cite>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mkdir data
<span class="c1"># note that this script requires sox to be installed</span>
<span class="c1"># to install sox on Ubuntu, simply do: sudo apt-get install sox</span>
<span class="c1"># and then: pip install sox</span>
<span class="c1"># get_librispeech_data.py script is located under &lt;nemo_git_repo_root&gt;/scripts</span>
python get_librispeech_data.py --data_root<span class="o">=</span>data --data_set<span class="o">=</span>dev_clean,train_clean_100
<span class="c1"># To get all LibriSpeech data, do:</span>
<span class="c1"># python get_librispeech_data.py --data_root=data --data_set=ALL</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You should have at least 52GB of disk space available if you’ve used <code class="docutils literal notranslate"><span class="pre">--data_set=dev_clean,train_clean_100</span></code>; and
at least 250GB if you used <code class="docutils literal notranslate"><span class="pre">--data_set=ALL</span></code>. Also, it will take some time to download and process, so go grab a
coffee. After downloading, you can remove the original .tar.gz archives and .flac files to cut the disk usage in
half.</p>
</div>
<p>After download and conversion, your <cite>data</cite> folder should contain 2 json files:</p>
<ul class="simple">
<li><p>dev_clean.json</p></li>
<li><p>train_clean_100.json</p></li>
</ul>
<p>In the tutorial we will use <cite>train_clean_100.json</cite> for training and <cite>dev_clean.json</cite> for evaluation.
Each line in json file describes a training sample - <cite>audio_filepath</cite> contains path to the wav file, <cite>duration</cite> it’s duration in seconds, and <cite>text</cite> is it’s transcript:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="nt">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;absolute_path_to&gt;/1355-39947-0000.wav&quot;</span><span class="p">,</span> <span class="nt">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">11.3</span><span class="p">,</span> <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;psychotherapy and the community both the physician and the patient find their place in the community the life interests of which are superior to the interests of the individual&quot;</span><span class="p">}</span>
<span class="p">{</span><span class="nt">&quot;audio_filepath&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;absolute_path_to&gt;/1355-39947-0001.wav&quot;</span><span class="p">,</span> <span class="nt">&quot;duration&quot;</span><span class="p">:</span> <span class="mf">15.905</span><span class="p">,</span> <span class="nt">&quot;text&quot;</span><span class="p">:</span> <span class="s2">&quot;it is an unavoidable question how far from the higher point of view of the social mind the psychotherapeutic efforts should be encouraged or suppressed are there any conditions which suggest suspicion of or direct opposition to such curative work&quot;</span><span class="p">}</span>
</pre></div>
</div>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>We will train a small model from the QuartzNet family <a class="bibtex reference internal" href="#asr-tut-kriman2019quartznet" id="id4">[ASR-TUT2]</a>. QuartzNet models are similar
to time delay neural networks (TDNN) composed of 1D convolutions. However QuartzNet models use separable convolutions
to reduce the total number of parameters. The Quartznet family of models are denoted as QuartzNet_[BxR] where B is the
number of blocks, and R - the number of convolutional sub-blocks within a block. Each sub-block contains a
1-D separable convolution, batch normalization, and ReLU:</p>
<img alt="quartznet model" class="align-center" src="../_images/quartz_vertical.png" />
<p>In the tutorial we will be using model [15x5] and will be using separable convolutions.
The script below does both training (on <cite>train_clean_100.json</cite>) and evaluation (on <cite>dev_clean.json</cite>) on single GPU:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python &lt;nemo_git_repo_root&gt;/examples/asr/speech2text.py --asr_model<span class="o">=</span>&lt;nemo_git_repo_root&gt;/examples/asr/configs/quartznet15x5.yaml --train_dataset<span class="o">=</span>train_clean_100.json --eval_datasets<span class="o">=</span>dev_clean.json --batch_size<span class="o">=</span><span class="m">16</span> --num_epochs<span class="o">=</span><span class="m">15</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<dl class="simple">
<dt>To improve your word error rates:</dt><dd><ol class="arabic simple">
<li><p>Train longer</p></li>
<li><p>Train on more data</p></li>
<li><p>Use larger model</p></li>
<li><p>Train on several GPUs and use mixed precision (on NVIDIA Volta and Turing GPUs)</p></li>
<li><p>Start with pre-trained checkpoints</p></li>
</ol>
</dd>
</dl>
</div>
</div>
<div class="section" id="mixed-precision-training">
<h2>Mixed Precision training<a class="headerlink" href="#mixed-precision-training" title="Permalink to this headline">¶</a></h2>
<p>Mixed precision and distributed training in NeMo is based on <a class="reference external" href="https://github.com/NVIDIA/apex">NVIDIA’s APEX library</a>.
Make sure it is installed.</p>
<p>To train with mixed-precision all you need is to set <cite>optimization_level</cite> parameter of <cite>nemo.core.NeuralModuleFactory</cite>  to <cite>nemo.core.Optimization.mxprO1</cite>. For example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span>
    <span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">,</span>
    <span class="n">optimization_level</span><span class="o">=</span><span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">Optimization</span><span class="o">.</span><span class="n">mxprO1</span><span class="p">,</span>
    <span class="n">cudnn_benchmark</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because mixed precision requires Tensor Cores it only works on NVIDIA Volta and Turing based GPUs</p>
</div>
</div>
<div class="section" id="multi-gpu-training">
<h2>Multi-GPU training<a class="headerlink" href="#multi-gpu-training" title="Permalink to this headline">¶</a></h2>
<p>Enabling multi-GPU training with NeMo is easy:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>First set <cite>placement</cite> to <cite>nemo.core.DeviceType.AllGpu</cite> in NeuralModuleFactory</p></li>
<li><p>Have your script accept ‘local_rank’ argument and do not set it yourself: <cite>parser.add_argument(“–local_rank”, default=None, type=int)</cite></p></li>
<li><p>Use <cite>torch.distributed.launch</cite> package to run your script like this (replace &lt;num_gpus&gt; with number of gpus):</p></li>
</ol>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span>&lt;num_gpus&gt; &lt;nemo_git_repo_root&gt;/examples/asr/speech2text.py ...
</pre></div>
</div>
<div class="section" id="large-training-example">
<h3>Large Training Example<a class="headerlink" href="#large-training-example" title="Permalink to this headline">¶</a></h3>
<p>Please refer to the <cite>&lt;nemo_git_repo_root&gt;/examples/asr/speech2text.py</cite> for comprehensive example. It builds one train DAG
and multiple validation DAGs. Each validation DAG shares the same model and parameters as the training DAG and can
be used to evaluate a different evaluation dataset.</p>
<p>Assuming, you are working with Volta-based DGX, you can run training like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python -m torch.distributed.launch --nproc_per_node<span class="o">=</span>&lt;num_gpus&gt; python &lt;nemo_git_repo_root&gt;/examples/asr/speech2text.py --asr_model<span class="o">=</span>&lt;nemo_git_repo_root&gt;/examples/asr/configs/quartznet15x5.yaml --train_dataset<span class="o">=</span>train_clean_100.json --eval_datasets<span class="o">=</span>dev_clean.json --batch_size<span class="o">=</span><span class="m">32</span> --num_epochs<span class="o">=</span><span class="m">15</span> --exp_name<span class="o">=</span>MyLARGE-ASR-EXPERIMENT
</pre></div>
</div>
<p>The command above should trigger 8-GPU training with mixed precision. In the command above various manifests (.json) files are various datasets. Substitute them with the ones containing your data.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>You can pass several manifests (comma-separated) to train on a combined dataset like this: <cite>–train_manifest=/manifests/librivox-train-all.json,/manifests/librivox-train-all-sp10pcnt.json,/manifests/cv/validated.json</cite>. Here it combines 3 data sets: LibriSpeech, Mozilla Common Voice and LibriSpeech speed perturbed.</p>
</div>
</div>
</div>
<div class="section" id="fine-tuning">
<span id="fine-tune"></span><h2>Fine-tuning<a class="headerlink" href="#fine-tuning" title="Permalink to this headline">¶</a></h2>
<p>Training time can be dramatically reduced if starting from a good pre-trained model:</p>
<p>Currently, you can automatically download English QuartzNet15x5, Jasper10x5 and Mandarin QuartzNet 15x5 by passing ‘QuartzNet15x5-En’, ‘JasperNet10x5-En’, or ‘QuartzNet15x5-Zh’ as <code class="docutils literal notranslate"><span class="pre">--asr_model</span></code> parameter in speech2text.py script.</p>
<p>A manual method of doing fine-tuning works like this:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Obtain a pre-trained model (encoder, decoder and configuration files) <a class="reference external" href="https://ngc.nvidia.com/catalog/models/nvidia:quartznet15x5">from here</a>.</p></li>
<li><p>load pre-trained weights right after you’ve instantiated your encoder and decoder, like this:</p></li>
</ol>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">encoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/15x5SEP/JasperEncoder-STEP-247400.pt&quot;</span><span class="p">)</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/15x5SEP/JasperDecoderForCTC-STEP-247400.pt&quot;</span><span class="p">)</span>
<span class="c1"># in case of distributed training add args.local_rank</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="s2">&quot;&lt;path_to_checkpoints&gt;/15x5SEP/JasperDecoderForCTC-STEP-247400.pt&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>When fine-tuning, use smaller learning rate.</p>
</div>
</div>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python &lt;nemo_git_repo_root&gt;/examples/asr/speech2text_infer.py --asr_model<span class="o">=</span>QuartzNet15x5-En --dataset<span class="o">=</span>&lt;path_to_data&gt;/dev-clean.json
</pre></div>
</div>
</div>
<div class="section" id="evaluation-with-language-model">
<h2>Evaluation with Language Model<a class="headerlink" href="#evaluation-with-language-model" title="Permalink to this headline">¶</a></h2>
<div class="section" id="using-kenlm">
<h3>Using KenLM<a class="headerlink" href="#using-kenlm" title="Permalink to this headline">¶</a></h3>
<p>We will be using <a class="reference external" href="https://github.com/PaddlePaddle/DeepSpeech">Baidu’s CTC decoder with LM implementation.</a>.</p>
<p>Perform the following steps:</p>
<blockquote>
<div><ul class="simple">
<li><p>Go to <code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">&lt;nemo_git_repo_root&gt;/scripts</span></code></p></li>
<li><dl class="simple">
<dt>Install Baidu’s CTC decoders (NOTE: no need for “sudo” if inside the container):</dt><dd><ul>
<li><p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">update</span> <span class="pre">&amp;&amp;</span> <span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">swig</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">pkg-config</span> <span class="pre">libflac-dev</span> <span class="pre">libogg-dev</span> <span class="pre">libvorbis-dev</span> <span class="pre">libboost-dev</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">libsndfile1-dev</span> <span class="pre">python-setuptools</span> <span class="pre">libboost-all-dev</span> <span class="pre">python-dev</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">apt-get</span> <span class="pre">install</span> <span class="pre">cmake</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">./install_decoders.sh</span></code></p></li>
</ul>
</dd>
</dl>
</li>
<li><p>Build 6-gram KenLM model on LibriSpeech <code class="docutils literal notranslate"><span class="pre">./build_6-gram_OpenSLR_lm.sh</span></code></p></li>
<li><p>Run jasper_eval.py with the –lm_path flag</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python &lt;nemo_git_repo_root&gt;/examples/asr/jasper_eval.py --model_config<span class="o">=</span>&lt;nemo_git_repo_root&gt;/examples/asr/configs/quartznet15x5.yaml --eval_datasets <span class="s2">&quot;&lt;path_to_data&gt;/dev_clean.json&quot;</span> --load_dir<span class="o">=</span>&lt;directory_containing_checkpoints&gt; --lm_path<span class="o">=</span>&lt;path_to_6gram.binary&gt;
</pre></div>
</div>
</div></blockquote>
</div>
</div>
<div class="section" id="using-and-converting-to-tarred-datasets">
<h2>Using and Converting to Tarred Datasets<a class="headerlink" href="#using-and-converting-to-tarred-datasets" title="Permalink to this headline">¶</a></h2>
<p>If you are training on a distributed cluster, you may want to avoid a dataset consisting of many small files and instead perform batched reads from tarballs.
In this case, you can use the <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code> to load your data.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code> takes in an <code class="docutils literal notranslate"><span class="pre">audio_tar_filepaths</span></code> argument, which specifies the path(s) to the tarballs that contain the audio files, and a <code class="docutils literal notranslate"><span class="pre">manifest_filepath</span></code> argument that should contain the transcripts and durations corresponding to those files (with a unique WAV basename per entry).
The <code class="docutils literal notranslate"><span class="pre">audio_tar_filepaths</span></code> argument can be in the form of a string, either containing a path to a single tarball or braceexpand-able to multiple paths, or a list of paths.
Note that the data layer’s size (via <code class="docutils literal notranslate"><span class="pre">len</span></code>) is set by the number of entries of the manifest, rather than the number of files across all tarballs.</p>
<p>This DataLayer uses <a class="reference external" href="https://github.com/tmbdev/webdataset">WebDataset</a> to read the tarred audio files.
Since reads are performed sequentially, shuffling is done with a buffer which can be specified by the argument <code class="docutils literal notranslate"><span class="pre">shuffle_n</span></code>.</p>
<p>Please see the <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code> <a class="reference external" href="https://nvidia.github.io/NeMo/collections/nemo_asr.html#nemo.collections.asr.data_layer.TarredAudioToTextDataLayer">documentation</a> and the WebDataset documentation for more details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> processes, the <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code> will automatically partition the audio tarballs across workers.
As such, if you are training on <cite>n</cite> workers, please make sure to divide your WAV files evenly across a number of tarballs that is divisible by <cite>n</cite>.</p>
</div>
<div class="section" id="conversion-from-an-existing-dataset-to-tarred-dataset">
<h3>Conversion from an Existing Dataset to Tarred Dataset<a class="headerlink" href="#conversion-from-an-existing-dataset-to-tarred-dataset" title="Permalink to this headline">¶</a></h3>
<p>If you already have an ASR dataset that you would like to convert to one that is compatible with the <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code>, you can use <code class="docutils literal notranslate"><span class="pre">scripts/convert_to_tarred_audio_dataset.py</span></code>.</p>
<p>This script takes a few arguments:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">manifest_path</span></code> (required): The path to your existing dataset’s manifest file.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">target_dir</span></code>: The directory where the tarballs and new manifest will be written. If none if given, defaults to <code class="docutils literal notranslate"><span class="pre">./tarred</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_shards</span></code>: The number of shards (tarballs) to create. If using multiple workers for training, set this to be a multiple of the number of workers you have. Defaults to 1.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">shuffle</span></code>: Setting this flag will shuffle the entries in your original manifest before creating the new dataset. You may want to do this if your original dataset is ordered, since the <code class="docutils literal notranslate"><span class="pre">TarredAudioToTextDataLayer</span></code> cannot shuffle the whole dataset (see <code class="docutils literal notranslate"><span class="pre">shuffle_n</span></code>).</p></li>
</ul>
</div>
</div>
<div class="section" id="kaldi-compatibility">
<h2>Kaldi Compatibility<a class="headerlink" href="#kaldi-compatibility" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">nemo_asr</span></code> collection can also load datasets that are in a Kaldi-compatible format using the <code class="docutils literal notranslate"><span class="pre">KaldiFeatureDataLayer</span></code>.
In order to load your Kaldi-formatted data, you will need to have a directory that contains the following files:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">feats.scp</span></code>, the file that maps from utterance IDs to the .ark files with the corresponding audio data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">text</span></code>, the file that contains a mapping from the utterance IDs to transcripts.</p></li>
<li><p>(Optional) <code class="docutils literal notranslate"><span class="pre">utt2dur</span></code>, the file that maps the utterance IDs to the audio file durations. This is required if you want to filter your audio based on duration.</p></li>
</ul>
<p>Of course, you will also need the .ark files that contain the audio data in the location that <code class="docutils literal notranslate"><span class="pre">feats.scp</span></code> expects.</p>
<p>To load your Kaldi-formatted data, you can simply use the <code class="docutils literal notranslate"><span class="pre">KaldiFeatureDataLayer</span></code> instead of the <code class="docutils literal notranslate"><span class="pre">AudioToTextDataLayer</span></code>.
The <code class="docutils literal notranslate"><span class="pre">KaldiFeatureDataLayer</span></code> takes in an argument <code class="docutils literal notranslate"><span class="pre">kaldi_dir</span></code> instead of a <code class="docutils literal notranslate"><span class="pre">manifest_filepath</span></code>, and this argument should be set to the directory that contains the files mentioned above.
See <a class="reference external" href="https://nvidia.github.io/NeMo/collections/nemo_asr.html#nemo.collections.asr.data_layer.KaldiFeatureDataLayer">the documentation</a> for more detailed information about the arguments to this data layer.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are switching to a <code class="docutils literal notranslate"><span class="pre">KaldiFeatureDataLayer</span></code>, be sure to set any <code class="docutils literal notranslate"><span class="pre">feat_in</span></code> parameters to correctly reflect the dimensionality of your Kaldi features, such as in the encoder. Additionally, your data is likely already preprocessed (e.g. into MFCC format), in which case you can leave out any audio preprocessors like the <code class="docutils literal notranslate"><span class="pre">AudioToMelSpectrogramPreprocessor</span></code>.</p>
</div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-asr/tutorial-0"><dl class="citation">
<dt class="bibtex label" id="asr-tut-graves2006"><span class="brackets"><a class="fn-backref" href="#id2">ASR-TUT1</a></span></dt>
<dd><p>Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In <em>Proceedings of the 23rd international conference on Machine learning</em>, 369–376. ACM, 2006.</p>
</dd>
<dt class="bibtex label" id="asr-tut-kriman2019quartznet"><span class="brackets">ASR-TUT2</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id4">2</a>)</span></dt>
<dd><p>Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang. Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions. <em>arXiv preprint arXiv:1910.10261</em>, 2019.</p>
</dd>
<dt class="bibtex label" id="asr-tut-panayotov2015librispeech"><span class="brackets"><a class="fn-backref" href="#id3">ASR-TUT3</a></span></dt>
<dd><p>Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In <em>Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on</em>, 5206–5210. IEEE, 2015.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="datasets.html" class="btn btn-neutral float-right" title="Datasets" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>