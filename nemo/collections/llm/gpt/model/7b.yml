
{

#NOTE: when using srun_launcher, 
# -DO NOT set deepspeed_slurm to true
# -DO NOT set master_port as this will be generated automatically in the SLURM script
# "use_srun_launcher": true,
# "enable_each_rank_log": false,
# "deepspeed_slurm": false,
# "use_wandb": false,
"hyena_width_expansion": 1,
"proj_groups": 1,
"modal_gamma_min": 0.01,
"modal_gamma_max": 0.1,
"use_depthwise_short_conv_grouping": true,
"hyena_short_conv_postgate": true,
"hyena_short_conv_pregate": true,
"activation": "gelu",

# MP / PP config,
"pipe_parallel_size": 1,
"model_parallel_size": 1,
"make_vocab_size_divisible_by": 8,

# cgcg config
"use_cgcg": false,
"use_cgcg_short": false,
"cgcg_dtype": "bfloat16",

# cgcg fwd kernel config
"cgcg_fwd_autotune": false, # @jeromeku, NOTE: hardcoded to false within hyena.py
"cgcg_medium_fwd_kernel_config_chunk_size": 128,
"cgcg_medium_fwd_kernel_config_block_d": 128,
"cgcg_medium_fwd_kernel_config_threadblock_swizzle": "row",
"cgcg_medium_fwd_kernel_config_chunk_tiles_per_program": 3,
"cgcg_short_fwd_kernel_config_num_warps": 4,
"cgcg_medium_fwd_kernel_config_num_stages": 3,
"cgcg_short_fwd_kernel_config_chunk_size": 128,
"cgcg_short_fwd_kernel_config_block_d": 128,
"cgcg_short_fwd_kernel_config_threadblock_swizzle": "row",
"cgcg_short_fwd_kernel_config_chunk_tiles_per_program": 1,
"cgcg_short_fwd_kernel_config_num_warps": 4,
"cgcg_short_fwd_kernel_config_num_stages": 1,

# cgcg bwd kernel config
"cgcg_bwd_autotune": true,
"cgcg_fused_bwd": true,

#
# "include": "localhost@localhost:0",
# "num_gpus": 1,

# Only needed if not autotuning
"cgcg_bwd_kernel_config_pre_conv_block_x": 128,
"cgcg_bwd_kernel_config_pre_conv_block_y": 128,
"cgcg_bwd_kernel_config_pre_conv_num_warps": 8,
"cgcg_bwd_kernel_config_post_conv_block_x": 32,
"cgcg_bwd_kernel_config_post_conv_block_y": 128,
"cgcg_bwd_kernel_config_post_conv_num_warps": 4,

"hyena_short_conv_len": 7,
"hyena_medium_conv_len": 128,  # default is null

"num_layers": 18,
"hidden_size": 4096,
"num_groups_hyena": 4096,
"num_groups_hyena_medium": 256,
"num_groups_hyena_short": 256,
"num_groups_hyena_mlp": 256,
"num_attention_heads": 32, 
"operator-config": [
  [["hyena_short_conv"], 1],
  [["hyena_medium_conv"], 1], 
  [["hyena"], 1],
  [["flash_v2"], 1],
  [["hyena_short_conv"], 1], 
  [["hyena_medium_conv"], 1], 
  [["hyena"], 1],
  [["hyena_short_conv"], 1], 
  [["hyena_medium_conv"], 1], 
  [["hyena"], 1],
  [["flash_v2"], 1],
  [["hyena_short_conv"], 1], 
  [["hyena_medium_conv"], 1], 
  [["hyena"], 1],
  [["hyena_short_conv"], 1], 
  [["hyena_medium_conv"], 1], 
  [["hyena"], 1],
  [["flash_v2"], 1],
  # [["hyena_short_conv"], 1], 
  # [["hyena_medium_conv"], 1], 
  # [["hyena"], 1],
  # [["hyena_short_conv"], 1], 
  # [["hyena_medium_conv"], 1], 
  # [["hyena"], 1],
  # [["flash_v2"], 1],
  # [["hyena_short_conv"], 1], 
  # [["hyena_medium_conv"], 1], 
  # [["hyena"], 1],
  # [["hyena_short_conv"], 1], 
  # [["hyena_medium_conv"], 1], 
  # [["hyena"], 1],
  # [["flash_v2"], 1],
  ],
"seq_length": 8192,
"max_position_embeddings": 8192,

"log_attn_norms": false,
"pos_emb": "rotary",
# "rotary_emb_base": 1000000,
"rotary_pct": 1,
"prenorm": true,
"postnorm": false,
"pre_mlp_norm": true,  
"outer_mlp_norm": false,
"no_weight_tying": false,
"gpt_j_residual": false,
"normalize_hyena_filters": false,
"short-conv-L": 3, 
"hyena_filter_fast_decay": 0.3,
"hyena_filter_slow_decay": 1.2,
"hyena_filter_w": 14, 
"hyena_filter_cls": "implicit_modal",
"hyena_medium_filter_cls": "explicit_single_decay",  
"explicit_filter_decay_preset": "weak",
# "modal_residue_factors": 5,
# "modal_pole_factors": 1,
"hyena_filter_order": 16,
"hyena_filter_wd": 0.,
"use_fast_heads": false,
"use_slow_heads": false,
"use-hyena-filter": true,
"output_layer_parallelism": "column",
"bias_dropout_fusion": false,
"norm": "rmsnorm",
"rms_norm_epsilon": 1.0e-6,
"identity_mlp": false,
  "mlp_type": "llama",
  "scaled-upper-triang-masked-softmax-fusion": true,
  "bias-gelu-fusion": false,
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.0003,
      "betas": [0.9, 0.95],
      "eps": 1.0e-8,
    }
  },
  "min_lr": 0.00003,
  "zero_optimization": {
  "stage": 1,
  "allgather_partitions": True,
  "allgather_bucket_size": 500000000,
  "overlap_comm": True,
  "reduce_scatter": True,
  "reduce_bucket_size": 500000000,
  "contiguous_gradients": True,
  "cpu_offload": false
},
  "train_micro_batch_size_per_gpu": 2,
  "gradient_accumulation_steps": 1,
  "data-impl": "mmap",
  "checkpoint-activations": true,
  "checkpoint-num-layers": 4,
  "partition-activations": true,
  "synchronize-each-layer": false,
  "gradient_clipping": 1.0,
  "weight-decay": 0.1,
  "hidden-dropout": 0.0,
  "attention-dropout": 0.0,
  "precision": "bfloat16",
  "bf16": {
  "enabled": true
  },
  "train-iters": 20,
  "lr-decay-iters": 20,
  "distributed-backend": "nccl",
  "lr-decay-style": "cosine",
  "warmup": 0.005,
  "checkpoint-factor": 2500,
"extra_save_iters": [4],
  "eval-interval": 200,
  "eval-iters": 20,
  "log-interval": 5,
  "steps_per_print": 5,
  "keep-last-n-checkpoints": 100,
  "wall_clock_breakdown": false,
  # "save": "/home/zymrael/checkpoints/evo2/evo2_test", # change this
  "tokenizer_type": CharLevelTokenizer,   
   "use_fp8_input_projections": false,
   "use_fp8_output_projections": false,
   "use_fp8_mlp_projections": false,
   "use_fp8_norm": false,
  "checkpoint_strict_load": false,

"make_gated_mlp_multiple_of": 128,
"materialize_attn_mask": false,  # default false, to save memory
"fast_conv_proj": true,   

"to_upper": "weighted",
"lowercase_loss_reweighting": 0.1,  
"hyena_mlp_len": 7,  
"log_memory_stats": true

}