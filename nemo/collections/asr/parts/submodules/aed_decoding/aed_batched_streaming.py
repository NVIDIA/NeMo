# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

from abc import ABC
from dataclasses import dataclass

import torch

from nemo.collections.asr.models.aed_multitask_models import EncDecMultiTaskModel, lens_to_mask
from nemo.collections.asr.parts.submodules.multitask_decoding import AEDStreamingDecodingConfig
from nemo.collections.asr.parts.utils.streaming_utils import ContextSize
from nemo.utils import logging


@dataclass
class AEDStreamingState:
    decoder_input_ids: torch.Tensor = None  # tokens ids of initial AED model prompt
    tgt: torch.Tensor = None  # buffer with deocoded tokens ids
    decoding_step: int = -1  # current decoding step
    decoder_mems_list: list = None  # decoder caches, helps to reduce the memory usage
    is_last_chunk_batch: torch.Tensor = False  # whether the current chunk is the last speech chunk in the audio
    max_generation_length: int = (
        512  # maximum number of tokens to be generated for each sample (can be bigger for long audio)
    )
    max_tokens_per_one_second: int = 10  # maximum number of tokens to be generated per one second of audio
    max_tokens_per_alignatt_step: int = (
        None  # maximum number of tokens to be generated for each step of alignatt decoding policy
    )
    use_avgpool_for_alignatt: bool = True  # use avgpooling for alignatt decoding policy
    tokens_frame_alignment: torch.Tensor = (
        None  # frame alignment of the predicted tokens (used for LAAL calculation in alignatt)
    )
    prev_encoder_shift: int = 0  # previous encoder shift (used for LAAL calculation in alignatt)
    device: torch.device = None


class GreedyBatchedStreamingAEDComputer(ABC):
    """
    Batched streaming AED decoding with support for waitk and alignatt decoding policies.
    """

    def __init__(
        self,
        asr_model: EncDecMultiTaskModel,
        frame_chunk_size: int,
        decoding_cfg: AEDStreamingDecodingConfig,
    ):
        """
        Init method.
        Args:
            asr_model: isntace of ASR model (Canary)
            frame_chunk_size: size of the frame chunk
            decoding_cfg: decoding configuration
        """
        super().__init__()

        self.asr_model = asr_model
        self.frame_chunk_size = frame_chunk_size
        self.decoding_cfg = decoding_cfg
        self.state = AEDStreamingState()

    def __call__(
        self,
        encoder_output: torch.Tensor,
        encoder_output_len: torch.Tensor,
        prev_batched_state: AEDStreamingState,
    ) -> AEDStreamingState:

        self.state = prev_batched_state
        self.state.encoder_output_len = encoder_output_len

        # prepare encoder embeddings for the decoding
        # enc_states = encoder_output.permute(0, 2, 1)
        encoded_speech = self.asr_model.encoder_decoder_proj(encoder_output)

        encoder_input_mask = lens_to_mask(encoder_output_len, encoded_speech.shape[1]).to(encoded_speech.dtype)

        # initiall waitk lagging. Applicable for Wait-k and AlignAtt decoding policies. Control the start of the decoding process.
        if encoder_output_len.max() // self.frame_chunk_size < self.decoding_cfg.waitk_lagging and torch.any(
            torch.logical_not(self.state.is_last_chunk_batch)
        ):
            # need to wait for more speech
            return self.state

        # wait-k streaming decoding policy
        elif self.decoding_cfg.streaming_policy == "waitk":
            self.run_waitk_decoding_step(encoded_speech, encoder_input_mask)
        # alignatt streaming decoding policy
        elif self.decoding_cfg.streaming_policy == "alignatt":
            self.run_alignatt_decoding_step(encoded_speech, encoder_input_mask)
        else:
            raise ValueError("Canary streaming decoding supports only alignatt or waitk decodong policy")

        return self.state

    def run_waitk_decoding_step(self, encoded_speech, encoder_input_mask):
        """
        Run a decoding step for waith streaming policy.
        """
        if self.state.decoding_step < 0:
            # first decoding step
            tgt, batch_size, _ = self.asr_model.decoding.decoding.greedy_search._prepare_for_search(
                self.state.decoder_input_ids,
                encoded_speech,
            )
            input_ids = tgt
        else:
            input_ids = self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths - 1].unsqueeze(-1)

        self.state.active_samples_inner_loop = (
            torch.ones(self.state.batch_size, dtype=torch.bool, device=self.state.device) * self.state.active_samples
        )
        decoder_mems_list = self.state.decoder_mems_list

        # define start and max generation lengths
        start_from = self.state.decoding_step + 1
        if torch.any(torch.logical_not(self.state.is_last_chunk_batch)):
            # predict only one token per speech chunk if not the last one
            max_generation_length = start_from + 1
        else:
            max_generation_length = self.decoding_cfg.max_generation_length

        # inner deocding loop (with same speech chunk)
        for i in range(start_from, max_generation_length):

            if not decoder_mems_list:
                positional_indexes = torch.zeros_like(self.state.current_context_lengths)
            else:
                positional_indexes = self.state.current_context_lengths - 1

            logits, decoder_mems_list, xatt_scores_list = (
                self.asr_model.decoding.decoding.greedy_search._one_step_forward(
                    input_ids,
                    encoded_speech,
                    encoder_input_mask,
                    decoder_mems_list,
                    positional_indexes,
                    return_scores=False,
                    return_xatt_scores=True,
                )
            )
            next_tokens = torch.argmax(logits[:, -1], dim=-1)

            # compute eos tokens mask
            is_eos_tokens = next_tokens == self.asr_model.tokenizer.eos
            # rearange active samples (inner loop) depends on eos prediction
            self.state.active_samples_inner_loop *= torch.logical_not(is_eos_tokens)
            # disable samples (upper loop) with eos and end of speech
            eos_and_end_speech_mask = is_eos_tokens * self.state.is_last_chunk_batch
            self.state.active_samples = self.state.active_samples * torch.logical_not(eos_and_end_speech_mask)

            if not torch.any(self.state.active_samples_inner_loop):
                break

            # write predicted tokens to the tgt tensor
            torch.where(self.state.active_samples_inner_loop, next_tokens, self.state.eos_tokens, out=next_tokens)
            self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths] = next_tokens

            self.state.decoding_step += input_ids.size(-1)

            # check for hallucinations
            if self.decoding_cfg.hallucinations_detector:
                hallucination_mask = self.detect_hallucinations(
                    self.state.tgt, self.state.batch_idxs, self.state.current_context_lengths
                )
                if torch.any(hallucination_mask):
                    self.state.active_samples *= torch.logical_not(hallucination_mask)
                    self.state.active_samples_inner_loop *= torch.logical_not(hallucination_mask)

            self.state.current_context_lengths += self.state.active_samples_inner_loop
            input_ids = self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths - 1].unsqueeze(-1)

            # disable samples with maximum context length
            samples_with_max_context_length = (
                self.state.current_context_lengths == self.decoding_cfg.max_generation_length - 1
            )
            if torch.any(samples_with_max_context_length * self.state.active_samples):
                logging.info(f"!!! maximum context length reached !!!")
                self.state.active_samples *= torch.logical_not(samples_with_max_context_length)
                self.state.active_samples_inner_loop *= torch.logical_not(samples_with_max_context_length)

            # zero out decoder_mems_list for non active samples
            if torch.any(torch.logical_not(self.state.active_samples_inner_loop)):
                for j in range(len(decoder_mems_list)):
                    decoder_mems_list[j][:, -1] *= self.state.active_samples_inner_loop.unsqueeze(-1)
            self.state.decoder_mems_list = decoder_mems_list

    def run_alignatt_decoding_step(self, encoded_speech, encoder_input_mask):
        """
        Run a decoding step for alignatt streaming policy.
        """
        if self.state.decoding_step < 0:
            # first decoding step
            tgt, batch_size, _ = self.asr_model.decoding.decoding.greedy_search._prepare_for_search(
                self.state.decoder_input_ids,
                encoded_speech,
            )
            input_ids = tgt
            start_from = 0
        else:
            input_ids = self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths - 1].unsqueeze(-1)
            start_from = torch.min(self.state.current_context_lengths).item() - 1

        decoder_mems_list = self.state.decoder_mems_list
        self.state.steps_per_inner_loop = torch.zeros(
            self.state.batch_size, dtype=torch.long, device=self.state.device
        )
        self.state.active_samples_inner_loop = (
            torch.ones(self.state.batch_size, dtype=torch.bool, device=self.state.device) * self.state.active_samples
        )

        for i in range(start_from, self.state.max_generation_length):
            # prepare positional indexes offset for attention decoder
            if not decoder_mems_list:
                positional_indexes = torch.zeros_like(self.state.current_context_lengths)
            else:
                positional_indexes = self.state.current_context_lengths - 1

            logits, decoder_mems_list, xatt_scores_list = (
                self.asr_model.decoding.decoding.greedy_search._one_step_forward(
                    input_ids,
                    encoded_speech,
                    encoder_input_mask,
                    decoder_mems_list,
                    positional_indexes,
                    return_scores=False,
                    return_xatt_scores=True,
                )
            )
            next_tokens = torch.argmax(logits[:, -1], dim=-1)

            # compute the most attended encoder token
            xatt_scores = xatt_scores_list[self.decoding_cfg.xatt_scores_layer]
            xatt_scores = torch.mean(xatt_scores, 1)
            if i == 0 and xatt_scores.shape[-1] <= self.decoding_cfg.exclude_sink_frames:
                exclude_sink_frames = xatt_scores.shape[-1] // 2
            else:
                exclude_sink_frames = (
                    self.decoding_cfg.exclude_sink_frames if self.state.prev_encoder_shift == 0 else 0
                )
            most_attended_idxs = torch.argmax(xatt_scores[:, :, exclude_sink_frames:], dim=-1) + exclude_sink_frames

            # we can try to smooth peaky xatt scores with avgpooling
            if self.decoding_cfg.use_avgpool_for_alignatt:
                average_pooling_xatt_scores = self.state.avgpool2d(xatt_scores[:, :, exclude_sink_frames:])
                most_attended_idxs_avgpool = torch.argmax(average_pooling_xatt_scores, dim=-1) + exclude_sink_frames
                most_attended_idxs = most_attended_idxs_avgpool

            # select the last attended token for each sample
            if most_attended_idxs.size(-1) > 1:
                most_attended_idxs = most_attended_idxs[:, -1]
            else:
                most_attended_idxs = most_attended_idxs.squeeze(-1)

            # aligatt condition (True -- continue decoding, False -- wait for more speech)
            alignatt_condition = (
                self.state.encoder_output_len - (most_attended_idxs + 1) >= self.decoding_cfg.alignatt_thr
            )
            # alignatt condition is always True for the last speech chunk
            alignatt_condition += self.state.is_last_chunk_batch

            # applay alignatt condition for inner loop
            self.state.active_samples_inner_loop *= alignatt_condition

            # increase speech chunk if no active samples in the inner loop
            if not torch.any(self.state.active_samples_inner_loop):
                break

            # compute eos tokens mask
            # TODO add a case of "." + EOS prediction for models with PC support?
            is_eos_tokens = next_tokens == self.asr_model.tokenizer.eos
            # rearange active samples (inner loop) depends on eos prediction
            self.state.active_samples_inner_loop *= torch.logical_not(is_eos_tokens)
            # disable samples (upper loop) with eos and end of speech
            eos_and_end_speech_mask = is_eos_tokens * self.state.is_last_chunk_batch
            self.state.active_samples *= torch.logical_not(eos_and_end_speech_mask)

            if not torch.any(self.state.active_samples_inner_loop):
                break

            # write predicted tokens to the tgt tensor
            torch.where(self.state.active_samples_inner_loop, next_tokens, self.state.eos_tokens, out=next_tokens)
            self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths] = next_tokens

            # update tokens frame alignment based on current encoder step (this alignment is used for LAAL calculation)
            self.state.tokens_frame_alignment[self.state.batch_idxs, self.state.current_context_lengths] = (
                self.state.encoder_output_len
                + self.state.prev_encoder_shift  # we need to add the real frame position in the audio signal
            )

            self.state.decoding_step += input_ids.size(-1)

            # check for hallucinations
            if self.decoding_cfg.hallucinations_detector:
                hallucination_mask = self.detect_hallucinations(
                    self.state.tgt, self.state.batch_idxs, self.state.current_context_lengths
                )
                if torch.any(hallucination_mask):
                    self.state.active_samples *= torch.logical_not(hallucination_mask)
                    self.state.active_samples_inner_loop *= torch.logical_not(hallucination_mask)

            # disable samples with maximum context length
            samples_with_max_context_length = (
                self.state.current_context_lengths == self.state.max_generation_length - 1
            )
            if torch.any(samples_with_max_context_length * self.state.active_samples):
                logging.info(f"!!! maximum context length reached !!!")
                self.state.active_samples *= torch.logical_not(samples_with_max_context_length)
                self.state.active_samples_inner_loop *= torch.logical_not(samples_with_max_context_length)

            # zero out decoder_mems_list for non active samples
            # TODO batched decoding works wrong if first token was EOS for one of the samples
            if torch.any(torch.logical_not(self.state.active_samples_inner_loop)):
                for j in range(len(decoder_mems_list)):
                    decoder_mems_list[j][:, -1] *= self.state.active_samples_inner_loop.unsqueeze(-1)

            self.state.decoder_mems_list = decoder_mems_list
            self.state.current_context_lengths += self.state.active_samples_inner_loop
            # TODO model does not predicts any real tokens in the case of first EOS prediction (rare case for batched decoding)
            input_ids = self.state.tgt[self.state.batch_idxs, self.state.current_context_lengths - 1].unsqueeze(-1)

            # limit number of steps per inner loop if not end of speech
            if self.state.max_tokens_per_alignatt_step is not None:
                self.state.steps_per_inner_loop += self.state.active_samples_inner_loop
                disable_samples_mask = self.state.steps_per_inner_loop >= self.state.max_tokens_per_alignatt_step
                disable_samples_mask *= torch.logical_not(self.state.is_last_chunk_batch)
                self.state.active_samples_inner_loop *= torch.logical_not(disable_samples_mask)

            if not torch.any(self.state.active_samples_inner_loop):
                break

    def detect_hallucinations(self, tgt, batch_idxs, current_context_lengths):

        ccl = current_context_lengths
        # pattern 1: four consequtive tokens are the same: "a a a a"
        hallucination_mask_1 = (
            (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 1])
            * (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 2])
            * (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 3])
            * (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 4])
        )
        if torch.any(hallucination_mask_1):
            logging.info(f"!!! hallucination 'a a a a' detected !!!")
        # pattern 2: "a b a b a b"
        hallucination_mask_2 = (
            (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 2])
            * (tgt[batch_idxs, ccl - 1] == tgt[batch_idxs, ccl - 3])
            * (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 4])
            * (tgt[batch_idxs, ccl - 1] == tgt[batch_idxs, ccl - 5])
        )
        if torch.any(hallucination_mask_2):
            logging.info(f"!!! hallucination 'a b a b a b' detected !!!")
        # pattern 3: "a b c a b c a b c"
        hallucination_mask_3 = (
            (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 3])
            * (tgt[batch_idxs, ccl - 1] == tgt[batch_idxs, ccl - 4])
            * (tgt[batch_idxs, ccl - 2] == tgt[batch_idxs, ccl - 5])
            * (tgt[batch_idxs, ccl] == tgt[batch_idxs, ccl - 6])
            * (tgt[batch_idxs, ccl - 1] == tgt[batch_idxs, ccl - 7])
            * (tgt[batch_idxs, ccl - 2] == tgt[batch_idxs, ccl - 8])
        )
        if torch.any(hallucination_mask_3):
            logging.info(f"!!! hallucination 'a b c a b c a b c' detected !!!")
        hallucination_mask = hallucination_mask_1 + hallucination_mask_2 + hallucination_mask_3
        return hallucination_mask

    def compute_laal(self, delays, source_length, target_length):
        if delays[0] > source_length:
            return delays[0]
        LAAL = 0
        gamma = max(len(delays), target_length) / source_length
        tau = 0
        for t_minus_1, d in enumerate(delays):
            LAAL += d - t_minus_1 / gamma
            tau = t_minus_1 + 1
            if d >= source_length:
                break
        LAAL /= tau
        return LAAL

    def compute_alignatt_lagging(
        self,
        records,
        predicted_token_ids,
        tokens_frame_alignment,
        context_encoder_frames,
        audio_encoder_fs,
        BOW_PREFIX="\u2581",
    ):
        # import ipdb; ipdb.set_trace()
        tokens_idx_shift = self.state.decoder_input_ids.size(-1)
        target_length_word = [len(item['text'].split()) for item in records]
        audio_signal_lengths = [float(item['duration']) * 1000 for item in records]
        # import ipdb; ipdb.set_trace()
        tokenizer_vocab = self.asr_model.tokenizer.vocab
        eos_token = tokenizer_vocab[self.asr_model.tokenizer.eos_id]
        laal_list = []
        for i, tokens in enumerate(predicted_token_ids):
            if len(tokens) == 0:
                laal_list.append(5000)
                continue
            audio_signal_length = audio_signal_lengths[i]
            # obtain lagging for alignatt
            lagging = []
            for j, cur_t in enumerate(tokens):
                pred_idx = (
                    tokens_frame_alignment[i][tokens_idx_shift + j] + context_encoder_frames.right
                )  # TODO: check right_context
                cur_t = tokenizer_vocab[cur_t.item()]
                if (cur_t.startswith(BOW_PREFIX) and cur_t != BOW_PREFIX) or cur_t == eos_token:  # word boundary
                    lagging.append(pred_idx * audio_encoder_fs)
                if cur_t == eos_token:
                    break
            if len(lagging) == 0:
                lagging.append(0)
            laal = self.compute_laal(lagging, audio_signal_length, target_length_word[i])
            if torch.is_tensor(laal):
                laal_list.append(laal.item())
            else:
                laal_list.append(laal)
        return laal_list

    def compute_waitk_lagging(
        self, records, predicted_token_ids, context_encoder_frames, audio_encoder_fs, BOW_PREFIX="\u2581"
    ):
        waitk_lagging = self.decoding_cfg.waitk_lagging
        pre_decision_ratio = context_encoder_frames.chunk
        target_length_word = [len(item['text'].split()) for item in records]
        audio_signal_lengths = [float(item['duration']) * 1000 for item in records]
        tokenizer_vocab = self.asr_model.tokenizer.vocab
        laal_list = []
        for i, tokens in enumerate(predicted_token_ids):
            lagging = []
            audio_signal_length = audio_signal_lengths[i]
            for j, cur_t in enumerate(tokens):
                cur_src_len = (j + waitk_lagging) * pre_decision_ratio + context_encoder_frames.right
                cur_src_len *= audio_encoder_fs  # to ms
                cur_src_len = min(audio_signal_length, cur_src_len)
                spm = tokenizer_vocab[cur_t.item()]
                # reach word boundary
                if (
                    spm.startswith(BOW_PREFIX) and spm != BOW_PREFIX
                ) or cur_t == self.asr_model.tokenizer.eos_id:  # word boundary
                    lagging.append(cur_src_len)
                if cur_t == self.asr_model.tokenizer.eos_id:
                    break
            if len(lagging) == 0:
                lagging.append(0)
            laal = self.compute_laal(lagging, audio_signal_length, target_length_word[i])
            laal_list.append(laal)
        return laal_list


def initialize_aed_model_state(
    asr_model,
    decoder_input_ids: torch.Tensor,
    batch_size: int,
    context_encoder_frames: ContextSize,
    chunk_secs: float,
    right_context_secs: float,
) -> AEDStreamingState:
    """
    Initialize AED model state for streaming inference.

    Args:
        asr_model: ASR model instance (used for tokenizer and device)
        decoder_input_ids: Prompt tensor for decoder input
        batch_size: Batch size for inference
        context_encoder_frames: Context size configuration

    Returns:
        Initialized AEDStreamingState object
    """
    # initialize AED model state
    model_state = AEDStreamingState(decoder_input_ids=decoder_input_ids, device=asr_model.device)

    model_state.frame_chunk_size = context_encoder_frames.chunk
    model_state.batch_idxs = torch.arange(batch_size, dtype=torch.long, device=asr_model.device)
    model_state.current_context_lengths = torch.zeros_like(model_state.batch_idxs) + decoder_input_ids.size(-1)
    model_state.decoder_input_ids = decoder_input_ids[:batch_size]
    model_state.tgt = torch.full(
        [batch_size, model_state.max_generation_length],
        asr_model.tokenizer.eos,
        dtype=torch.long,
        device=asr_model.device,
    )
    model_state.tgt[:, : model_state.decoder_input_ids.size(-1)] = model_state.decoder_input_ids
    model_state.tokens_frame_alignment = torch.zeros_like(model_state.tgt)
    model_state.active_samples = torch.ones(batch_size, dtype=torch.bool, device=asr_model.device)
    model_state.active_samples_inner_loop = torch.ones(batch_size, dtype=torch.bool, device=asr_model.device)
    model_state.right_context = context_encoder_frames.right
    model_state.eos_tokens = torch.full(
        [batch_size], asr_model.tokenizer.eos, dtype=torch.long, device=asr_model.device
    )
    model_state.avgpool2d = torch.nn.AvgPool2d(5, stride=1, padding=2, count_include_pad=False)
    model_state.batch_size = batch_size
    model_state.max_tokens_per_alignatt_step = model_state.max_tokens_per_one_second * int(
        chunk_secs + right_context_secs
    )

    return model_state
