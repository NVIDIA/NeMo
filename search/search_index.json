{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NVIDIA NeMo","text":"<p>Nvidia NeMo Toolkit</p>"},{"location":"blogs/","title":"Research Notes","text":""},{"location":"blogs/template/","title":"Notes","text":"<ul> <li>Add folder inside <code>blogs/posts</code> directory with year</li> <li>Copy the contents of this template to the folder.</li> <li>Edit the contents.</li> <li>Add a <code>&lt;!-- more --&gt;</code> tag to the blog post to indicate where it should say 'Continue reading' in the blogpost preview.</li> <li>Send PR and merge.</li> <li>ASSETS: All blog images and external content must be hosted somewhere else. Do NOT add things to github for blog contents!</li> </ul>"},{"location":"blogs/template/#blog-post","title":"Blog Post","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras in massa et lacus consectetur maximus. Donec fringilla, justo vitae condimentum feugiat, est sapien interdum purus, vel rutrum neque ex quis ipsum. Etiam in mauris odio. </p> <p>Mauris in mattis massa. Vivamus tempor libero eu ante aliquet tempor. Vestibulum porttitor odio eu ante posuere, sit amet sagittis quam auctor. Nunc sem sem, ultrices eget porta ac, vulputate non nibh. In tempor risus non felis porta, id scelerisque eros interdum.</p>"},{"location":"blogs/2022/2022-08-introduction/","title":"NeMo Blog Posts and Announcements","text":"<p>NVIDIA NeMo is a conversational AI toolkit that supports multiple domains such as Automatic Speech Recognition (ASR), Text to Speech generation (TTS), Speaker Recognition (SR), Diarization (SDR), Natural Language Processing (NLP), Neural Machine translation (NMT) and much more. NVIDIA RIVA has long been the toolkit that enables efficient deployment of NeMo models. In recent months, NeMo Megatron supports training and inference on large language models (upto 1 trillion parameters !).</p> <p>As NeMo becomes capable of more advanced tasks, such as p-tuning / prompt tuning of NeMo Megatron models, domain adaptation of ASR models using Adapter modules, customizable generative TTS models and much more, we introduce this website as a collection of blog posts and announcements for:</p> <ul> <li>Technical deep dives of NeMo's capabilities</li> <li>Presenting state-of-the-art research results</li> <li>Announcing new capabilities and domains of research that our team will work on.</li> </ul> <p>Visit NVIDIA NeMo to get started</p>"},{"location":"blogs/2022/2022-nvidia-technical-blog/","title":"NeMo on the NVIDIA Technical blog in 2022","text":"<p>The following blog posts were published by the NeMo team on the NVIDIA Technical blog in 2022.</p>"},{"location":"blogs/2022/2022-nvidia-technical-blog/#august-2022","title":"August 2022","text":"<ul> <li>Solving Automatic Speech Recognition Deployment Challenges</li> </ul>"},{"location":"blogs/2022/2022-nvidia-technical-blog/#september-2022","title":"September 2022","text":"<ul> <li>Simplifying Model Development and Building Models at Scale with PyTorch Lightning and NGC</li> </ul> <p>Based on work accepted to Interspeech 2022:</p> <ul> <li>Improving Japanese Language ASR by Combining Convolutions with Attention Mechanisms</li> <li>Changing CTC Rules to Reduce Memory Consumption in Training and Decoding</li> <li>Dynamic Scale Weighting Through Multiscale Speaker Diarization</li> <li>Text Normalization and Inverse Text Normalization with NVIDIA NeMo</li> </ul>"},{"location":"blogs/2022/2022-nvidia-technical-blog/#october-2022","title":"October 2022","text":"<ul> <li>Building an Automatic Speech Recognition Model for the Kinyarwanda Language</li> <li>Making an NVIDIA Riva ASR Service for a New Language</li> </ul>"},{"location":"blogs/2022/2022-nvidia-technical-blog/#november-2022","title":"November 2022","text":"<ul> <li>Deploying a 1.3B GPT-3 Model with NVIDIA NeMo Megatron</li> <li>Speech Recognition: Generating Accurate Domain-Specific Audio Transcriptions Using NVIDIA Riva</li> </ul>"},{"location":"blogs/2022/2022-nvidia-technical-blog/#december-2022","title":"December 2022","text":"<ul> <li>Training and Tuning Text-to-Speech with NVIDIA NeMo and W&amp;B</li> <li>Deep Learning is Transforming ASR and TTS Algorithms</li> </ul>"},{"location":"blogs/2023/2023-06-07-fast-conformer/","title":"Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition","text":"<p>The Conformer architecture, introduced by Gulati et al. has been a standard architecture used for not only Automatic Speech Recognition, but has also been extended to other tasks such as Spoken Language Understanding, Speech Translation, and used as a backbone for Self Supervised Learning for various downstream tasks. While they are highly accurate models on each of these tasks, and can be extended for use in other tasks, they are also very computationally expensive. This is due to the quadratic complexity of the attention mechanism, which makes it difficult to train and infer on long sequences, which are used as input to these models due to the granular stride of audio pre-processors (commonly Mel Spectrograms or even raw audio signal in certain models with 10 milliseconds stride). Furthermore, the memory requirement of quadratic attention also significantly limits the audio duration during inference.</p> <p>In this paper, we introduce the Fast Conformer architecture, which applies simple changes to the architecture to significantly reduce the computational cost of training and inference, all while mantaining the strong results of the original Conformer model. We further show that by modifying (on the fly) the global attention module to a linearly scalable attention mechanism - the same model can be used to train (or finetune) and then infer on long sequences (up to 1 hour !).</p> <p>Please refer to the paper here: Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#fast-conformer-architecture","title":"Fast Conformer: Architecture","text":"<p>We propose 4 changes to the original Conformer architecture to make it more efficient:</p> <p>1) Downsampling module: The original Conformer paper uses a stack of 2-D Convolutions with a large number of output filters to perform the downsampling in order to reduce the resolution of the incoming audio frames from 10 ms to 40 ms, which makes it more tractable for the subsequent attention layers to operate on. However, these convolutions amount to roughly 20 % of the entire time required to perform a single forward pass of the \"Large\" Conformer (with 120 M parameters). Furthermore, due to the quadratic attention cost, we can obtain a 4x reduction in all subsequent attention layers by downsampling the audio to 80 ms frames. So as the first change, we perform 8x downsampling before any of the Conformer layers are applied. This reduces GMACs to roughly 65% of the baseline Conformer.</p> <p>2) Depthwise convolution: Multiple other works have shown that it is not necessary to use full Convolution layers and that we can save both compute and memory simply by using Depthwise Separable Convoltions. Therefore, we replace all Convolution layers in the preprocessor by Depthwise Seperable Convolutions. This reduces GMACs to roughly 37% of the baseline Conformer.</p> <p>3) Channel reduction: In literature, it is common for the hidden dimension of the downsampling module to match the <code>d_model</code> of the Conformer module for easy application to the subsequent stack of Conformer modules. However, this is not necessary, and we can reduce the number of channels in the downsampling module to reduce the number of parameters and GMACs. We reduce the number of channels to 256, which reduces GMACs to roughly 33% of the baseline Conformer.</p> <p>4) Kernel size reduction: Finally, as we have performed 8x downsampling of the incoming audio, it is no longer required to use the rather large kernel size of 31 in the Convolution layers of the Conformer block. We find that we can roughly reduce it to 9, which preserves the same accuracy while executing slightly faster and reducing the memory cost. This finally reduces GMACs to roughly 33% of the baseline Conformer.</p> <p>Below, we tabulate the accuracy vs speed for each component of Fast Conformer modification schema. Models were tested on LibriSpeech test-other incrementally for each modification starting from the original Conformer. Encoder inference speed (samples/sec) was measured with batch size 128 on 20 sec audio samples. The number of parameters (M) is shown for the encoder only. </p> Encoder WER, Test Other % Inference Samples / sec Params (M) GMACS Baseline Conformer 5.19 169 115 143.2   +8X Stride 5.11 303 115 92.5   +Depthwise conv 5.12 344 111 53.2    +256 channels 5.09 397 109 48.8     +Kernel 9 4.99 467 109 48.7"},{"location":"blogs/2023/2023-06-07-fast-conformer/#fast-conformer-linearly-scalable-attention","title":"Fast Conformer: Linearly Scalable Attention","text":"<p>On an NVIDIA A100 GPU with 80 GB of memory, we find that Conformer reaches the memory limit at around 10-minute long single audio clip. This mean that it is not feasible to perform inference without performing streaming inference which may lead to degraded results.</p> <p>Fast Conformer, due to its 8x stride fairs a little better and can perform inference on roughly 18-minute long audio clips. However, this is still not sufficient for many use cases.</p> <p>We therefore extend Longformer to the Conformer architecture. Longformer uses local attention augmented with global tokens.  We use a single global attention token, which attends to all other tokens and has all other tokens attend to it, using a separate set of query, key and value linear projections, while others attend in a fixed-size window surrounding each token (see below). </p> <p>By switching to limited context attention, we extend the maximum duration that the model can process at once on a single A100 GPU by ~4x: from 10 minutes for Conformer to 18 minutes for Fast Conformer. Furthermore, you can use a pre-trained Fast Conformer model and readily convert its attention to Longformer attention without any further training ! While this will not use the global attention token, it will still be able to process 70-minute long audio clips.`</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#checkpoints","title":"Checkpoints","text":"<p>We provide checkpoints for multiple languages on HuggingFace and will add more when we support other languages or tasks.</p> <p>Each of these models are \"Punctuation and Capitalization\" enabled - meaning that they can be used to perform ASR and Punctuation and Capitalization (PnC) in a single pass and can produce text that is more natural to read. Post-processing to normalize text will be provided in a future release.</p> <p>Some languages we currently support for ASR are :</p> <ul> <li>English</li> <li>French</li> <li>German</li> <li>Italian</li> <li>Spanish</li> <li>Belarusian</li> <li>Croatian</li> <li>Polish</li> <li>Ukranian</li> <li>Russian</li> </ul>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#data-processing","title":"Data Processing","text":"<p>When constructing datasets with support for Punctuation and Capitalization, dataset preparation is an important piece of the puzzle. When training a model with Punctuation and Capitalization, you may face the following issues:</p> <p>1) During training, there may be the case the standard evaluation benchmarks do not have punctuation or capitalization, but the model still predicts them, providing an incorrect evaluation of model training. </p> <p>2) Not all training data may have punctuation or capitalization, so you may want to filter out such samples to prevent confusing the model about whether it should predict punctuation or capitalization.</p> <p>In order to provide a consistent and reproducible way to process such dataset, we will begin providing the dataset preprocessing strategies in Speech Data Processor.</p> <p>Speech Dataset Processor currently hosts dataset processing recipies for Spanish, and we will add more languages in the future.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#usage","title":"Usage","text":"<p>Fast Conformer can be instantiated and used with just a few lines of code when the NeMo ASR library is installed.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#global-attention","title":"Global Attention","text":"<p>For global attention on modest files (upto 15-18 minutes on an A100), you can perform the following steps :</p> <pre><code>from nemo.collections.asr.models import ASRModel\n\nmodel = ASRModel.from_pretrained(\"nvidia/stt_en_fastconformer_hybrid_large_pc\")\nmodel.transcribe([\"&lt;path to a audio file&gt;.wav\"])  # ~10-15 minutes long!\n</code></pre>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#local-attention","title":"Local Attention","text":"<p>Coming in NeMo 1.20, you can easily modify the attention type to local attention after building the model. Then you can also apply audio chunking for the subsampling module to perform inference on huge audio files!</p> <p>For local attention on huge files (upto 11 hours on an A100), you can perform the following steps :</p> <pre><code>from nemo.collections.asr.models import ASRModel\n\nmodel = ASRModel.from_pretrained(\"nvidia/stt_en_fastconformer_hybrid_large_pc\")\n\nmodel.change_attention_model(\"rel_pos_local_attn\", [128, 128])  # local attn\n\n# (Coming in NeMo 1.20)\nmodel.change_subsampling_conv_chunking_factor(1)  # 1 = auto select\n\nmodel.transcribe([\"&lt;path to a huge audio file&gt;.wav\"])  # 10+ hours !\n</code></pre>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#results","title":"Results","text":"<p>By performing the simple modifications in Fast Conformer, we obtain strong scores throughout multiple speech tasks as shown below, all while having more efficient training and inference.</p> <p>For ASR alone, we obtain a 2.8x speedup as compared to Conformer encoder of similar size for inference, and can use larger batch sizes during training to further speedup training. We also compare results against tasks such as Speech Translation and Spoken Language Understanding in order to validate that these changes lead only to improvements in efficiency and do not degrade accuracy on downstream tasks.</p>"},{"location":"blogs/2023/2023-06-07-fast-conformer/#asr-results","title":"ASR Results","text":"<p>Below, we list some of our results on Fast Conformer on LibriSpeech test-other. We compare against the original Conformer and other recent efficient architectures. We compare against the Efficient Conformer from Burchi2021 which uses a progressive downsampling of the Conformer architecture. We also compare against Kim2022 SqueezeFormer which uses a U-Net like architecture to progressively downsample the input and upsample it to 40 ms resolution prior to applying the decoder. </p> <p>We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being 2.8x faster and using fewer parameters. </p> <p>Fast Conformer-Large with CTC and RNNT decoders trained on Librispeech. Greedy WER (%) was measured on Librispeech test-other.  The number of parameters  (M) and compute (Multiply-Acc, GMAC) are shown for encoder only.</p> Encoder WER, % Params, Compute, test-other M GMACS RNNT decoder Conformer 5.19 115 143.2 Fast Conformer 4.99 109 48.7 CTC decoder Conformer 5.74 121 149.2 Eff. Conformer [Burchi2021] 5.79 125 101.3 SqeezeFormer [Kim2022] 6.05 125 91.0 Fast Conformer 6.00 115 51.5"},{"location":"blogs/2023/2023-06-07-fast-conformer/#speech-translation-results","title":"Speech Translation Results","text":"<p>We also evaluate Fast Conformer on the IWSLT 2014 German-English speech translation task. We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being upto 1.8x faster and using fewer parameters.</p> <p>Our models have been trained on all available datasets from IWSLT22 competition which corresponds to 4k hours of speech. Some of the datasets did not contain German translations, so we generated them ourselves with text-to-text machine translation model trained on WMT21 and in-domain finetuned on Must-C v2.</p> <p>Speech Translation, MUST-C V2 tst-COMMON dataset. SacreBLEU, total inference time, and relative inference speed-up were measured with a batch size \\(32\\) for two speech translation models with Conformer-based encoder and either RNNT, or Transformer decoder.</p> Encoder BLEU Time (sec) Speed-up Transformer decoder Conformer 31.0 267 1X Fast Conformer 31.4 161 1.66X RNNT decoder Conformer 23.2 83 1X Fast Conformer 27.9 45 1.84X"},{"location":"blogs/2023/2023-06-07-fast-conformer/#spoken-language-understanding-results","title":"Spoken Language Understanding Results","text":"<p>For the Speech Intent Clasification and Slot Filling task, experiments are conducted using the SLURP dataset, where intent accuracy and SLURP-F1 are used as the evaluation metric. For a fair comparison, both Conformer and Fast Conformer models are initialized by training on the same NeMo ASR Set dataset (roughly 25,000 hours of speech) and then the weights of the entire model are finetuned with the respective decoders.</p> <p>Speech intent classification and slot filling on SLURP dataset. ESPNet-SLU  and SpeechBrain-SLU models use a HuBERT encoder pre-trained via a self-supervised objective on LibriLight-60k.  Inference time and relative speed-up against Conformer are measured with batch size 32.</p> Model Intent Acc SLURP F1 Inference, sec Rel. Speed-up SpeechBrain-SLU 87.70 76.19 - - ESPnet-SLU 86.52 76.91 - - Conformer/Fast Conformer+Transformer Decoder Conformer 90.14 82.27 210 1X Fast Conformer 90.68 82.04 191 1.1X"},{"location":"blogs/2023/2023-06-07-fast-conformer/#long-form-speech-recognition-results","title":"Long Form Speech Recognition Results","text":"<p>While Fast Conformer can be modified post training to do simple inference on long audio, due to the mismatch in attention window with limited future information, the model's WER may degrade a small amount. Users can therefore add global token followed by subsequent fine-tuning for some small steps on the same dataset in order to significantly recover (and outperform) the original models WER.</p> <p>Note that with Longformer style attention, we can still perform buffered inference with large chunk size - upto 1 hour long, therefore inference on even longer audio can be done efficiently with few inference steps.</p> <p>Fast Conformer versus Conformer on long audio. We evaluated four versions of FastConformer: (1) FC with global attention (2) FC with limited context (3) FC with limited context and global token (4) FC with limited context and global token, trained on long concatenated utterances. Models have been evaluated on two long-audio bencmarks: TED-LIUM v3 and Earning 21. Greedy WER(%).</p> Model TED-LIUM v3 Earnings21 Conformer 9.71 24.34 Fast Conformer 9.85 23.84  + Limited Context 9.92 28.42   + Global Token 8.00 20.68    + Concat utterances 7.85 19.52"},{"location":"blogs/2023/2023-08-forced-alignment/","title":"How does forced alignment work?","text":"<p>In this blog post we will explain how you can use an Automatic Speech Recognition (ASR) model<sup>1</sup> to match up the text spoken in an audio file with the time when it is spoken. Once you have this information, you can do downstream tasks such as:</p> <ul> <li> <p>creating subtitles such as in the video below<sup>2</sup> or in the Hugging Face space</p> </li> <li> <p>obtaining durations of tokens or words to use in Text To Speech or speaker diarization models</p> </li> <li> <p>splitting long audio files (and their transcripts) into shorter ones. This is especially useful when making datasets for training new ASR models, since audio files that are too long will not be able to fit onto a single GPU during training. <sup>3</sup></p> </li> </ul> <p> </p> Video with words highlighted according to timestamps obtained with NFA"},{"location":"blogs/2023/2023-08-forced-alignment/#what-is-forced-alignment","title":"What is forced alignment?","text":"<p>This task of matching up text to when it is spoken is called 'forced alignment'. We use 'best alignment', 'most likely alignment' or sometimes just 'the alignment' to refer to the most likely link between the text and where in the audio it is spoken. Normally these links are between chunks of the audio and the text tokens<sup>4</sup>. If we are interested in word alignments, we can simply group together the token alignments for each word.</p> <p> </p> Diagram of a possible alignment between audio with 5 timesteps and text with 3 tokens ('C', 'A', 'T') <p>The 'forced' in 'forced alignment' refers to the fact that we provide the reference text ourselves and use the ASR model to get an alignment based on the assumption that this reference text is the real ground truth, i.e. exactly what is spoken - sometimes it makes sense to drop this requirement in case your reference text is incorrect. There are various other aligners that work on this assumption<sup>5</sup>.</p> <p>Sometimes in discussing this topic, we may drop the 'forced' and just call it 'alignment' when we mean 'forced alignment'. We will sometimes do this in this tutorial, for brevity.</p> <p> </p> We can think of an alignment as a way to arrange the S number of tokens into T number of boxes <p>In forced alignment our two inputs are the audio and the text. You can think of the audio as being split into \\(T\\) equally-sized chunks, or 'timesteps', and the text as being a sequence of \\(S\\) tokens. So we can think of an alignment as either a mapping from the \\(S\\) tokens to the \\(T\\) timesteps, or as a way of duplicating some of the tokens so that we have a sequence of \\(T\\) of them, each being mapped to the timestep when it is spoken. Thus this alignment algorithm will only work if \\(T \\ge S\\).</p> <p>The task of forced alignment is basically figuring out what the exact \\(T\\)-length sequence of these tokens should be in order to give you the best alignment.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#formulating-the-problem","title":"Formulating the problem","text":"<p>To do forced alignment, we will need an already-trained ASR model <sup>6</sup>. This model's input is the spectrogram of an audio file, which is a representation of the frequencies present in the audio signal. The spectrogram will have \\(T_{in}\\) timesteps. The ASR model will output a probability matrix of size \\(V \\times T\\) where \\(V\\) is the number of tokens in our vocabulary (e.g. the number of letters in the alphabet of the language we are transcribing) and \\(T\\) is the number of output timesteps. \\(T\\) may be equal to \\(T_{in}\\), or it may be smaller by some ratio if our ASR model has some downsampling in its neural net architecture. For example, NeMo's pretrained models have the following downsampling ratios:</p> <ul> <li>NeMo QuartzNet models have \\(T = \\frac{T_{in}}{2}\\)</li> <li>NeMo Conformer models have \\(T = \\frac{T_{in}}{4}\\)</li> <li>NeMo Citrinet and FastConformer models have \\(T = \\frac{T_{in}}{8}\\)</li> </ul> <p>In the diagram below, we have drawn \\(T_{in} = 40\\) and \\(T = 5\\), as one would obtain from one of the pretrained NeMo Citrinet or FastConformer models, which have a downsampling ratio of 8. In terms of seconds, as spectrogram frames are 0.01 seconds apart, each column in the spectrogram corresponds to 0.01 seconds, and each column in the ASR Model output matrix corresponds to \\(0.01 \\times 8 = 0.08\\) seconds.</p> <p> </p> The input audio contains T_{in} timesteps. The matrix outputted by the ASR Model has shape V x T <p>As mentioned above, the ASR Model's output matrix is of size \\(V \\times T\\). The number found in row \\(v\\) of column \\(t\\) of this matrix is the probability that the \\(v\\)-th token is being spoken at timestep \\(t\\). Thus, all the numbers in a given column must add up to 1.</p> <p>If we didn't know anything about what is spoken in the audio, we would need to \"decode\" this output matrix to produce the best possible transcription. That is a whole research area of its own - a topic for another day.</p> <p>Our task is forced alignment, where by definition we have some reference text matching the ground truth text that is actually spoken (e.g. \"cat\") and we want to specify exactly when each token is spoken.</p> <p>As mentioned in the previous section, we essentially have \\(T\\) slots, and we want to fill each slot with the tokens <code>'C', 'A', 'T'</code> (in that order) in the locations where the sound of each token is spoken.</p> <p>To make sure we go through the letters in order, we can think of this \\(T\\)-length sequence as being one which passes through the graph below from start to finish, making a total of \\(T\\) stops on the red tokens.</p> <p> </p> Every possible alignment passes from \"START\" to \"END\" with T stops at each red token <p>For now we ignore the possibility of some of the audio not containing any speech and the possibility of a 'blank' token, which is a key feature of CTC (\"Connectionist Temporal Classification\") models \u2014 we will get to that later.</p> <p>Let's look at the (made-up) output of our ASR model. We've removed all the tokens from our vocabulary except <code>CAT</code> and normalized the columns to make the maths easier for our example.  We denote the values in this matrix as \\(p(s,t)\\), where \\(s\\) is the index of the token in the ground truth, and \\(t\\) is the timestep in the audio.</p> Timestep: 1 2 3 4 5 C 0.7 0.4 0.1 0.1 0.1 A 0.1 0.3 0.2 0.4 0.2 T 0.2 0.3 0.7 0.5 0.7 <p>Our first instinct may be to try to take the argmax of each column in \\(p(s,t)\\), however that may lead to an alignment which does not match the order of tokens in the reference text, or may leave out some tokens entirely. In the current example, such a strategy will yield <code>C (0.7) -&gt; C (0.4) -&gt; T (0.7) -&gt; T (0.5) -&gt; T (0.5)</code>, which spells <code>CT</code> instead of <code>CAT</code>.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#forced-alignment-the-naive-way","title":"Forced alignment the naive way","text":"<p>The issue with the above attempt is we did not restrict our search to only alignments that would spell out <code>CAT</code>.</p> <p>Since our number of timesteps (\\(T=5\\)) and tokens (\\(S=3\\)) is small, we can list out every possible alignment, i.e. every possible arrangement of <code>'CAT'</code> that will fit in our 5 slots:</p> Timestep: 1 2 3 4 5 alignment 1 C C C A T alignment 2 C C A A T alignment 3 C C A T T alignment 4 C A A A T alignment 5 C A A T T alignment 6 C A T T T <p>Each token has a certain probability of being spoken at each time step, determined by our ASR model. The probability of a particular sequence of tokens is calculated by multiplying together the individual probabilities of each token at each timestep. Assuming our ASR model is a good one, the best alignment is the one with the highest cumulative probability.</p> <p>Let's show the \\(p(s,t)\\) probabilities again.</p> Timestep: 1 2 3 4 5 C 0.7 0.4 0.1 0.1 0.1 A 0.1 0.3 0.2 0.4 0.2 T 0.2 0.3 0.7 0.5 0.7 <p>We can calculate the probability of each possible alignment by multiplying together each \\(p(s,t)\\) that it passes through:</p> Timestep: 1 2 3 4 5 Total probability of alignment alignment 1 C C C A T0.7 * 0.4 * 0.1 * 0.4 * 0.7 = 0.008 alignment 2 C C A A T 0.7 * 0.4 * 0.2 * 0.4 * 0.7 = 0.016 alignment 3 C C A T T0.7 * 0.4 * 0.2 * 0.5 * 0.7 = 0.020 alignment 4 C A A A T0.7 * 0.3 * 0.2 * 0.4 * 0.7 = 0.012  alignment 5 C A A T T0.7 * 0.3 * 0.2 * 0.5 * 0.7 = 0.015 alignment 6 C A T T T0.7 * 0.3 * 0.7 * 0.5 * 0.7 = 0.051 &lt;- the max <p>We can see that the most likely path is <code>'CATTT'</code> because it has the highest total probability. In other words, based on our ASR model, the most likely alignment is that <code>'C'</code> was spoken at the first timestep, <code>'A'</code> was spoken at the second timestep, and <code>'T'</code> was spoken for the last 3 timesteps. </p>"},{"location":"blogs/2023/2023-08-forced-alignment/#the-naive-way-but-listing-all-the-possible-paths-using-a-graph","title":"The naive way but listing all the possible paths using a graph","text":"<p>To make further progress in understanding forced alignment, let's list all the possible paths in a systematic way by arranging them in a tree-like graph like the one below. </p> <p>We initialize the graph with a 'start' node (for clarity), then connect it to nodes representing the tokens that our alignment can have at the first timestep (<code>t=1</code>). In our case, this is only a single token <code>C</code>. From that <code>C</code> node, we draw arrows to 2 other nodes. The higher node represents staying at the same token (<code>C</code>) for the next timestep (<code>t=2</code>). The lower node represents going to the next token (<code>A</code>) for the next timestep (<code>t=2</code>). We continue this process until we have drawn all the possible paths through our reference text tokens for the fixed duration \\(T\\). We do not include paths that reach the final token too early or too late.</p> <p>We end up with the tree below, which represents all the possible paths through the <code>CAT</code> tokens over 5 timesteps. </p> <p>You can check for yourself that every alignment we listed in the table in the previous section is represented as a path from left to right in this tree.</p> <p>We can label each node in the graph with its \\(p(s,t)\\) probabilities (dark green).</p> <p>Let's also calculate the cumulative product along each path and include that as well (light green).</p> <p>Once we do that, we can look at all the nodes at the final timestep, and see that the cumulative product at each node is the cumulative probability of the path from start to T that lands at that node. </p> <p>As before, we can see that the probability of the most likely path, i.e. the most likely alignment, is 0.051. If we trace back our steps from that T node to the start, then we see that the path is <code>'CATTT'</code>.<sup>7</sup></p> <p> </p> This graph lists every possible alignment. The most likely alignment becomes highlighted in purple."},{"location":"blogs/2023/2023-08-forced-alignment/#the-trouble-with-longer-sequences","title":"The trouble with longer sequences","text":"<p>The naive method in the previous sections was intuitive, easy to calculate and gave us the correct answer, but it was only feasible because we had such a small number of possible alignments. In an utterance of 10 words over 5 seconds, conservatively you can expect 20 tokens and 63 timesteps<sup>8</sup> - that would lead to about \\(4.8 \\times 10^{15}\\) possible alignments<sup>9</sup>! </p> <p>Fortunately there is a method to obtain the most likely alignment, for which you:</p> <ul> <li>don't need to calculate all the cumulative products for every path, and</li> <li>don't even need to draw the full tree graph.</li> </ul> <p>We will work towards this method in the next sections.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#spotting-graph-redundancies","title":"Spotting graph redundancies","text":"<p>Fortunately, because we are only interested in the highest probability path from the start node to one of the final <code>T</code> nodes on the right, this means that the tree graph has a lot of redundant nodes that we don't need to worry about.</p> <p>For example, consider the two nodes in the tree corresponding to token <code>A</code> (<code>s=2</code>) at time <code>t=3</code>. Looking at the cumulative products of these two nodes (in light green), we can see that the top <code>A</code> node (corresponding to the partial path <code>CCA</code>) has a cumulative product of 0.056, while the bottom <code>A</code> node (corresponding to the partial path <code>CAA</code>) has a smaller cumulative product of 0.042.</p> <p>The paths downstream of the top node are identical in graph structure to those downstream of the bottom node, as are the \\(p(s,t)\\) values (in dark green) of any nodes in these downstream paths. Therefore, since the cumulative product of any path downstream of an <code>A</code> node at <code>t=3</code> will just be the cumulative product of that <code>A</code> node at <code>t=3</code> multiplied by these downstream \\(p(s,t)\\) values, it is impossible for any path downstream of the bottom <code>A</code> node to have a higher cumulative product than the corresponding path downstream of the top <code>A</code> node. Thus, it is impossible that any of the paths downstream of the bottom <code>A</code> node will end up being the optimal path overall, and we can safely discard them. This is shown in the animation below.</p> <p> </p> If we examine the two 'A' nodes at 't=3', we see that we can discard the node with the lower cumulative product, and the nodes downstream of it. <p>These redundancies exist between any nodes with the same \\(s\\) and \\(t\\) values. All of their downstream nodes will have the same structure, but we only need to keep the node that corresponds to the most probable path from the start node to (s,t).</p> <p>So, for each \\((s,t)\\) we only need to record the most probable path to (s,t) and can discard the non-winning node and its downstream nodes. Discarding the downstream nodes means that we will have fewer nodes to look at in the next timesteps, helping to keep the number of computations required relatively low.</p> <p>The animation below shows this. We start with all nodes in their original colors, and color nodes red if they represent the most probable path to (s,t). When there is only one node in the graph that has a particular \\((s,t)\\) value, it by default is the most probable path to (s,t), so we color it red immediately. When there is more than one node with the same \\((s,t)\\) value, we look at the cumulative probability (in light green) of these nodes. We mark the node with the lower cumulative probability dark blue, meaning we calculated its cumulative probability but realized that we can discard it. We mark its downstream nodes dark gray, to indicate that we can discard them, and don't need to consider them in future timesteps. Finally, for the node with the higher cumulative probability, we mark it red, to indicate that it is the most probable path to (s,t).</p> <p> </p> We cycle through all possible (s,t) and discard the nodes that we do not need. <p>The cumulative product of the remaining node at the final timestep is the probability of the most likely alignment path. Again, we can trace back the steps from that node to the start to recover the exact path that gives that probability (<code>'CATTT'</code>).</p> <p>Although we show the cumulative probabilities for all nodes in the animation (for illustrative purposes), we didn't need to calculate the cumulative probabilities for any of the nodes that are dark gray, but we still managed to find the most likely path. We obtained the same result as with naive graph method but a lot fewer operations.</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#formalizing-the-efficient-forced-alignment-algorithm","title":"Formalizing the efficient forced alignment algorithm","text":"<p>Let\u2019s formalize the steps we followed. If we look at the nodes that we didn\u2019t discard, they form a different shape of graph. The animation below transforms the tree graph into its new shape by hiding the discarded nodes behind the non-discarded nodes.</p> <p> </p> We create a different shape of graph using only the nodes that we did not discard. <p>The resulting graph has a single node for each \\((s,t)\\). This graph is often referred to as a trellis. Each node has an associated number in red which is the probability of the most likely path from start to \\((s,t)\\). (We also keep the \\(p(s,t)\\) values in dark green for illustrative purposes).</p> <p>We can recover the most likely alignment by looking at the node at \\((S,T)\\). Its cumulative probability, in red, is the probability of the most likely alignment. We can also recover the exact path that has that probability by tracing following the non-discarded edges (in light gray) backwards to the start token. This produces the <code>'CATTT'</code> sequences yet again.</p> <p>At each node \\((s,t)\\), the procedure we used to calculate the most probable path to \\((s,t)\\), was to look at the tokens in the previous timestep that could have  transitioned into this \\((s,t)\\), calculate the candidates for the most probable path to \\((s,t)\\), and pick the maximum value.</p> <p>In our scenario, where at each timestep the token either stays the same or moves onto the next one, candidates come from \\((s-1, t-1)\\) &amp; \\((s,t-1)\\). </p> <p>We can denote this formula as:</p> <p><code>prob-of-most-likely-path-to(s, t) = max (prob-of-most-likely-path-to(s-1, t-1) * p(s,t), prob-of-most-likely-path-to(s, t-1) * p(s,t))</code>.</p> <p>Let's make the formula shorter by denoting <code>prob-of-most-likely-path-to(s-1, t-1)</code> using the letter 'v' (to be explained later). So the formula becomes: </p> \\[v(s, t) = \\max (v(s-1, t-1) \\times p(s,t), v(s, t-1) \\times p(s,t))\\] <p>We can also show the rule on a subsection of our trellis graph, as below.</p> <p> </p> The formula we use to calculate v(s,t) probabilities in our current setup."},{"location":"blogs/2023/2023-08-forced-alignment/#the-viterbi-algorithm","title":"The Viterbi algorithm","text":"<p>What we described above is actually known as the Viterbi algorithm. What we denoted <code>prob-of-most-likely-path-to(s, t)</code> is often denoted <code>v(s,t)</code>, A.K.A. the Viterbi probability (of token <code>s</code> at time <code>t</code>).</p> <p>The Viterbi algorithm is an efficient method for finding the most likely alignment, and exploits the redundancies in the tree graph discussed above. It involves creating a matrix of size \\(S \\times T\\) (called a 'Viterbi matrix') and populating it column-by-column. </p> <p>The shape of the initialized Viterbi matrix for our scenario is shown below. Every element in the matrix corresponds to a node in the trellis (except for elements in the bottom left and top right of the matrix, which we did not draw in our trellis as they would not form valid alignments).</p> t=1 t=2 t=3 t=4 t=5 s=0 AKA token is C ?? ?? ?? ?? ?? s=1 AKA token is A ?? ?? ?? ?? ?? s=2 AKA token is T ?? ?? ?? ?? ?? <p>We need to fill in every element in the Viterbi matrix column-by-column and row-by-row<sup>10</sup>. Once we have finished, \\(v(s=S,t=T)\\) will contain the probability of the most likely path to (S,T), i.e. from start to finish, and if we recorded the argmax for computing each \\((s,t)\\), then we can use these values to recover what the exact sequence is which has this highest probability. The recorded argmaxes (which in our trellis look like light gray arrows) are often called \"backpointers\". The process of using the backpointers to recover the token sequence of the most likely alignment is known as \"backtracking\".</p> <p>Some special cases:</p> <ul> <li> <p>For the first \\((t=1)\\) column of the Viterbi matrix, we set \\(v(s=1,t=1)\\) to \\(p(s=1,t=1)\\) and all other \\(v(s&gt;1,t=1)\\) to 0. We do this because all possible alignments must start with the first token in the ground truth \\((s=1)\\). (This only holds for our current setup, and would be different in a CTC setup, see below).</p> </li> <li> <p>For some values of \\((s,t)\\), either the \\((s-1, t-1)\\) or \\((s, t-1)\\) nodes are not reachable, in which cases we ignore their terms in the \\(v(s,t)\\) formula, and do a trivial max over one element.</p> </li> </ul>"},{"location":"blogs/2023/2023-08-forced-alignment/#extending-to-ctc","title":"Extending to CTC","text":"<p>The example above was simplified from a CTC approach to make it easier to understand and visualize. If you want to work with CTC alignments, you must allow blank tokens in between your ground truth tokens (the blank tokens are always optional except for in between repeated tokens.)</p> <p>Thus, if the reference text tokens are still <code>'C', 'A', 'T'</code>, we add optional 'blank' tokens in between them, which we denote as <code>&lt;b&gt;</code>. The diagram of the allowed sequence of tokens would look like this:</p> <p> </p> Every possible alignment passes from \"START\" to \"END\" with T stops at each red token <p>As you can see, <code>S</code> \u2014 the total number of tokens \u2014 is 7 (3 non-blank tokens and 4 blank tokens). If we keep \\(T=5\\), the trellis for CTC with reference text tokens <code>'C', 'A', 'T'</code> would look like this:</p> <p> </p> The shape of the trellis if we use a CTC model, our reference text tokens are 'C', 'A', 'T', and the number of timesteps in the ASR model output (T) is 5. <p>The Viterbi algorithm rules would also change, as follows:</p> <p> </p> The formula we use to calculate v(s,t) probabilities for a CTC model. <p>However, the principle remains the same: we initialize a Viterbi matrix of size \\(S \\times T\\) and fill it in according to the recursive formula. Once we fill it in, because there are 2 valid final tokens, we need to compare \\(v(s=S-1,t=T)\\) and \\(v(s=S,t=T)\\) - the token with the higher value is the end token of the most likely probability. To recover the overall most likely alignment, we need to backtrack from the higher of \\(v(s=S-1,t=T)\\) or \\(v(s=S,t=T)\\).</p>"},{"location":"blogs/2023/2023-08-forced-alignment/#conclusion","title":"Conclusion","text":"<p>In this tutorial, we have shown how to do forced alignment using the Viterbi algorithm, which is an efficient way to find the most likely path through the reference text tokens.</p> <p>You can obtain forced alignments using the NeMo Forced Aligner (NFA) tool within NeMo, which has an efficient PyTorch tensor-based implementation of Viterbi decoding on CTC models. An efficient CUDA-based implementation of Viterbi decoding was also recently added to TorchAudio, though it is currently only available in the nightly version of TorchAudio, and is not always faster than the current NFA PyTorch tensor-based implementation.</p> <p>Although our examples used characters are tokens, most NeMo models use sub-word tokens, such as in the diagram below. Furthermore, although we've given examples using probabilities ranging from 0 to 1, most Viterbi algorithms operate on log probabilities, which will make the operations in the algorithm more numerically stable. </p> <p> </p> The NFA forced alignment pipeline, which has been described in this blog post. <p>To learn more about NFA, you can now refer to the resources here.</p> <ol> <li> <p>Specifically we will be explaining how to use CTC-like (Connectionist Temporal Classification) models which output a probability distribution over vocabulary tokens per audio timestep. We will explain how forced alignment works using a simplified CTC-like model, and explain how to extend it to a CTC model at the end of the tutorial. There are many CTC models available out of the box in NeMo. Alternative types of ASR models inlcude Transducer models (also available in NeMo), and attention-based encoder-decoder models (many of which build on this work, and a recent example of which is Whisper). These types of models would require a different approach to obtaining forced alignments.\u00a0\u21a9</p> </li> <li> <p>This video is of an excerpt from 'The Jingle Book' by Carolyn Wells. The audio is a reading of a poem called \"The Butter Betty Bought\". The audio is taken from a LibriVox recording of the book. We used NeMo Forced Aligner to generate the subtitle files for the video. The text was adapted from Project Gutenberg. Both the original audio and the text are in the public domain.\u00a0\u21a9</p> </li> <li> <p>There are toolkits specialized for this purpose such as CTC Segmentation, which has an integration in NeMo. It uses an extended version of the algorithm that we describe in this tutorial. The key difference is that the algorithm in this tutorial is forced alignment which assumes that the ground truth text provided is exactly what is spoken in the text. However, in practice the ground truth text may be different from what is spoken, and the algorithm in CTC Segmentation accounts for this.\u00a0\u21a9</p> </li> <li> <p>These can be graphemes (i.e. letters or characters), phonemes, or subword-tokens\u00a0\u21a9</p> </li> <li> <p>Such as CTC Segmentation (also integrated in NeMo), Gentle aligner.\u00a0\u21a9</p> </li> <li> <p>If you want to learn more about how ASR models are trained, this is an excellent tutorial.\u00a0\u21a9</p> </li> <li> <p>In the graph, we can also try to follow a 'greedy' path where we only take the outgoing path with the highest probability. In this example, this 'greedy' approach would give us the alignment path <code>CCATT</code>, which has a probability of 0.020 - lower than the actual most likely path.\u00a0\u21a9</p> </li> <li> <p>To estimate the number of tokens, we assume there are 2 tokens per word \\(\\implies 10\\) words \\(\\times 2\\) tokens/word = 20 tokens. </p> <p>To estimate the number of timesteps, we assume a spectrogram frame hop size of 0.01 \\(\\implies \\frac{5}{0.01} = 500 = T_{in} \\implies {T} = \\frac{T_{in}}{8} = \\frac{500}{8} = 62.5 \\approx 63\\).\u00a0\u21a9</p> </li> <li> <p>The exact number of possible paths in our formulation is equal to \\({T-1 \\choose S-1 }\\), and if \\(T=63\\) and \\(S=20\\), then \\({T-1 \\choose S-1 }={63-1 \\choose 20-1 }={62 \\choose 19 }=4.8 \\times 10^{15}\\).</p> <p>We will explain how we we deduced the formula \\({T-1 \\choose S-1 }\\) below.</p> <p>The number of possible paths is the same as the number of ways we could fit \\(S\\) tokens into \\(T\\) boxes, with the \\(S\\) tokens being in some fixed order.</p> <p>This formulation is the same as having \\(T\\) boxes, and putting \\(S-1\\) markers in between the boxes, where the markers indicate a switch from one token to the next. There are \\(T-1\\) locations where the markers can go (i.e. \\(T-1\\) spaces between the boxes), therefore the number of possible ways we could arrange the markers is \\({T-1 \\choose S-1 }\\). Therefore, this is also the number of possible alignment paths in our setup with \\(S\\) tokens and \\(T\\) timesteps.</p> <p>This is analogous to a known result in combinatorics called stars and bars - in our case our 'boxes' are the 'stars' and our 'markers' are the 'bars'.\u00a0\u21a9</p> </li> <li> <p>Each entry within the same column is actually independent of the other entries in that column. We can compute those entries in any order or, even better, simultaneously - which would speed up the time to complete the algorithm.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/2023/2023-08-nfa/","title":"Introducing NeMo Forced Aligner","text":"<p>Today we introduce NeMo Forced Aligner: a NeMo-based tool for forced alignment.</p> <p>NFA allows you to obtain token-level, word-level and segment-level timestamps for words spoken in an audio file. NFA produces timestamp information in a variety of output file formats, including subtitle files, which you can use to create videos such as the one below<sup>1</sup>:</p> <p> </p> Video with words highlighted according to word alignment timestamps obtained with NFA <p>Ways to get started:</p> <ul> <li>Try out our HuggingFace Space demo to quickly test NFA in various languages.</li> <li>Follow along with our step-by-step NFA \"how-to\" tutorial.</li> <li>Learn more about how forced alignment works in this explainer tutorial.</li> </ul> <p>You can also download NFA from the NeMo repository.</p> <p>You can use NFA timestamps to:</p> <ul> <li>Split audio files into shorter segments</li> <li>Generate token- or word-level subtitles, like in our HuggingFace Space</li> <li>Train token/word duration components of text-to-speech or speaker diarization models</li> </ul> <p>NFA alignment timestamps can be based on reference text that you provide, or reference text obtained from speech-to-text transcription from a NeMo model. NFA works on audio in 14+ languages: it will work any of the 14 (and counting) languages for which there is an open-sourced NeMo speech-to-text model checkpoint, or you can train your own ASR model for a new language.</p> <p> </p> The NFA forced alignment pipeline <ol> <li> <p>This video is of an excerpt from 'The Jingle Book' by Carolyn Wells. The audio is a reading of a poem called \"The Butter Betty Bought\". The audio is taken from a LibriVox recording of the book. We used NeMo Forced Aligner to generate the subtitle files for the video. The text was adapted from Project Gutenberg. Both the original audio and the text are in the public domain.\u00a0\u21a9</p> </li> </ol>"},{"location":"blogs/2023/2023-10-28-numba-fp16/","title":"Training NeMo RNN-T Models Efficiently with Numba FP16 Support","text":"<p>In the field of Automatic Speech Recognition research, RNN Transducer (RNN-T) is a type of sequence-to-sequence model that is well-known for being able to achieve state-of-the-art transcription accuracy in offline and real-time (A.K.A. \"streaming\") speech recognition applications. They are also notorious for having high memory requirements. In this blog post we will explain why they have this reputation, and how NeMo allows you to side-step many of the memory requirements issues, including how to make use of Numba\u2019s recent addition of FP16 support.</p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#whats-so-great-about-transducer-models","title":"What\u2019s so great about Transducer models?","text":"<p>As we mentioned, RNN-T models (often called just \u201cTransducer\u201d models, since they don\u2019t need to use an RNN) have been shown to achieve state-of-the-art results for accurate, streaming speech recognition. RNN-Ts are also able to handle longer sequences than they were trained on, as well as out-of-vocabulary words, which is a common problem in speech recognition.</p> <p>If you want to learn more about Transducer models, we recommend the excellent blog post Sequence-to-sequence learning with Transducers.</p> <p> </p> Figure 1. The RNN-Transducer architecture. The audio sequence 'x' is passed through the encoder network, and the text sequence 'y' is passed to the prediction network. The outputs of both networks are combined in the joint network."},{"location":"blogs/2023/2023-10-28-numba-fp16/#why-do-transducer-models-consume-a-lot-of-memory","title":"Why do Transducer models consume a lot of memory?","text":"<p>A significant drawback of the Transducer architecture is the vast GPU memory required during training. As discussed in Sequence-to-sequence learning with Transducers, the output of the joint network (which is the final step before the softmax, see Figure 1) in the transducer is a 4-dimensional tensor which occupies significant amounts of memory. The size of this tensor (both its activations and its gradients) can be calculated as follows:</p> <p>\\[\\textnormal{Joint tensor size} = \\: B \\times T \\times U \\times V \\times 2 \\times 4 \\,\\, \\textnormal{bytes}\\] </p> <p>Here, \\(B\\) is the batch size, \\(T\\) is the audio sequence length, \\(U\\) is the text sequence length and \\(V\\) is the vocabulary size. We multiply by 2 so we get the size of both the activations and the gradients. We then multiply by 4 because we assume an FP32 datatype (and a single FP32 value occupies 4 bytes).</p> <p>The audio waveform signal is commonly converted to 100 Hz spectrogram frames, which means each second of audio corresponds to 100 audio frames. Thus, for a single 20-second audio clip with about 100 subwords in its transcript, and a vocabulary of 1024 subword tokens, the size of the tensor would be ~1.6 Gigabytes:</p> <p>\\[\\textnormal{Joint tensor size} = \\: B \\times \\phantom{....}T\\phantom{....}\\times \\phantom{.}U\\phantom{.} \\times \\phantom{..}V\\phantom{.} \\times 2 \\times 4 \\,\\, \\textnormal{bytes}\\phantom{=1.6 \\textnormal{ Gigabytes}}\\]  \\[\\phantom{\\textnormal{Joint tensor size}} = \\: 1 \\times (20 \\times 100)  \\times 100 \\times 1024 \\times 2 \\times 4 \\,\\, \\textnormal{bytes}=1.6 \\textnormal{ Gigabytes}\\] </p> <p>This number is for a single audio sample. If we use a larger batch size, e.g. 10, for training, we will quickly run out of memory even on 16 GB GPUs. Also, remember, this is just the size of the joint network tensor: there is additional memory required to keep the model in memory, and to calculate the activation and gradients of the rest of the network!</p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#enter-numba-support-for-fp16-datatype","title":"Enter Numba support for FP16 datatype","text":"<p>As of Numba 0.57 release, FP16 datatype format is now supported natively. Using this, we can effectively halve the memory requirement of the above joint network tensor and support larger batch sizes with almost no changes to our NeMo workflow!</p> <p>NeMo utilizes Numba's Just-in-time compile CUDA kernels written in Python in order to efficiently compute the RNN-T loss (which requires manipulation of the joint network tensor). This allows a user to simply have Numba installed on their system, and without explicit compilation of C++ / CUDA code, they can train their RNN-T models easily. Furthermore, since the kernels are written in Python, it allows for simple modifications by researchers to develop advanced features such as FastEmit, and even other extensions to the Transducer loss, such as Token-and-Duration Transducers.</p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#prerequisites","title":"Prerequisites","text":"<ul> <li>Pytorch 1.13.1+</li> <li>Nvidia NeMo 1.20.0+</li> <li>Numba 0.57+ (<code>conda install numba=0.57.1 -c conda-forge</code>)</li> <li>CUDA Python</li> <li>CUDA 11.8 (installed as part of <code>cudatoolkit</code>)</li> <li>It is preferable to install these libraries in a Conda environment (Python 3.10) for correct dependency resolution.</li> </ul> <p>The following snippet can be used to install the requirements:</p> <pre><code>conda create -n nemo -c pytorch -c nvidia -c conda-forge python=3.10 numba=0.57.1 cudatoolkit=11.8 cuda-python=11.8 pytorch torchvision torchaudio pytorch-cuda=11.8 cython\nconda activate nemo\npip install nemo-toolkit[all]&gt;=1.20.0\n</code></pre>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#enabling-numba-fp16-support-in-nemo","title":"Enabling Numba FP16 Support in NeMo","text":"<ul> <li>Set the Numba environment variable: <code>export NUMBA_CUDA_USE_NVIDIA_BINDING=1</code></li> <li>Set the NeMo environment variable: <code>export STRICT_NUMBA_COMPAT_CHECK=0</code></li> <li>Check if installation is successful by using the following snippet: </li> </ul> <pre><code>from nemo.core.utils import numba_utils\n\n# Should be True\nprint(numba_utils.numba_cuda_is_supported(numba_utils.__NUMBA_MINIMUM_VERSION_FP16_SUPPORTED__))\n\n# Should also be True\nprint(numba_utils.is_numba_cuda_fp16_supported())\n</code></pre>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#train-a-transducer-asr-model-with-fp16","title":"Train a Transducer ASR model with FP16","text":"<p>With the above environment flags set, and the latest Numba version installed, NeMo supports training with FP16 loss out of the box. For a tutorial on how to setup and train a Transducer ASR model, please refer to the NeMo ASR with Transducers tutorial.</p> <p>The only change necessary to use the FP16 loss is to specify <code>trainer.precision=16</code> in the NeMo model config.</p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#measuring-memory-and-compute-improvements","title":"Measuring Memory and Compute Improvements","text":"<p>We devised a simple benchmarking script that measures the memory usage when computing the RNN-T loss (with gradients enabled) for various combinations of inputs which are common during training on the Librispeech speech recognition dataset. The script used can be found in this Gist.</p> <p>We assume that we are training a Conformer or Fast Conformer Transducer model, which performs 4x or 8x audio signal reduction respectively. For Librispeech, the longest audio file is approximately 17 seconds, which becomes approximately 200 timesteps after 8x reduction. We check memory consumption for both Character tokenization (\\(V\\)=28) and Subword Tokenization (\\(V\\)=1024). Due to the tokenization, the transcript text may be between 80 to 250 tokens but we take a conservative limit of 100 to 200 tokens. As well as the output tensor, the benchmarking script takes into account the memory consumption of the activations and gradients of the intermediate layer of the joint network, which has a shape of \\(B \\times T \\times U \\times H\\). We set \\(H\\)=640 (this is a common value for this parameter in the literature).</p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#results","title":"Results","text":"<p>We show the results of the benchmarking in the graph below. You can see that using FP16 effectively halves the memory cost of an RNN-T model (compared with FP32), allowing to keep the memory usage relatively low as the values of parameters \\(B\\), \\(T\\), \\(U\\) and \\(V\\) increase.</p> <p> </p> Figure 1. Plot of GPU Memory usage for a given combination of Batch size (B), Timesteps (T), Text length (U), Vocabulary size (V) and the hidden dimension of the RNN-Transducer Joint.  <p>It is to be noted that NVIDIA NeMo has several other mechanisms to significantly reduce peak memory consumption, such as Batch Splitting. When combined with FP16 support in Numba, this allows us to train even larger ASR models with a Transducer loss. </p>"},{"location":"blogs/2023/2023-10-28-numba-fp16/#conclusion","title":"Conclusion","text":"<p>Numba FP16 support alleviates one of the crucial issues of RNN-Transducer training: memory usage. This unlocks efficient streaming speech recognition model training for a wider audience of researchers and developers. With a simple installation step, users are empowered to train and fine-tune their own speech recognition solutions on commonly available GPUs.</p> <p>Users can learn more about Numba and how to leverage it for high-performance computing using Python in their 5-minute guide. Furthermore, NeMo users can read up on how to perform speech recognition with many models and losses in the NeMo ASR documentation.</p>"},{"location":"blogs/2023/2023-nvidia-technical-blog/","title":"NeMo on the NVIDIA Technical blog in 2023","text":"<p>The following blog posts have been published by the NeMo team on the NVIDIA Technical blog in 2023.</p>"},{"location":"blogs/2023/2023-nvidia-technical-blog/#january-2023","title":"January 2023","text":"<ul> <li>Autoscaling NVIDIA Riva Deployment with Kubernetes for Speech AI in Production</li> <li>Multilingual and Code-Switched Automatic Speech Recognition with NVIDIA NeMo</li> </ul> <p>Based on work accepted to SLT 2022:</p> <ul> <li>Entropy-Based Methods for Word-Level ASR Confidence Estimation</li> <li>Controlled Adaptation of Speech Recognition Models to New Domains</li> </ul>"},{"location":"blogs/2024/2024-01-parakeet-tdt/","title":"Unveiling NVIDIA NeMo's Parakeet-TDT -- Turbocharged ASR with Unrivaled Accuracy","text":"<p>Earlier this month, we announced Parakeet, a cutting-edge collection of state-of-the-art ASR models built by NVIDIA's NeMo toolkit, developed jointly with Suno.ai. Today, we're thrilled to announce the latest addition to the Parakeet family -- Parakeet TDT. Parakeet TDT achieves unrivaled accuracy while running 64% faster over our previous best model, making it a great choice for powering speech recognition engines in diverse environments.</p> <p>The \"TDT\" in Parakeet-TDT is short for \"Token-and-Duration Transducer\", a novel sequence modeling architecture developed by NVIDIA and is open-sourced through NVIDIA's NeMo toolkit. Our research on TDT models, presented in a paper at the ICML 2023 conference, showcases the superior speed and recognition accuracy of TDT models compared to conventional Transducers of similar sizes. </p> <p>To put things in perspective, our Parakeet-TDT model with 1.1 billion parameters outperforms similar-sized Parakeet-RNNT-1.1b in accuracy, as measured as the average performance among 9 benchmarks on the HuggingFace Leaderboard. Notably, Parakeet-TDT is the first model to achieve an average WER below 7.0 on the leaderboard. Additionally, it achieves an impressive real-time factor (RTF) of 8.8e-3, 64% faster than Parakeet-RNNT-1.1b's RTF of 14.4e-3. Remarkably, Parakeet-TDT's RTF is even 40% faster than Parakeet-RNNT-0.6b (RTF 12.3), despite the latter having about half the model size.</p> <p> </p> Figure 1.  HuggingFace Leaderboard as of 01/31/2024."},{"location":"blogs/2024/2024-01-parakeet-tdt/#use-parakeet-tdt-model-in-your-code","title":"Use Parakeet-TDT model in your code","text":"<p>To run speech recognition with Parakeet-TDT, NVIDIA NeMo needs to be installed as a pip package as shown below. Cython and PyTorch (2.0 and above) should be installed before attempting to install NeMo Toolkit.</p> <pre><code>pip install nemo_toolkit['asr']\n</code></pre> <p>Once NeMo is installed, you can use Parakeet-TDT to recognize your audio files as follows: <pre><code>import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-tdt-1.1b\")\ntranscript = asr_model.transcribe([\"some_audio_file.wav\"])\n</code></pre></p>"},{"location":"blogs/2024/2024-01-parakeet-tdt/#understanding-token-and-duration-transducers","title":"Understanding Token-and-Duration Transducers","text":"<p>Token-and-Duration Transducers (TDT) represent a significant advancement over traditional Transducer models by drastically reducing wasteful computations during the recognition process. To grasp this improvement, let's delve into the workings of a typical Transducer model.</p> <p> </p> Figure 2. Transducer Model Architecture <p>Transducer models, as illustrated in Figure 2, consist of an encoder, a decoder, and a joiner. During speech recognition, the encoder processes audio signals, extracting crucial information from each frame. The decoder extracts information from already predicted text. The joiner then combines the outputs from the encoder and decoder, and predict a text token for each audio frame. From the joiner's perspective, a frame typically covers 40 to 80 milliseconds of audio signal, while on average people speak a word per 400 milliseconds. This discrepancy makes it so that certain frames don't associate with any text output. For those frames, the Transducer would predict a \"blank\" symbol. A typical sequence of predictions of a Transducer looks something like,</p> <p><code> _ _ _ _ NVIDIA _ _ _ _ is _ _ _  a _ _  great _ _ _ _ _  place _ _ _ _ to work _ _ _ </code></p> <p>where <code>_</code> represents the blank symbol. To generate the final recognizion output, the model would delete all the blanks, and generate the output</p> <p><code> NVIDIA is a great place to work </code></p> <p>As we can see, there are many blanks symbols in the original output and this means the Transducer model wasted a lot of time on \"blank frames\" -- frames for which the model predicts blanks which don't contribute to the final output.</p> <p> </p> Figure 3. TDT Model Architecture <p>TDT is designed to mitigate wasted computation by intelligently detecting and skipping blank frames during recognition. As Figure 3 shows, when a TDT model processes a frame, it simultaneously predicts two things: </p> <ol> <li>probability of token P<sub>T</sub>(v|t, u): the token that should be predicted at the current frame;</li> <li>probability of duration P<sub>D</sub>(d|t, u): the number of frames the current token lasts before the model can make the next token prediction.   <p>The TDT model is trained to maximize the number of frames skipped by using the duration prediction while maintaining the same recognition accuracy. For example in the example above, unlike a conventional Transducer that predict a token for every speech frame, the TDT model can simply the process as follows,</p> <p><code> frame 1:\u00a0  predict token=_, \u00a0\u00a0\u00a0\u00a0    duration=4  frame 5:\u00a0  predict token=NVIDIA, duration=5  frame 10: predict token=is,\u00a0\u00a0\u00a0\u00a0     duration=4  frame 14: predict token=a,\u00a0\u00a0\u00a0\u00a0\u00a0      duration=3  frame 17: predict token=great,\u00a0  duration=6  frame 23: predict token=place,\u00a0  duration=5  frame 28: predict token=to,\u00a0\u00a0\u00a0\u00a0     duration=1  frame 29: predict token=work,\u00a0\u00a0   duration=4  frame 33: reached the end of audio, recognition completed. </code></p> <p>In this toy example, TDT can reduce the number of predictions the model have to make from 33 to 8. In our extensive experiments with TDT models, we see this optimization indeed leads to a substantial acceleration in recognition speed. Our research has also demonstrated that TDT models exhibit enhanced robustness to noisy speech and token repetitions in the text compared to traditional Transducer models. Note, this blog post simplies certain aspects of Transducer models in order to better illustrate the design differences between Transducers and TDT, and we would refer interested readers to our paper for technical details.</p>"},{"location":"blogs/2024/2024-01-parakeet-tdt/#additional-resources","title":"Additional Resources","text":"<ul> <li>HuggingFace ASR Leaderboard</li> <li>HuggingFace ASR Leaderboard Evaluation</li> <li>NeMo Parakeet Models on HuggingFace</li> <li>NVIDIA NeMo Webpage</li> <li>NVIDIA NeMo ASR Documentation</li> <li>Papers:<ul> <li>Efficient Sequence Transduction by Jointly Predicting Tokens and Durations</li> <li>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</li> <li>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</li> </ul> </li> </ul>"},{"location":"blogs/2024/2024-01-parakeet/","title":"Announcing NVIDIA NeMo Parakeet ASR Models for Pushing the Boundaries of Speech Recognition","text":"<p>NVIDIA NeMo, a leading open-source toolkit for conversational AI, announces the release of Parakeet, a family of state-of-the-art automatic speech recognition (ASR) models (Figure 1.), capable of transcribing spoken English with exceptional accuracy. Developed in collaboration with Suno.ai, Parakeet ASR models mark a significant leap forward in speech recognition, paving the way for more natural and efficient human-computer interactions.</p> <p> </p> Figure 1.  HuggingFace Leaderboard as of 01/03/2024.  <p>NVIDIA announces four Parakeet models based on RNN Transducer / Connectionist Temporal Classification decoders and the size of the models. They boast 0.6-1.1 billion parameters and capable of tackling diverse audio environments. Trained on only a 64,000-hour dataset encompassing various accents, domains, and noise conditions, the models deliver exceptional word error rate (WER) performance across benchmark datasets, outperforming previous models.</p> <ul> <li>Parakeet RNNT 1.1B - Best recognition accuracy, modest inference speed. Best used when the most accurate transcriptions are necessary.</li> <li>Parakeet CTC 1.1B - Fast inference, strong recognition accuracy. A great middle ground between accuracy and speed of inference.</li> <li>Parakeet RNNT 0.6B - Strong recognition accuracy and fast inference. Useful for large-scale inference on limited resources.</li> <li>Parakeet CTC 0.6B - Fastest speed, modest recognition accuracy. Useful when transcription speed is the most important.</li> </ul> <p>Parakeet models exhibit resilience against non-speech segments, including music and silence, effectively preventing the generation of hallucinated transcripts.</p> <p>Built using the NVIDIA NeMo toolkit, Parakeet prioritizes user-friendliness and flexibility. With pre-trained checkpoints readily available, integrating the model into your projects is a breeze. Whether looking for immediate inference capabilities or fine-tuning for specific tasks, NeMo provides a robust and intuitive framework to leverage the model's full potential.</p> <p>Key benefits of Parakeet models:</p> <ul> <li>State-of-the-art accuracy: Superior WER performance across diverse audio sources and domains with strong robustness to non-speech segments.</li> <li>Different model sizes: Two models with 0.6B and 1.1B parameters for robust comprehension of complex speech patterns.</li> <li>Open-source and extensibility: Built on NVIDIA NeMo, allowing for seamless integration and customization.</li> <li>Pre-trained checkpoints: Ready-to-use models for inference or fine-tuning.</li> <li>Permissive license: Released under CC-BY-4.0 license, model checkpoints can be used in any commercial application.</li> </ul> <p>Parakeet is a major step forward in the evolution of conversational AI. Its exceptional accuracy, coupled with the flexibility and ease of use offered by NeMo, empowers developers to create more natural and intuitive voice-powered applications. The possibilities are endless, from enhancing the accuracy of virtual assistants to enabling seamless real-time communication.</p> <p>The Parakeet family of models achieves state-of-the-art numbers on the HuggingFace Leaderboard. Users can try out the parakeet-rnnt-1.1b firsthand at the Gradio demo. To access the model locally and explore the toolkit, visit the NVIDIA NeMo Github page.</p>"},{"location":"blogs/2024/2024-01-parakeet/#architecture-details","title":"Architecture Details","text":"<p>Parakeet models are based on the Fast Conformer architecture published in ASRU 2023. Fast Conformer is an optimized version of the Conformer model with 8x depthwise-separable convolutional downsampling, modified convolution kernel size, and an efficient subsampling module. Additionally it supports inference on very long audio segments (up to 11 hours of speech) on an A100 80GB card using Local Attention. The model is trained end-to-end using the Transducer decoder (RNNT) or Connectionist Temporal Classification decoder. For further details on long audio inference, please refer to the ICASSP 2024 paper \u201cInvestigating End-to-End ASR Architectures for Long Form Audio Transcription\u201d.</p> <p> </p> Figure 2.  Fast Conformer Architecture shows blocks of downsampling, conformer encoder blocks with limited context attention (LCA), and global token (GT)."},{"location":"blogs/2024/2024-01-parakeet/#usage","title":"Usage","text":"<p>NVIDIA NeMo can be installed as a pip package as shown below. Cython and PyTorch (2.0 and above) should be installed before attempting to install NeMo Toolkit.</p> <p>Then simply use: <pre><code>pip install nemo_toolkit['asr']\n</code></pre></p> <p>Once installed, you can evaluate a list of audio files as follows: <pre><code>import nemo.collections.asr as nemo_asr\nasr_model = nemo_asr.models.ASRModel.from_pretrained(model_name=\"nvidia/parakeet-rnnt-1.1b\")\ntranscript = asr_model.transcribe([\"some_audio_file.wav\"])\n</code></pre></p>"},{"location":"blogs/2024/2024-01-parakeet/#long-form-speech-inference","title":"Long-Form Speech Inference","text":"<p>Once you have a Fast Conformer model loaded, you can easily modify the attention type to limited context attention after building the model. You can also apply audio chunking for the subsampling module to perform inference on huge audio files!</p> <p>Note</p> <p>These models were trained with global attention, and switching to local attention will degrade their performance. However, they will still be able to transcribe long audio files reasonably well.</p> <p>For limited context attention on huge files (upto 11 hours on an A100), perform the following steps:</p> <pre><code># Enable local attention\nasr_model.change_attention_model(\"rel_pos_local_attn\", [128, 128])  # local attn\n\n# Enable chunking for subsampling module\nasr_model.change_subsampling_conv_chunking_factor(1)  # 1 = auto select\n\n# Transcribe a huge audio file\nasr_model.transcribe([\"&lt;path to a huge audio file&gt;.wav\"])  # 10+ hours !\n</code></pre>"},{"location":"blogs/2024/2024-01-parakeet/#additional-resources","title":"Additional Resources","text":"<ul> <li>HuggingFace ASR Leaderboard</li> <li>HuggingFace ASR Leaderboard Evaluation</li> <li>NeMo Parakeet Models on HuggingFace</li> <li>NVIDIA NeMo Webpage</li> <li>NVIDIA NeMo ASR Documentation</li> <li>Papers:<ul> <li>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</li> <li>Investigating End-to-End ASR Architectures for Long Form Audio Transcription</li> </ul> </li> </ul>"},{"location":"blogs/2024/2024-02-canary/","title":"NVIDIA NeMo Canary Model Pushes the Frontier of Speech Recognition and Translation","text":"<p>NVIDIA NeMo team is thrilled to announce Canary, a multilingual model that sets a new standard in speech-to-text recognition and translation. Canary transcribes speech in English, Spanish, German, and French and also generates text with punctuation and capitalization. Canary supports bi-directional translation, between English and three other supported languages. Canary achieves the first place on HuggingFace Open ASR leaderboard with an average word error rate of 6.67%, outperforming all other open source models by a wide margin.</p> <p> </p> Canary can transcribe and translate English, German, Spanish and French. <p>Canary is trained on a combination of public and in-house data. It uses 85,000 hours of annotated speech to learn speech recognition. To teach Canary translation, we used NVIDIA NeMo machine translation models to generate translations of the original transcripts in all supported languages.  Despite using an order of magnitude less data, Canary outperforms similarly-sized Whisper-large-v3, and SeamlessM4T-Medium-v1 on both transcription and translation tasks.</p> <p> </p> Figure 1.  Speech recognition: average WER on MCV 16.1 test sets for English, Spanish, French, and German (Lower is better).  Figure 2. Speech Translation: (left) average BLEU scores on Fleurs and MExpresso test sets translating from English to Spanish, French, and German. (right) average BLEU scores on Fleurs and CoVoST test sets translating from Spanish, French, and German to English (Higher is better). <p>Canary is an encoder-decoder model built on several innovations from the NVIDIA NeMo team. The encoder is Fast-Conformer, an efficient Conformer architecture optimized for ~3x savings on compute and ~4x savings on memory. The encoder processes audio in the form of log-mel spectrogram features and the decoder, a transformer decoder, generates output text tokens in an auto-regressive manner. The decoder is prompted with special tokens to control whether Canary performs transcription or translation. Canary also incorporates the Concatenated tokenizer, offering explicit control of output token space. </p> <p>The model weights are distributed under a research-friendly non-commercial CC BY-NC 4.0 license, while the code used to train this model is available under the Apache 2.0 license from NVIDIA NeMo Toolkit. </p>"},{"location":"blogs/2024/2024-02-canary/#transcribing-with-canary","title":"Transcribing with Canary","text":"<p>To use Canary, NVIDIA NeMo toolkit needs to be installed as a pip package as shown below. Cython and PyTorch (2.0 and above) should be installed before attempting to install NeMo Toolkit.</p> <pre><code>pip install git+https://github.com/NVIDIA/NeMo.git@r1.23.0#egg=nemo_toolkit[asr]\n</code></pre> <p>Once NeMo is installed, you can use Canary to transcribe or translate audio files as follows: <pre><code># Load Canary model \nfrom nemo.collections.asr.models import EncDecMultiTaskModel\ncanary_model = EncDecMultiTaskModel.from_pretrained('nvidia/canary-1b')\n\n# Prepare input - Example lines in transcribe_manifest.json\n{\n    # Example to trasribe En audio\n    \"audio_filepath\": \"/path/to/audio.wav\",  # path to the audio file\n    \"duration\": 40.0,  # duration of the audio in sec\n    \"taskname\": \"asr\",  # use \"asr\" for transcription and \"ast\" for Speech to Text translation.\n    \"source_lang\": \"en\",  # Set `source_lang`=`target_lang` for ASR, choices=['en','de','es','fr']; set `source_lang`='en' and `target_lang`='de' for En -&gt; De translation.\n    \"target_lang\": \"en\",  # choices=['en','de','es','fr']\n    \"pnc\": 'yes',  # whether to have PnC output, choices=['yes', 'no'] \n}\n\n{\n    # Example to translate from English audio to German text\n    \"audio_filepath\": \"/path/to/audio.wav\",  # path to the audio file\n    \"duration\": 40.0,  \n    \"taskname\": \"ast\",  \n    \"source_lang\": \"en\",  \n    \"target_lang\": \"de\", \n    \"pnc\": 'yes',\n}\n\n# Finally transcribe\ntranscript = canary_model.transcribe(paths2audio_files=\"&lt;path to transcribe_manifest.json&gt;\", batch_size=4,)\n</code></pre></p>"},{"location":"blogs/2024/2024-02-canary/#additional-resources","title":"Additional Resources","text":"<ul> <li>HuggingFace ASR Leaderboard</li> <li>NeMo Canary Model on HuggingFace</li> <li>NVIDIA NeMo Webpage</li> <li>NVIDIA NeMo ASR Documentation</li> <li>Papers:<ul> <li>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</li> <li>Unified Model for Code-Switching Speech Recognition and Language Identification Based on Concatenated Tokenizer</li> </ul> </li> </ul>"},{"location":"publications/","title":"Publications","text":""},{"location":"publications/template/","title":"Notes","text":"<ul> <li>Add folder inside <code>blogs/posts</code> directory with year</li> <li>Copy the contents of this template to the folder.</li> <li>Edit the contents.</li> <li>Add a <code>&lt;!-- more --&gt;</code> tag to the blog post to indicate where it should say 'Continue reading' in the blogpost preview.</li> <li>Send PR and merge.</li> <li>ASSETS: All blog images and external content must be hosted somewhere else. Do NOT add things to GitHub for blog contents!</li> </ul>"},{"location":"publications/template/#note-about-continue_url","title":"Note about <code>continue_url</code>","text":"<p>If \"continue_url\" is set, AND a URL is specified in \"# Article Title\", then there will be no links to the full blog post on the website, although the full blog post will be accessible if a user knows the URL to look for. So the content under the 'more' tag won't be findable by clicking, but will be findable if someone looks for the exact blog post link.</p>"},{"location":"publications/template/#article-title","title":"Article Title","text":"<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Cras in massa et lacus consectetur maximus. Donec fringilla, justo vitae condimentum feugiat, est sapien interdum purus, vel rutrum neque ex quis ipsum. Etiam in mauris odio. </p> <p>Mauris in mattis massa. Vivamus tempor libero eu ante aliquet tempor. Vestibulum porttitor odio eu ante posuere, sit amet sagittis quam auctor. Nunc sem sem, ultrices eget porta ac, vulputate non nibh. In tempor risus non felis porta, id scelerisque eros interdum.</p>"},{"location":"publications/2019/2019-jasper/","title":"Jasper: An End-to-End Convolutional Neural Acoustic Model","text":"<p>In this paper, we report state-of-the-art results on LibriSpeech among end-to-end speech recognition models without any external training data. Our model, Jasper, uses only 1D convolutions, batch normalization, ReLU, dropout, and residual connections. To improve training, we further introduce a new layer-wise optimizer called NovoGrad. Through experiments, we demonstrate that the proposed deep architecture performs as well or better than more complex choices. Our deepest Jasper variant uses 54 convolutional layers. With this architecture, we achieve 2.95% WER using a beam-search decoder with an external neural language model and 3.86% WER with a greedy decoder on LibriSpeech test-clean. We also report competitive results on the Wall Street Journal and the Hub5'00 conversational evaluation datasets. </p>"},{"location":"publications/2019/2019-quartznet/","title":"QuartzNet: Deep Automatic Speech Recognition with 1D Time-Channel Separable Convolutions","text":"<p>We propose a new end-to-end neural acoustic model for automatic speech recognition. The model is composed of multiple blocks with residual connections between them. Each block consists of one or more modules with 1D time-channel separable convolutional layers, batch normalization, and ReLU layers. It is trained with CTC loss. The proposed network achieves near state-of-the-art accuracy on LibriSpeech and Wall Street Journal, while having fewer parameters than all competing models. We also demonstrate that this model can be effectively fine-tuned on new datasets.</p>"},{"location":"publications/2020/2020-asr-correction-with-transformers/","title":"Correction of Automatic Speech Recognition with Transformer Sequence-To-Sequence Model","text":"<p>In this work, we introduce a simple yet efficient post-processing model for automatic speech recognition. Our model has Transformer-based encoder-decoder architecture which \"translates\" acoustic model output into grammatically and semantically correct text. We investigate different strategies for regularizing and optimizing the model and show that extensive data augmentation and the initialization with pretrained weights are required to achieve good performance. On the LibriSpeech benchmark, our method demonstrates significant improvement in word error rate over the baseline acoustic model with greedy decoding, especially on much noisier dev-other and test-other portions of the evaluation dataset. Our model also outperforms baseline with 6-gram language model re-scoring and approaches the performance of re-scoring with Transformer-XL neural language model.</p>"},{"location":"publications/2020/2020-asr-noise-robustness/","title":"Improving Noise Robustness of an End-to-End Neural Model for Automatic Speech Recognition","text":"<p>We present our experiments in training robust to noise an end-to-end automatic speech recognition (ASR) model using intensive data augmentation. We explore the efficacy of fine-tuning a pre-trained model to improve noise robustness, and we find it to be a very efficient way to train for various noisy conditions, especially when the conditions in which the model will be used, are unknown. Starting with a model trained on clean data helps establish baseline performance on clean speech. We carefully fine-tune this model to both maintain the performance on clean speech, and improve the model accuracy in noisy conditions. With this schema, we trained robust to noise English and Mandarin ASR models on large public corpora. All described models and training recipes are open sourced in NeMo, a toolkit for conversational AI.</p>"},{"location":"publications/2020/2020-biomegatron/","title":"BioMegatron: Larger Biomedical Domain Language Model","text":"<p>There has been an influx of biomedical domain-specific language models, showing language models pre-trained on biomedical text perform better on biomedical domain benchmarks than those trained on general domain text corpora such as Wikipedia and Books. Yet, most works do not study the factors affecting each domain language application deeply. Additionally, the study of model size on domain-specific models has been mostly missing. We empirically study and evaluate several factors that can affect performance on domain language applications, such as the sub-word vocabulary set, model size, pre-training corpus, and domain transfer. We show consistent improvements on benchmarks with our larger BioMegatron model trained on a larger domain corpus, contributing to our understanding of domain language model applications. We demonstrate noticeable improvements over the previous state-of-the-art (SOTA) on standard biomedical NLP benchmarks of question answering, named entity recognition, and relation extraction. Code and checkpoints to reproduce our experiments are available at [github.com/NVIDIA/NeMo].</p>"},{"location":"publications/2020/2020-fast-dialog-tracking/","title":"A Fast and Robust BERT-based Dialogue State Tracker for Schema-Guided Dialogue Dataset","text":"<p>Dialog State Tracking (DST) is one of the most crucial modules for goal-oriented dialogue systems. In this paper, we introduce FastSGT (Fast Schema Guided Tracker), a fast and robust BERT-based model for state tracking in goal-oriented dialogue systems. The proposed model is designed for the Schema-Guided Dialogue (SGD) dataset which contains natural language descriptions for all the entities including user intents, services, and slots. The model incorporates two carry-over procedures for handling the extraction of the values not explicitly mentioned in the current user utterance. It also uses multi-head attention projections in some of the decoders to have a better modelling of the encoder outputs. In the conducted experiments we compared FastSGT to the baseline model for the SGD dataset. Our model keeps the efficiency in terms of computational and memory consumption while improving the accuracy significantly. Additionally, we present ablation studies measuring the impact of different parts of the model on its performance. We also show the effectiveness of data augmentation for improving the accuracy without increasing the amount of computational resources.</p>"},{"location":"publications/2020/2020-matchboxnet/","title":"MatchboxNet - 1D Time-Channel Separable Convolutional Neural Network Architecture for Speech Commands Recognition","text":"<p>Abstract: We present MatchboxNet - an end-to-end neural network for speech command recognition. MatchboxNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. MatchboxNet reaches state-of-the art accuracy on the Google Speech Commands dataset while having significantly fewer parameters than similar models. The small footprint of MatchboxNet makes it an attractive candidate for devices with limited computational resources. The model is highly scalable, so model accuracy can be improved with modest additional memory and compute. Finally, we show how intensive data augmentation using an auxiliary noise dataset improves robustness in the presence of background noise.</p>"},{"location":"publications/2020/2020-speakernet/","title":"SpeakerNet: 1D Depth-wise Separable Convolutional Network for Text-Independent Speaker Recognition and Verification","text":"<p>We propose SpeakerNet - a new neural architecture for speaker recognition and speaker verification tasks. It is composed of residual blocks with 1D depth-wise separable convolutions, batch-normalization, and ReLU layers. This architecture uses x-vector based statistics pooling layer to map variable-length utterances to a fixed-length embedding (q-vector). SpeakerNet-M is a simple lightweight model with just 5M parameters. It doesn't use voice activity detection (VAD) and achieves close to state-of-the-art performance scoring an Equal Error Rate (EER) of 2.10% on the VoxCeleb1 cleaned and 2.29% on the VoxCeleb1 trial files.</p>"},{"location":"publications/2021/2021-sgdqa/","title":"SGD-QA: Fast Schema-Guided Dialogue State Tracking for Unseen Services","text":"<p>Dialogue state tracking is an essential part of goal-oriented dialogue systems, while most of these state tracking models often fail to handle unseen services. In this paper, we propose SGD-QA, a simple and extensible model for schema-guided dialogue state tracking based on a question answering approach. The proposed multi-pass model shares a single encoder between the domain information and dialogue utterance. The domain's description represents the query and the dialogue utterance serves as the context. The model improves performance on unseen services by at least 1.6x compared to single-pass baseline models on the SGD dataset. SGD-QA shows competitive performance compared to state-of-the-art multi-pass models while being significantly more efficient in terms of memory consumption and training performance. We provide a thorough discussion on the model with ablation study and error analysis.</p>"},{"location":"publications/2021/2021-carnelinet/","title":"CarneliNet: Neural Mixture Model for Automatic Speech Recognition","text":"<p>End-to-end automatic speech recognition systems have achieved great accuracy by using deeper and deeper models. However, the increased depth comes with a larger receptive field that can negatively impact model performance in streaming scenarios. We propose an alternative approach that we call Neural Mixture Model. The basic idea is to introduce a parallel mixture of shallow networks instead of a very deep network. To validate this idea we design CarneliNet -- a CTC-based neural network composed of three mega-blocks. Each mega-block consists of multiple parallel shallow sub-networks based on 1D depthwise-separable convolutions. We evaluate the model on LibriSpeech, MLS and AISHELL-2 datasets and achieved close to state-of-the-art results for CTC-based models. Finally, we demonstrate that one can dynamically reconfigure the number of parallel sub-networks to accommodate the computational requirements without retraining.</p>"},{"location":"publications/2021/2021-citrinet/","title":"Citrinet: Closing the Gap between Non-Autoregressive and Autoregressive End-to-End Models for Automatic Speech Recognition","text":"<p>We propose Citrinet - a new end-to-end convolutional Connectionist Temporal Classification (CTC) based automatic speech recognition (ASR) model. Citrinet is deep residual neural model which uses 1D time-channel separable convolutions combined with sub-word encoding and squeeze-and-excitation. The resulting architecture significantly reduces the gap between non-autoregressive and sequence-to-sequence and transducer models. We evaluate Citrinet on LibriSpeech, TED-LIUM2, AISHELL-1 and Multilingual LibriSpeech (MLS) English speech datasets. Citrinet accuracy on these datasets is close to the best autoregressive Transducer models.</p>"},{"location":"publications/2021/2021-cross-language-domain-transfer/","title":"Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition","text":"<p>In this paper, we demonstrate the efficacy of transfer learning and continuous learning for various automatic speech recognition (ASR) tasks using end-to-end models trained with CTC loss. We start with a large pre-trained English ASR model and show that transfer learning can be effectively and easily performed on: (1) different English accents, (2) different languages (from English to German, Spanish, Russian, or from Mandarin to Cantonese) and (3) application-specific domains. Our extensive set of experiments demonstrate that in all three cases, transfer learning from a good base model has higher accuracy than a model trained from scratch. Our results indicate that, for fine-tuning, larger pre-trained models are better than small pre-trained models, even if the dataset for fine-tuning is small. We also show that transfer learning significantly speeds up convergence, which could result in significant cost savings when training with large datasets.</p>"},{"location":"publications/2021/2021-ctc-segmentation/","title":"A Toolbox for Construction and Analysis of Speech Datasets","text":"<p>Automatic Speech Recognition and Text-to-Speech systems are primarily trained in a supervised fashion and require high-quality, accurately labeled speech datasets. In this work, we examine common problems with speech data and introduce a toolbox for the construction and interactive error analysis of speech datasets. The construction tool is based on K\u00fcrzinger et al. work, and, to the best of our knowledge, the dataset exploration tool is the world\u2019s first open-source tool of this kind. We demonstrate how to apply these tools to create a Russian speech dataset and analyze existing speech datasets (Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as a part of the NeMo framework.</p>"},{"location":"publications/2021/2021-ctc-wfst/","title":"CTC Variations Through New WFST Topologies","text":"<p>This paper presents novel Weighted Finite-State Transducer (WFST) topologies to implement Connectionist Temporal Classification (CTC)-like algorithms for automatic speech recognition. Three new CTC variants are proposed: (1) the \"compact-CTC\", in which direct transitions between units are replaced with  back-off transitions; (2) the \"minimal-CTC\", that only adds  self-loops when used in WFST-composition; and (3) the \"selfless-CTC\" variants, which disallows self-loop for non-blank units. Compact-CTC allows for 1.5 times smaller WFST decoding graphs and reduces memory consumption by two times when training CTC models with the LF-MMI objective without hurting the recognition accuracy. Minimal-CTC reduces graph size and memory consumption by two and four times for the cost of a small accuracy drop. Using selfless-CTC can improve the accuracy for wide context window models."},{"location":"publications/2021/2021-duplex-text-norm/","title":"A Unified Transformer-based Framework for Duplex Text Normalization","text":"<p>Text normalization (TN) and inverse text normalization (ITN) are essential preprocessing and postprocessing steps for text-to-speech synthesis and automatic speech recognition, respectively. Many methods have been proposed for either TN or ITN, ranging from weighted finite-state transducers to neural networks. Despite their impressive performance, these methods aim to tackle only one of the two tasks but not both. As a result, in a complete spoken dialog system, two separate models for TN and ITN need to be built. This heterogeneity increases the technical complexity of the system, which in turn increases the cost of maintenance in a production setting. Motivated by this observation, we propose a unified framework for building a single neural duplex system that can simultaneously handle TN and ITN. Combined with a simple but effective data augmentation method, our systems achieve state-of-the-art results on the Google TN dataset for English and Russian. They can also reach over 95% sentence-level accuracy on an internal English TN dataset without any additional fine-tuning. In addition, we also create a cleaned dataset from the Spoken Wikipedia Corpora for German and report the performance of our systems on the dataset. Overall, experimental results demonstrate the proposed duplex text normalization framework is highly effective and applicable to a range of domains and languages</p>"},{"location":"publications/2021/2021-hifi-tts/","title":"Hi-Fi Multi-Speaker English TTS Dataset","text":"<p>This paper introduces a new multi-speaker English dataset for training text-to-speech models. The dataset is based on LibriVox audiobooks and Project Gutenberg texts, both in the public domain. The new dataset contains about 292 hours of speech from 10 speakers with at least 17 hours per speaker sampled at 44.1 kHz. To select speech samples with high quality, we considered audio recordings with a signal bandwidth of at least 13 kHz and a signal-to-noise ratio (SNR) of at least 32 dB. The dataset is publicly released at \u201chttp://www.openslr.org/109/\u201d.</p>"},{"location":"publications/2020/2021-marblenet/","title":"MarbleNet: Deep 1D Time-Channel Separable Convolutional Neural Network for Voice Activity Detection","text":"<p>We present MarbleNet, an end-to-end neural network for Voice Activity Detection (VAD). MarbleNet is a deep residual network composed from blocks of 1D time-channel separable convolution, batch-normalization, ReLU and dropout layers. When compared to a state-of-the-art VAD model, MarbleNet is able to achieve similar performance with roughly 1/10-th the parameter cost. We further conduct extensive ablation studies on different training methods and choices of parameters in order to study the robustness of MarbleNet in real-world VAD tasks.</p>"},{"location":"publications/2021/2021-mixer-tts/","title":"Mixer-TTS: non-autoregressive, fast and compact text-to-speech model conditioned on language model embeddings","text":"<p>This paper describes Mixer-TTS, a non-autoregressive model for mel-spectrogram generation. The model is based on the MLP-Mixer architecture adapted for speech synthesis. The basic Mixer-TTS contains pitch and duration predictors, with the latter being trained with an unsupervised TTS alignment framework. Alongside the basic model, we propose the extended version which additionally uses token embeddings from a pre-trained language model. Basic Mixer-TTS and its extended version achieve a mean opinion score (MOS) of 4.05 and 4.11, respectively, compared to a MOS of 4.27 of original LJSpeech samples. Both versions have a small number of parameters and enable much faster speech synthesis compared to the models with similar quality.</p>"},{"location":"publications/2021/2021-nemo-itn/","title":"NeMo Inverse Text Normalization: From Development To Production","text":"<p>Inverse text normalization (ITN) converts spoken-domain automatic speech recognition (ASR) output into written-domain text to improve the readability of the ASR output. Many stateof-the-art ITN systems use hand-written weighted finite-state transducer (WFST) grammars since this task has extremely low tolerance to unrecoverable errors. We introduce an open-source Python WFST-based library for ITN which enables a seamless path from development to production. We describe the specification of ITN grammar rules for English, but the library can be adapted for other languages. It can also be used for writtento-spoken text normalization. We evaluate the NeMo ITN library using a modified version of the Google Text normalization dataset.</p>"},{"location":"publications/2021/2021-nemo-nmt/","title":"NVIDIA NeMo Neural Machine Translation Systems for English-German and English-Russian News and Biomedical Tasks at WMT21","text":"<p>This paper provides an overview of NVIDIA NeMo's neural machine translation systems for the constrained data track of the WMT21 News and Biomedical Shared Translation Tasks. Our news task submissions for English-German (En-De) and English-Russian (En-Ru) are built on top of a baseline transformer-based sequence-to-sequence model. Specifically, we use a combination of 1) checkpoint averaging 2) model scaling 3) data augmentation with backtranslation and knowledge distillation from right-to-left factorized models 4) finetuning on test sets from previous years 5) model ensembling 6) shallow fusion decoding with transformer language models and 7) noisy channel re-ranking. Additionally, our biomedical task submission for English-Russian uses a biomedically biased vocabulary and is trained from scratch on news task data, medically relevant text curated from the news task dataset, and biomedical data provided by the shared task. Our news system achieves a sacreBLEU score of 39.5 on the WMT'20 En-De test set outperforming the best submission from last year's task of 38.8. Our biomedical task Ru-En and En-Ru systems reach BLEU scores of 43.8 and 40.3 respectively on the WMT'20 Biomedical Task Test set, outperforming the previous year's best submissions.</p>"},{"location":"publications/2021/2021-protien-interactions/","title":"Text Mining Drug/Chemical-Protein Interactions using an Ensemble of BERT and T5 Based Models","text":"<p>In Track-1 of the BioCreative VII Challenge participants are asked to identify interactions between drugs/chemicals and proteins. In-context named entity annotations for each drug/chemical and protein are provided and one of fourteen different interactions must be automatically predicted. For this relation extraction task, we attempt both a BERT-based sentence classification approach, and a more novel text-to-text approach using a T5 model. We find that larger BERT-based models perform better in general, with our BioMegatron-based model achieving the highest scores across all metrics, achieving 0.74 F1 score. Though our novel T5 text-to-text method did not perform as well as most of our BERT-based models, it outperformed those trained on similar data, showing promising results, achieving 0.65 F1 score. We believe a text-to-text approach to relation extraction has some competitive advantages and there is a lot of room for research advancement.</p>"},{"location":"publications/2021/2021-spgispeech/","title":"SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition","text":"<p>In the English speech-to-text (STT) machine learning task, acoustic models are conventionally trained on uncased Latin characters, and any necessary orthography (such as capitalization, punctuation, and denormalization of non-standard words) is imputed by separate post-processing models. This adds complexity and limits performance, as many formatting tasks benefit from semantic information present in the acoustic signal but absent in transcription. Here we propose a new STT task: end-to-end neural transcription with fully formatted text for target labels. We present baseline Conformer-based models trained on a corpus of 5,000 hours of professionally transcribed earnings calls, achieving a CER of 1.7. As a contribution to the STT research community, we release the corpus free for non-commercial use.</p>"},{"location":"publications/2021/2021-talknet-1/","title":"TalkNet: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis","text":"<p>We propose TalkNet, a non-autoregressive convolutional neural model for speech synthesis with explicit pitch and duration prediction. The model consists of three feed-forward convolutional networks. The first network predicts grapheme durations. An input text is then expanded by repeating each symbol according to the predicted duration. The second network predicts pitch value for every mel frame. The third network generates a mel-spectrogram from the expanded text conditioned on predicted pitch. All networks are based on 1D depth-wise separable convolutional architecture. The explicit duration prediction eliminates word skipping and repeating. The quality of the generated speech nearly matches the best auto-regressive models \u2014 TalkNet trained on the LJSpeech dataset got a MOS of 4.08. The model has only 13.2M parameters, almost 2\u00d7 less than the present state-of-the-art text-to-speech models. The non-autoregressive architecture allows for fast training and inference. The small model size and fast inference make TalkNet an attractive candidate for embedded speech synthesis.</p>"},{"location":"publications/2021/2021-talknet-2/","title":"TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model for Speech Synthesis with Explicit Pitch and Duration Prediction","text":"<p>We propose TalkNet, a non-autoregressive convolutional neural model for speech synthesis with explicit pitch and duration prediction. The model consists of three feed-forward convolutional networks. The first network predicts grapheme durations. An input text is expanded by repeating each symbol according to the predicted duration. The second network predicts pitch value for every mel frame. The third network generates a mel-spectrogram from the expanded text conditioned on predicted pitch. All networks are based on 1D depth-wise separable convolutional architecture. The explicit duration prediction eliminates word skipping and repeating. The quality of the generated speech nearly matches the best auto-regressive models - TalkNet trained on the LJSpeech dataset got MOS 4.08. The model has only 13.2M parameters, almost 2x less than the present state-of-the-art text-to-speech models. The non-autoregressive architecture allows for fast training and inference. The small model size and fast inference make the TalkNet an attractive candidate for embedded speech synthesis.</p>"},{"location":"publications/2022/2022-accidental-learners-slid/","title":"Accidental Learners: Spoken Language Identification in Multilingual Self-Supervised Models","text":"<p>In this paper, we extend previous self-supervised approaches for language identification by experimenting with Conformer based architecture in a multilingual pre-training paradigm. We find that pre-trained speech models optimally encode language discriminatory information in lower layers. Further, we demonstrate that the embeddings obtained from these layers are significantly robust to classify unseen languages and different acoustic environments without additional training. After fine-tuning a pre-trained Conformer model on the VoxLingua107 dataset, we achieve results similar to current state-of-the-art systems for language identification. More, our model accomplishes this with 5x less parameters. We open-source the model through the NVIDIA NeMo toolkit.</p>"},{"location":"publications/2022/2022-ambernet/","title":"A Compact End-to-End Model with Local and Global Context for Spoken Language Identification","text":"<p>We introduce TitaNet-LID, a compact end-to-end neural network for Spoken Language Identification (LID) that is based on the ContextNet architecture. TitaNet-LID employs 1D depth-wise separable convolutions and Squeeze-and-Excitation layers to effectively capture local and global context within an utterance. Despite its small size, TitaNet-LID achieves performance similar to state-of-the-art models on the VoxLingua107 dataset while being 10 times smaller. Furthermore, it can be easily adapted to new acoustic conditions and unseen languages through simple fine-tuning, achieving a state-of-the-art accuracy of 88.2% on the FLEURS benchmark. Our model is scalable and can achieve a better trade-off between accuracy and speed. TitaNet-LID performs well even on short utterances less than 5s in length, indicating its robustness to input length.</p>"},{"location":"publications/2022/2022-efficient-peft/","title":"Evaluating Parameter Efficient Learning for Generation","text":"<p>Parameter efficient learning methods (PERMs) have recently gained significant attention as they provide an efficient way for pre-trained language models (PLMs) to adapt to a downstream task. However, these conclusions are mostly drawn from in-domain evaluations over the full training set. In this paper, we present comparisons between PERMs and finetuning from three new perspectives: (1) the effect of sample and model size to in-domain evaluations, (2) generalization to unseen domains and new datasets, and (3) the faithfulness of generations. Our results show that for in-domain settings (a) there is a cross point of sample size for which PERMs will perform better than finetuning when training with fewer samples, and (b) larger PLMs have larger cross points. For cross-domain and cross-dataset cases, we show that (a) Adapter (Houlsby et al., 2019) performs the best amongst all the PERMs studied here, and (b) it outperforms finetuning if the task dataset is below a certain size. We also compare the faithfulness of generations and show that PERMs can achieve better faithfulness score than finetuning, especially for small training set, by as much as 6%. Finally, we apply Adapter to MT-NLG 530b (Smith et al., 2022) and achieve new state-of-the-art results on Xsum (Narayan et al., 2018) for all ROUGE scores (ROUGE-1 49.17, ROUGE-2 27.20, ROUGE-L 40.98).</p>"},{"location":"publications/2022/2022-fusion-wfst-norm/","title":"Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization","text":"<p>Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. The approach is easy to extend and we show it is effective. It achieves comparable or better results than existing state-of-the-art TN models.</p>"},{"location":"publications/2022/2022-low-resource-nmt-adaptation/","title":"Finding the Right Recipe for Low Resource Domain Adaptation in Neural Machine Translation","text":"<p>General translation models often still struggle to generate accurate translations in specialized domains. To guide machine translation practitioners and characterize the effectiveness of domain adaptation methods under different data availability scenarios, we conduct an in-depth empirical exploration of monolingual and parallel data approaches to domain adaptation of pre-trained, third-party, NMT models in settings where architecture change is impractical. We compare data centric adaptation methods in isolation and combination. We study method effectiveness in very low resource (8k parallel examples) and moderately low resource (46k parallel examples) conditions and propose an ensemble approach to alleviate reductions in original domain translation quality. Our work includes three domains: consumer electronic, clinical, and biomedical and spans four language pairs - Zh-En, Ja-En, Es-En, and Ru-En. We also make concrete recommendations for achieving high in-domain performance and release our consumer electronic and medical domain datasets for all languages and make our code publicly available.</p>"},{"location":"publications/2022/2022-multi-speaker-tts-adapter/","title":"Adapter-Based Extension of Multi-Speaker Text-to-Speech Model for New Speakers","text":"<p>Fine-tuning is a popular method for adapting text-to-speech (TTS) models to new speakers. However this approach has some challenges. Usually fine-tuning requires several hours of high quality speech per speaker. There is also that fine-tuning will negatively affect the quality of speech synthesis for previously learnt speakers. In this paper we propose an alternative approach for TTS adaptation based on using parameter-efficient adapter modules. In the proposed approach, a few small adapter modules are added to the original network. The original weights are frozen, and only the adapters are fine-tuned on speech for new speaker. The parameter-efficient fine-tuning approach will produce a new model with high level of parameter sharing with original model. Our experiments on LibriTTS, HiFi-TTS and VCTK datasets validate the effectiveness of adapter-based method through objective and subjective metrics.</p>"},{"location":"publications/2022/2022-multiblank-rnnt/","title":"Multi-blank Transducers for Speech Recognition","text":"<p>This paper proposes a modification to RNN-Transducer (RNN-T) models for automatic speech recognition (ASR). In standard RNN-T, the emission of a blank symbol consumes exactly one input frame; in our proposed method, we introduce additional blank symbols, which consume two or more input frames when emitted. We refer to the added symbols as big blanks, and the method multi-blank RNN-T. For training multi-blank RNN-Ts, we propose a novel logit under-normalization method in order to prioritize emissions of big blanks. With experiments on multiple languages and datasets, we show that multi-blank RNN-T methods could bring relative speedups of over +90%/+139% to model inference for English Librispeech and German Multilingual Librispeech datasets, respectively. The multi-blank RNN-T method also improves ASR accuracy consistently. We will release our implementation of the method in the NeMo (\\url{this https URL}) toolkit.</p>"},{"location":"publications/2022/2022-speech-translation/","title":"NVIDIA NeMo Offline Speech Translation Systems for IWSLT 2022","text":"<p>This paper provides an overview of NVIDIA NeMo\u2019s speech translation systems for the IWSLT 2022 Offline Speech Translation Task. Our cascade system consists of 1) Conformer RNN-T automatic speech recognition model, 2) punctuation-capitalization model based on pre-trained T5 encoder, 3) ensemble of Transformer neural machine translation models fine-tuned on TED talks. Our end-to-end model has less parameters and consists of Conformer encoder and Transformer decoder. It relies on the cascade system by re-using its pre-trained ASR encoder and training on synthetic translations generated with the ensemble of NMT models. Our En-&gt;De cascade and end-to-end systems achieve 29.7 and 26.2 BLEU on the 2020 test set correspondingly, both outperforming the previous year\u2019s best of 26 BLEU.</p>"},{"location":"publications/2022/2022-thutmose-tagger/","title":"Thutmose Tagger: Single-pass neural model for Inverse Text Normalization","text":"<p>Inverse text normalization (ITN) is an essential post-processing step in automatic speech recognition (ASR). It converts numbers, dates, abbreviations, and other semiotic classes from the spoken form generated by ASR to their written forms. One can consider ITN as a Machine Translation task and use neural sequence-to-sequence models to solve it. Unfortunately, such neural models are prone to hallucinations that could lead to unacceptable errors. To mitigate this issue, we propose a single-pass token classifier model that regards ITN as a tagging task. The model assigns a replacement fragment to every input token or marks it for deletion or copying without changes. We present a dataset preparation method based on the granular alignment of ITN examples. The proposed model is less prone to hallucination errors. The model is trained on the Google Text Normalization dataset and achieves state-of-the-art sentence accuracy on both English and Russian test sets. One-to-one correspondence between tags and input words improves the interpretability of the model's predictions, simplifies debugging, and allows for post-processing corrections. The model is simpler than sequence-to-sequence models and easier to optimize in production settings. The model and the code to prepare the dataset is published as part of NeMo project.</p>"},{"location":"publications/2022/2022-titanet/","title":"TitaNet: Neural Model for Speaker Representation with 1D Depth-Wise Separable Convolutions and Global Context","text":"<p>In this paper, we propose TitaNet, a novel neural network architecture for extracting speaker representations. We employ 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with global context followed by channel attention based statistics pooling layer to map variable-length utterances to a fixed-length embedding (t-vector). TitaNet is a scalable architecture and achieves state-of-the-art performance on speaker verification task with an equal error rate (EER) of 0.68% on the VoxCeleb1 trial file and also on speaker diarization tasks with diarization error rate (DER) of 1.73% on AMI-MixHeadset, 1.99% on AMI-Lapel and 1.11% on CH109. Furthermore, we investigate various sizes of TitaNet and present a light TitaNet-S model with only 6M parameters that achieve near state-of-the-art results in diarization tasks.</p>"},{"location":"publications/2023/2023-acevc/","title":"ACE-VC: Adaptive and Controllable Voice Conversion using Explicitly Disentangled Self-supervised Speech Representations","text":"<p>In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers.</p>"},{"location":"publications/2023/2023-adapter-domain-adaptation/","title":"Damage Control During Domain Adaptation for Transducer Based Automatic Speech Recognition","text":"<p>Automatic speech recognition models are often adapted to improve their accuracy in a new domain. A potential drawback of model adaptation to new domains is catastrophic forgetting, where the Word Error Rate on the original domain is significantly degraded. This paper addresses the situation when we want to simultaneously adapt automatic speech recognition models to a new domain and limit the degradation of accuracy on the original domain without access to the original training dataset. We propose several techniques such as a limited training strategy and regularized adapter modules for the Transducer encoder, prediction, and joiner network. We apply these methods to the Google Speech Commands and to the UK and Ireland English Dialect speech data set and obtain strong results on the new target domain while limiting the degradation on the original domain.</p>"},{"location":"publications/2023/2023-confidence-ensembles/","title":"Confidence-based Ensembles of End-to-End Speech Recognition Models","text":"<p>The number of end-to-end speech recognition models grows every year. These models are often adapted to new domains or languages resulting in a proliferation of expert systems that achieve great results on target data, while generally showing inferior performance outside of their domain of expertise. We explore combination of such experts via confidence-based ensembles: ensembles of models where only the output of the most-confident model is used. We assume that models' target data is not available except for a small validation set. We demonstrate effectiveness of our approach with two applications. First, we show that a confidence-based ensemble of 5 monolingual models outperforms a system where model selection is performed via a dedicated language identification block. Second, we demonstrate that it is possible to combine base and adapted models to achieve strong results on both original and target data. We validate all our results on multiple datasets and model architectures.</p>"},{"location":"publications/2023/2023-entropy-word-confidence/","title":"Fast Entropy-Based Methods of Word-Level Confidence Estimation for End-to-End Automatic Speech Recognition","text":"<p>This paper presents a class of new fast non-trainable entropy-based confidence estimation methods for automatic speech recognition. We show how per-frame entropy values can be normalized and aggregated to obtain a confidence measure per unit and per word for Connectionist Temporal Classification (CTC) and Recurrent Neural Network Transducer (RNN-T) models. Proposed methods have similar computational complexity to the traditional method based on the maximum per-frame probability, but they are more adjustable, have a wider effective threshold range, and better push apart the confidence distributions of correct and incorrect words. We evaluate the proposed confidence measures on LibriSpeech test sets, and show that they are up to 2 and 4 times better than confidence estimation based on the maximum per-frame probability at detecting incorrect words for Conformer-CTC and Conformer-RNN-T models, respectively.</p>"},{"location":"publications/2023/2023-fast-conformer/","title":"Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition","text":"<p>Conformer-based models have become the dominant end-to-end architecture for speech processing tasks. With the objective of enhancing the conformer architecture for efficient training and inference, we carefully redesigned Conformer with a novel downsampling schema. The proposed model, named Fast Conformer(FC), is 2.8x faster than the original Conformer, supports scaling to Billion parameters without any changes to the core architecture and also achieves state-of-the-art accuracy on Automatic Speech Recognition benchmarks. To enable transcription of long-form speech up to 11 hours, we replaced global attention with limited context attention post-training, while also improving accuracy through fine-tuning with the addition of a global token. Fast Conformer, when combined with a Transformer decoder also outperforms the original Conformer in accuracy and in speed for Speech Translation and Spoken Language Understanding.</p>"},{"location":"publications/2023/2023-guardrails/","title":"NeMo Guardrails: A Toolkit for Controllable and Safe LLM Applications with Programmable Rails","text":"<p>NeMo Guardrails is an open-source toolkit for easily adding programmable guardrails to LLM-based conversational systems. Guardrails (or rails for short) are a specific way of controlling the output of an LLM, such as not talking about topics considered harmful, following a predefined dialogue path, using a particular language style, and more. There are several mechanisms that allow LLM providers and developers to add guardrails that are embedded into a specific model at training, e.g. using model alignment. Differently, using a runtime inspired from dialogue management, NeMo Guardrails allows developers to add programmable rails to LLM applications - these are user-defined, independent of the underlying LLM, and interpretable. Our initial results show that the proposed approach can be used with several LLM providers to develop controllable and safe LLM applications using programmable rails.</p>"},{"location":"publications/2023/2023-investigate-long-form-asr/","title":"Investigating End-to-End ASR Architectures for Long Form Audio Transcription","text":"<p>This paper presents an overview and evaluation of some of the end-to-end ASR models on long-form audios. We study three categories of Automatic Speech Recognition(ASR) models based on their core architecture: (1) convolutional, (2) convolutional with squeeze-and-excitation and (3) convolutional models with attention. We selected one ASR model from each category and evaluated Word Error Rate, maximum audio length and real-time factor for each model on a variety of long audio benchmarks: Earnings-21 and 22, CORAAL, and TED-LIUM3. The model from the category of self-attention with local attention and global token has the best accuracy comparing to other architectures. We also compared models with CTC and RNNT decoders and showed that CTC-based models are more robust and efficient than RNNT on long form audio.</p>"},{"location":"publications/2023/2023-nemo-forced-aligner/","title":"NeMo Forced Aligner and its application to word alignment for subtitle generation","text":"<p>We present NeMo Forced Aligner (NFA): an efficient and accurate forced aligner which is part of the NeMo conversational AI open-source toolkit. NFA can produce token, word, and segment-level alignments, and can generate subtitle files for highlighting words or tokens as they are spoken. We present a demo which shows this functionality, and demonstrate that NFA has the best word alignment accuracy and speed of alignment generation compared with other aligners.</p>"},{"location":"publications/2023/2023-steerlm/","title":"SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF","text":"<p>Model alignment with human preferences is an essential step in making Large Language Models (LLMs) helpful and consistent with human values. It typically consists of supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) stages. However, RLHF faces inherent limitations stemming from a complex training setup and its tendency to align the model with implicit values that end users cannot control at run-time. Moreover, reward models in RLHF stage commonly rely on single-dimensional feedback as opposed to explicit, multifaceted signals that indicate attributes such as helpfulness, humor, and toxicity. To address these limitations, we propose SteerLM, a supervised fine-tuning method that empowers end-users to control responses during inference. SteerLM conditions responses to conform to an explicitly defined multi-dimensional set of attributes, thereby empowering a steerable AI capable of generating helpful and high-quality responses while maintaining customizability. Experiments show that SteerLM trained on open source datasets generates responses that are preferred by human and automatic evaluators to many state-of-the-art baselines trained with RLHF while being much easier to train. Try SteerLM at https://huggingface.co/nvidia/SteerLM-llama2-13B</p>"},{"location":"publications/2023/2023-synthetic-nmt/","title":"Leveraging Synthetic Targets for Machine Translation","text":"<p>In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.</p>"},{"location":"publications/2023/2023-token-duration-transducer/","title":"Efficient Sequence Transduction by Jointly Predicting Tokens and Durations","text":"<p>This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (this https URL) toolkit.</p>"},{"location":"publications/2024/2024-boring-problems-tn/","title":"A Chat about Boring Problems: Studying GPT-Based Text Normalization","text":"<p>Text normalization - the conversion of text from written to spoken form - is traditionally assumed to be an ill-formed task for language modeling. In this work, we argue otherwise. We empirically show the capacity of Large-Language Models (LLM) for text normalization in few-shot scenarios. Combining self-consistency reasoning with linguistic-informed prompt engineering, we find LLM-based text normalization to achieve error rates approximately 40% lower than production-level normalization systems. Further, upon error analysis, we note key limitations in the conventional design of text normalization tasks. We create a new taxonomy of text normalization errors and apply it to results from GPT-3.5-Turbo and GPT-4.0. Through this new framework, we identify strengths and weaknesses of LLM-based TN, opening opportunities for future work.</p>"},{"location":"publications/2024/2024-cacheaware-streaming/","title":"Stateful Conformer with Cache-based Inference for Streaming Automatic Speech Recognition","text":"<p>In this paper, we propose an efficient and accurate streaming speech recognition model based on the FastConformer architecture. We adapted the FastConformer architecture for streaming applications through: (1) constraining both the look-ahead and past contexts in the encoder, and (2) introducing an activation caching mechanism to enable the non-autoregressive encoder to operate autoregressively during inference. The proposed model is thoughtfully designed in a way to eliminate the accuracy disparity between the train and inference time which is common for many streaming models. Furthermore, our proposed encoder works with various decoder configurations including Connectionist Temporal Classification (CTC) and RNN-Transducer (RNNT) decoders. Additionally, we introduced a hybrid CTC/RNNT architecture which utilizes a shared encoder with both a CTC and RNNT decoder to boost the accuracy and save computation. We evaluate the proposed model on LibriSpeech dataset and a multi-domain large scale dataset and demonstrate that it can achieve better accuracy with lower latency and inference time compared to a conventional buffered streaming model baseline. We also showed that training a model with multiple latencies can achieve better accuracy than single latency models while it enables us to support multiple latencies with a single model. Our experiments also showed the hybrid architecture would not only speedup the convergence of the CTC decoder but also improves the accuracy of streaming models compared to single decoder models. </p>"},{"location":"publications/2024/2024-selfvc/","title":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","text":"<p>We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on explicitly disentangling speech representations to separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss by discarding finer nuances of the original signal. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. In this training approach, the current state of the synthesis model is used to generate voice-converted variations of an utterance, which serve as inputs for the reconstruction task, ensuring a continuous and purposeful refinement of the model. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. SelfVC is trained without any text and is applicable to a range of tasks such as zero-shot voice conversion, cross-lingual voice conversion, and controllable speech synthesis with pitch and pace modifications. SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.</p>"},{"location":"blogs/archive/2024/","title":"2024","text":""},{"location":"blogs/archive/2023/","title":"2023","text":""},{"location":"blogs/archive/2022/","title":"2022","text":""},{"location":"blogs/category/announcements/","title":"Announcements","text":""},{"location":"blogs/category/technical-deep-dive/","title":"Technical deep-dive","text":""},{"location":"blogs/category/papers/","title":"Papers","text":""},{"location":"blogs/category/nvidia-technical-blog/","title":"NVIDIA Technical blog","text":""},{"location":"publications/archive/2024/","title":"2024","text":""},{"location":"publications/archive/2023/","title":"2023","text":""},{"location":"publications/archive/2022/","title":"2022","text":""},{"location":"publications/archive/2021/","title":"2021","text":""},{"location":"publications/archive/2020/","title":"2020","text":""},{"location":"publications/archive/2019/","title":"2019","text":""},{"location":"publications/category/voice-conversion/","title":"Voice Conversion","text":""},{"location":"publications/category/speech-generation/","title":"Speech Generation","text":""},{"location":"publications/category/text-normalization/","title":"Text Normalization","text":""},{"location":"publications/category/large-language-models/","title":"Large-Language Models","text":""},{"location":"publications/category/automatic-speech-recognition/","title":"Automatic Speech Recognition","text":""},{"location":"publications/category/natural-language-processing/","title":"Natural Language Processing","text":""},{"location":"publications/category/tools/","title":"Tools","text":""},{"location":"publications/category/neural-machine-translation/","title":"Neural Machine Translation","text":""},{"location":"publications/category/speech-classification/","title":"Speech Classification","text":""},{"location":"publications/category/text-to-speech/","title":"Text to Speech","text":""},{"location":"publications/category/inverse-text-normalization/","title":"(Inverse) Text Normalization","text":""},{"location":"publications/category/speech-translation/","title":"Speech Translation","text":""},{"location":"publications/category/speaker-recognition/","title":"Speaker Recognition","text":""},{"location":"publications/category/dialog-state-tracking/","title":"Dialog State Tracking","text":""},{"location":"publications/page/2/","title":"Publications","text":""},{"location":"publications/page/3/","title":"Publications","text":""},{"location":"publications/page/4/","title":"Publications","text":""},{"location":"publications/page/5/","title":"Publications","text":""},{"location":"publications/archive/2023/page/2/","title":"2023","text":""},{"location":"publications/archive/2021/page/2/","title":"2021","text":""},{"location":"publications/category/automatic-speech-recognition/page/2/","title":"Automatic Speech Recognition","text":""}]}