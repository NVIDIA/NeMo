variables: &VARS
  HP_REGISTRY: "gitlab-master.nvidia.com:5005/dl/joc/bignlp-hp-tool"
  HP_REGISTRY_SRUN: "gitlab-master.nvidia.com#dl/joc/bignlp-hp-tool"
  HP_BASE_TRAINING_IMAGE: "nvcr.io/nvidian/bignlp-training:22.09.04-py"
  BUILD_IMAGE_TRAINING_NAME_SRUN: ${HP_REGISTRY_SRUN}/bignlp_training_hp_tool:pipe.${CI_PIPELINE_ID}
  HP_CI_PATH: "/lustre/fsw/joc/big_nlp/hp_tool_ci"
  PIPELINE_DIR: "${HP_CI_PATH}/${CI_PIPELINE_ID}"

stages:
  - build
  - generate_docs
  - deploy_docs
  - test
  - cleanup

################
# JOB Templates
################

docs:sphinx_generate_docs_html:
  tags:
    - V100
  stage: generate_docs
  script:
    - python3 -m pip install sphinx
    - python3 -m pip install sphinx-rtd-theme
    - export PATH="/home/gitlab-runner/.local/bin:$PATH"
    - cd docs
    - make clean
    - make html
    - cd ..
    - mkdir public
    - mv docs/_build/html/* ./public/
  artifacts:
    paths:
    - public
  rules:
    - when: always

pages:
  stage: deploy_docs
  script:
    - echo "The site will be deployed to $CI_PAGES_URL"
  artifacts:
    paths:
      - public
  rules:
    - if: '$CI_COMMIT_REF_NAME == $CI_DEFAULT_BRANCH'


test:unit_tests:
  tags:
    - V100
  stage: test
  script:
    - pip install -r requirements.txt
    - pip install pytest
    - export PATH="/home/gitlab-runner/.local/bin:$PATH"
    - pytest tests/unit_tests
  rules:
    - when: always

.build: &build_template
  stage: build
  script:
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${CI_REGISTRY}"
    - export DOCKER_REGISTRY="${CI_REGISTRY/:5005/}"
    - docker login -u gitlab-ci-token -p $CI_JOB_TOKEN "${DOCKER_REGISTRY}"
    - docker login -u "\$oauthtoken" -p $NGC_CLI_API_KEY nvcr.io
    - set -x
    - ls
    - env
    - export FROM_IMAGE_ARG="--build-arg FROM_IMAGE_NAME=${HP_BASE_IMAGE}"
    - docker build -t ${BUILD_IMAGE_NAME} ${FROM_IMAGE_ARG} .
    - docker push ${BUILD_IMAGE_NAME}
  allow_failure: false
  tags:
    - vm-builder

before_script:
  - umask 0007
  - export DOCKERFILE=./Dockerfile

.LUNA: &LUNA
  variables: &LUNA_VARS
    SLURM_PARTITION: "luna"
    SLURM_NIGHTLY_PARTITION: "luna"
    SLURM_ACCOUNT: "joc"
    CLUSTER:       "selene"
    PYXIS_LITE:    "1"
    ENROOT_MOUNT_HOME: "n"
    GIT_CLONE_PATH: $CI_BUILDS_DIR/$SLURM_ACCOUNT/big_nlp/hp_tool_ci/$CI_PIPELINE_ID/$CI_JOB_ID/$CI_PROJECT_NAME # THIS DOES NOT HAVE QUOTES FOR A REASON
    EXCLUDE_NODES: ""
    GPU_ARCH:      "A100"
  tags: &LUNA_TAGS
    - selene_ssh

.hp-tool-LUNA-test-LAUNCHER: &hp-tool-LUNA-test-LAUNCHER
  tags: *LUNA_TAGS
  stage: test
  script: &hp-tool-LUNA-test-LAUNCHER-SCRIPT
    - chmod 774 ./* -R
    - umask 0007
    - source /lustre/fsw/joc/big_nlp/nemo_gpt3/my_venv/bin/activate
    - set -x
    - export RUN_NAME=${RUN_TASK}_${RUN_MODEL}_${RUN_JOB_NAME}
    - export RESULTS_DIR=${BASE_RESULTS_DIR}/${RUN_MODEL}/${RUN_SIZE}_${GPU_MEM}gb
    - env
    # Emulate "enroot login" since enroot is not available on the head node
    - export CONFIG_DIR="${HP_CI_PATH}/.config/${CI_JOB_ID}" && echo "CONFIG_DIR=${CONFIG_DIR}"
    - mkdir -p "${CONFIG_DIR}" && chmod 755 "${CONFIG_DIR}"
    - export ENROOT_CONFIG_PATH="${CONFIG_DIR}/.enroot" && mkdir -p "${ENROOT_CONFIG_PATH}"
    - echo "machine ${CI_REGISTRY/:5005/} login gitlab-ci-token password ${CI_BUILD_TOKEN}"
    - echo "machine ${CI_REGISTRY/:5005/} login gitlab-ci-token password ${CI_BUILD_TOKEN}" > $ENROOT_CONFIG_PATH/.credentials  
    # Clone bignlp-scripts
    - cd ..
    - git clone ssh://git@gitlab-master.nvidia.com:12051/dl/JoC/bignlp-scripts.git
    - cd bignlp-hp-tool
    - bash tests/ci_tests/selene/scripts/${RUN_TASK}/${RUN_MODEL}/${RUN_JOB_NAME}.sh
    # Wait for job to launch
    - sleep 10s # Without this, "sacct" in jobstate.sh does not always find the SLURM job.
    - export SLURM_JOBID=$(grep 'Submitted batch job' "${RESULTS_DIR}/launcher.log" | awk '{ print $4 }')
    - echo $SLURM_JOBID
    - export SLURM_OUTPUT=${RESULTS_DIR}/slurm_${SLURM_JOBID}.log
    #export SLURM_OUTPUT=$(scontrol show job "${SLURM_JOBID}" | grep 'StdOut' | awk -F '=' '{ print $2 }')
    - cd tests/ci_tests/utils
    - chmod 777 ./* -R
    - bash jobwait.sh "${SLURM_JOBID}" & PID=$!
    - touch "${SLURM_OUTPUT}"
    - \[ ! -z ${SLURM_JOBID} \] && echo -e " --------------------------------------------------\n"
                "----------WAITING FOR SLURM JOB TO BEGIN-----------\n"
                "---------------------------------------------------\n"
                "$(scontrol show job=${SLURM_JOBID})\n"
                "---------------------------------------------------\n"
    # Gitlab logs collapsible section markers
    - echo -e "\e[0Ksection_end:`date +%s`:slurm_setup\r\e[0K"
    # Follow output of the job
    - tail --pid="${PID}" -f "${SLURM_OUTPUT}" # Stream job output until it finishes.
    - echo "Finished job with name ${RUN_NAME}"
    - cd ${GIT_CLONE_PATH}

    # Run Pytest
    - pip3 install pytest
    - pytest tests/ci_tests/selene/pytest/${RUN_TASK}/${RUN_MODEL}/test_${RUN_JOB_NAME}.py
    - echo "Finished pytest job"
  allow_failure: false


build-BigNLP-training:
  <<: *build_template
  variables:
    <<: [*VARS]
    IMAGE_NAME: bignlp_training_hp_tool
    HP_BASE_IMAGE: ${HP_BASE_TRAINING_IMAGE}
    BUILD_IMAGE_TRAINING_NAME: ${HP_REGISTRY}/${IMAGE_NAME}:pipe.${CI_PIPELINE_ID}
    BUILD_IMAGE_NAME: ${BUILD_IMAGE_TRAINING_NAME}
  rules:
    - when: always

train.gpt3.126m_80gb_2runs_8nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 0.126b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 8
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: always

train.gpt3.126m_40gb_2runs_8nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 0.126b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 40
    RUNS: 2
    NODES: 8
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: always

train.gpt3.5b_80gb_2runs_16nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 5b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 16
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: always

train.gpt3.5b_40gb_2runs_16nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 5b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 40
    RUNS: 2
    NODES: 16
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: always

train.gpt3.20b_80gb_2runs_64nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 20b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 64
    TP_SIZES: "[2,4]"
    PP_SIZES: "[1]"
    MBS_SIZES: "[1]"
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: always

train.t5.220m_80gb_2runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 0.22b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: always

train.t5.220m_40gb_2runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 0.22b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 40
    RUNS: 2
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: always

train.t5.2p8b_80gb_2runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 2.8b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: always

train.t5.2p8b_40gb_2runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 2.8b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 40
    RUNS: 2
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: always

train.mt5.170m_80gb_2runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 0.17b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: always

train.mt5.170m_40gb_2runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 0.17b
    GPU_MEM: 40
    RUNS: 2
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: always

train.mt5.3p2b_80gb_2runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 3.2b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    RUNS: 2
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: always

train.mt5.3p2b_40gb_2runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 3.2b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 40
    RUNS: 2
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: always


###### BENCHMARKS ######
benchmark.gpt3.126m_80gb_100runs_1nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 0.126b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 1
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.126m_80gb_100runs_8nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 0.126b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 8
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.5b_80gb_100runs_16nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 5b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 16
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.20b_80gb_100runs_64nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 20b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 64
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.40b_80gb_100runs_128nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 40b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 128
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.175b_80gb_100runs_128nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 175b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 128
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.gpt3.175b_80gb_100runs_256nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: gpt3
    RUN_SIZE: 175b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 256
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: gpt3_training_search
  rules:
    - when: manual

benchmark.t5.220m_80gb_100runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 0.22b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: manual
    
benchmark.t5.2.8b_80gb_100runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 2.8b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: manual

benchmark.t5.11b_80gb_100runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 11b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: manual

benchmark.t5.23.5b_80gb_100runs_40nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 23.5b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 40
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: manual

benchmark.t5.41.2b_80gb_100runs_40nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: t5
    RUN_SIZE: 41.2b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 40
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: t5_training_search
  rules:
    - when: manual

benchmark.mt5.170m_80gb_100runs_4nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 0.17b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 4
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

benchmark.mt5.390m_80gb_100runs_8nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 0.39b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 8
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

benchmark.mt5.3.2b_80gb_100runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 3.2b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

benchmark.mt5.11.9b_80gb_100runs_20nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 11.9b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 20
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

benchmark.mt5.24.65b_80gb_100runs_40nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 24.65b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 40
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

benchmark.mt5.42.54b_80gb_100runs_40nodes:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: train
    RUN_MODEL: mt5
    RUN_SIZE: 42.54b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/benchmark_results"
    RUNS: 100
    GPU_MEM: 80
    NODES: 40
    TP_SIZES: auto
    PP_SIZES: auto
    MBS_SIZES: auto
    RUN_JOB_NAME: mt5_training_search
  rules:
    - when: manual

# =========== INFERENCE =============

inference.gpt3.126m_80gb_2runs_1node:
  <<: *hp-tool-LUNA-test-LAUNCHER
  variables:
    <<: [*VARS, *LUNA_VARS]
    RUN_TASK: inference
    RUN_MODEL: gpt3
    RUN_SIZE: 0.126b
    BASE_RESULTS_DIR: "${PIPELINE_DIR}/results"
    GPU_MEM: 80
    TP_SIZES: "[1]"
    PP_SIZES: "[1]"
    BS_SIZES: "[2,4]"
    RUN_JOB_NAME: gpt3_inference_search
  rules:
    - when: always


# ======= CLEANUP ========
cleanup.selene:
  tags: *LUNA_TAGS
  stage: cleanup
  variables:
    <<: [*VARS, *LUNA_VARS]
  script:
    - rm -rf ${CI_BUILDS_DIR}/${SLURM_ACCOUNT}/big_nlp/hp_tool_ci/*
    - echo "Finished cleaning everything on Selene"
  allow_failure: true
  rules:
    - when: manual
