

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; nemo 0.11.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Tutorial" href="question_answering.html" />
    <link rel="prev" title="Tutorial" href="ner.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.11.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speaker_recognition/intro.html">Speaker Recognition</a></li>
<li class="toctree-l1"><a class="reference internal" href="../speech_command/intro.html">Speech Commands</a></li>
<li class="toctree-l1"><a class="reference internal" href="../voice_activity_detection/intro.html">Voice Activity Detection</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Natural Language Processing</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#pretraining-bert">Pretraining BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#transformer-language-model">Transformer Language Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#megatron-lm-for-downstream-tasks">Megatron-LM for Downstream tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#glue-benchmark">GLUE Benchmark</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#intent-and-slot-filling">Intent and Slot filling</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#text-classification">Text Classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#named-entity-recognition">Named Entity Recognition</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="intro.html#punctuation-and-word-capitalization">Punctuation and Word Capitalization</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#task-description">Task Description</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dataset">Dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#code-overview">Code overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#inference">Inference</a></li>
<li class="toctree-l4"><a class="reference internal" href="#training-and-inference-scripts">Training and inference scripts</a></li>
<li class="toctree-l4"><a class="reference internal" href="#multi-gpu-training">Multi GPU Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#question-answering">Question Answering</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#dialogue-state-tracking">Dialogue State Tracking</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#asr-postprocessing-with-bert">ASR Postprocessing with BERT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../tts/intro.html">Speech Synthesis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../chinese/intro.html">中文支持</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Natural Language Processing</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/nlp/punctuation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<span id="punctuation"></span><h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>An ASR system typically generates text with no punctuation and capitalization of the words. This tutorial explains how to implement a model in NeMo that will predict punctuation and capitalization for each word in a sentence to make ASR output more readable and to boost performance of the downstream tasks such as name entity recognition or machine translation. We’ll show how to train network for this task using a pre-trained BERT model.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We recommend you to try this example in Jupyter notebook examples/nlp/token_classification/PunctuationWithBERT.ipynb.</p>
<p>All code used in this tutorial is based on <a class="reference internal" href="#punct-scripts"><span class="std std-ref">Training and inference scripts</span></a>.
For pretraining BERT in NeMo and pretrained model checkpoints go to <a class="reference external" href="https://nvidia.github.io/NeMo/nlp/bert_pretraining.html">BERT pretraining</a>.</p>
</div>
<div class="section" id="task-description">
<h2>Task Description<a class="headerlink" href="#task-description" title="Permalink to this headline">¶</a></h2>
<p>For every word in our training dataset we’re going to predict:</p>
<ol class="arabic simple">
<li><p>punctuation mark that should follow the word and</p></li>
<li><p>whether the word should be capitalized</p></li>
</ol>
<p>In this model, we’re jointly training 2 token-level classifiers on top of the pretrained BERT model: one classifier to predict punctuation and the other one - capitalization.</p>
</div>
<div class="section" id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<p>This model can work with any dataset as long as it follows the format specified below. For this tutorial, we’re going to use the <a class="reference external" href="https://tatoeba.org/eng">Tatoeba collection of sentences</a>. <a class="reference external" href="https://github.com/NVIDIA/NeMo/blob/master/examples/nlp/token_classification/get_tatoeba_data.py">This</a> script downloads and preprocesses the dataset.</p>
<p>The training and evaluation data is divided into 2 files: text.txt and labels.txt. Each line of the text.txt file contains text sequences, where words are separated with spaces:
[WORD] [SPACE] [WORD] [SPACE] [WORD], for example:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">when</span> <span class="ow">is</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">flight</span> <span class="n">to</span> <span class="n">new</span> <span class="n">york</span>
<span class="n">the</span> <span class="nb">next</span> <span class="n">flight</span> <span class="ow">is</span> <span class="o">...</span>
<span class="o">...</span>
</pre></div>
</div>
</div></blockquote>
<p>The labels.txt file contains corresponding labels for each word in text.txt, the labels are separated with spaces.
Each label in labels.txt file consists of 2 symbols:</p>
<ul class="simple">
<li><p>the first symbol of the label indicates what punctuation mark should follow the word (where <code class="docutils literal notranslate"><span class="pre">O</span></code> means no punctuation needed);</p></li>
<li><p>the second symbol determines if the word needs to be capitalized or not (where <code class="docutils literal notranslate"><span class="pre">U</span></code> indicates that the associated with this label word should be upper cased, and <code class="docutils literal notranslate"><span class="pre">O</span></code> - no capitalization needed.)</p></li>
</ul>
<p>We’re considering only commas, periods, and question marks for this task; the rest punctuation marks were removed.
Each line of the labels.txt should follow the format:
[LABEL] [SPACE] [LABEL] [SPACE] [LABEL] (for labels.txt). For example, labels for the above text.txt file should be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>OU OO OO OO OO OO OU ?U
OU OO OO OO ...
...
</pre></div>
</div>
<p>The complete list of all possible labels for this task is: <code class="docutils literal notranslate"><span class="pre">OO</span></code>, <code class="docutils literal notranslate"><span class="pre">,O</span></code>, <code class="docutils literal notranslate"><span class="pre">.O</span></code>, <code class="docutils literal notranslate"><span class="pre">?O</span></code>, <code class="docutils literal notranslate"><span class="pre">OU</span></code>, <code class="docutils literal notranslate"><span class="pre">,U</span></code>, <code class="docutils literal notranslate"><span class="pre">.U</span></code>, <code class="docutils literal notranslate"><span class="pre">?U</span></code>.</p>
</div>
<div class="section" id="code-overview">
<h2>Code overview<a class="headerlink" href="#code-overview" title="Permalink to this headline">¶</a></h2>
<p>First, let’s set some parameters that we’re going to need through out this tutorial:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DATA_DIR</span> <span class="o">=</span> <span class="s2">&quot;PATH_TO_WHERE_THE_DATA_IS&quot;</span>
<span class="n">WORK_DIR</span> <span class="o">=</span> <span class="s2">&quot;PATH_TO_WHERE_TO_STORE_CHECKPOINTS_AND_LOGS&quot;</span>
<span class="n">PRETRAINED_BERT_MODEL</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span>

<span class="c1"># model parameters</span>
<span class="n">BATCHES_PER_STEP</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">CLASSIFICATION_DROPOUT</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">MAX_SEQ_LENGTH</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">NUM_EPOCHS</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">LEARNING_RATE</span> <span class="o">=</span> <span class="mf">0.00002</span>
<span class="n">LR_WARMUP_PROPORTION</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">OPTIMIZER</span> <span class="o">=</span> <span class="s2">&quot;adam&quot;</span>
<span class="n">STEP_FREQ</span> <span class="o">=</span> <span class="mi">200</span> <span class="c1"># determines how often loss will be printed and checkpoint saved</span>
<span class="n">PUNCT_NUM_FC_LAYERS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">NUM_SAMPLES</span> <span class="o">=</span> <span class="mi">100000</span>
</pre></div>
</div>
</div></blockquote>
<p>To download and preprocess a subset of the Tatoeba collection of sentences, run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python get_tatoeba_data.py --data_dir DATA_DIR --num_sample NUM_SAMPLES
</pre></div>
</div>
<p>Then, we need to create our neural factory with the supported backend. This tutorial assumes that you’re training on a single GPU, with mixed precision (<code class="docutils literal notranslate"><span class="pre">optimization_level=&quot;O1&quot;</span></code>). If you don’t want to use mixed precision, set <code class="docutils literal notranslate"><span class="pre">optimization_level</span></code> to <code class="docutils literal notranslate"><span class="pre">O0</span></code>.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">nf</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">NeuralModuleFactory</span><span class="p">(</span><span class="n">local_rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                   <span class="n">optimization_level</span><span class="o">=</span><span class="s2">&quot;O1&quot;</span><span class="p">,</span>
                                   <span class="n">log_dir</span><span class="o">=</span><span class="n">WORK_DIR</span><span class="p">,</span>
                                   <span class="n">placement</span><span class="o">=</span><span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">GPU</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Next, we’ll need to define our tokenizer and our BERT model. Currently, there are 3 pretrained back-bone models supported:
BERT, ALBERT and RoBERTa. These are pretrained model checkpoints from <a class="reference external" href="https://huggingface.co/transformers">transformers</a> . Apart from these, the user can also do fine-tuning
on a custom BERT checkpoint, specified by the <cite>–bert_checkpoint</cite> argument in the training script.
The pretrained back-bone models can be specified <cite>–pretrained_model_name</cite>.
See the list of available pre-trained models by calling <cite>nemo_nlp.nm.trainables.get_pretrained_lm_models_list()</cite>. </p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">bert_model</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">get_pretrained_lm_model</span><span class="p">(</span>
    <span class="n">pretrained_model_name</span><span class="o">=</span><span class="n">PRETRAINED_BERT_MODEL</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">collections</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">tokenizers</span><span class="o">.</span><span class="n">get_tokenizer</span><span class="p">(</span>
    <span class="n">tokenizer_name</span><span class="o">=</span><span class="s2">&quot;nemobert&quot;</span><span class="p">,</span>
    <span class="n">pretrained_model_name</span><span class="o">=</span><span class="n">PRETRAINED_BERT_MODEL</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, create the train and evaluation data layers:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">data_layers</span><span class="o">.</span><span class="n">PunctuationCapitalizationDataLayer</span><span class="p">(</span>
                                    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                                    <span class="n">text_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;text_train.txt&#39;</span><span class="p">),</span>
                                    <span class="n">label_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;labels_train.txt&#39;</span><span class="p">),</span>
                                    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span>
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">)</span>

<span class="n">punct_label_ids</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">punct_label_ids</span>
<span class="n">capit_label_ids</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">capit_label_ids</span>

<span class="n">hidden_size</span> <span class="o">=</span> <span class="n">bert_model</span><span class="o">.</span><span class="n">hidden_size</span>

<span class="c1"># Note that you need to specify punct_label_ids and capit_label_ids  - mapping form labels</span>
<span class="c1"># to label_ids generated during creation of the train_data_layer to make sure that</span>
<span class="c1"># the mapping is correct in case some of the labels from</span>
<span class="c1"># the train set are missing in the dev set.</span>
<span class="n">eval_data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">BertPunctuationCapitalizationDataLayer</span><span class="p">(</span>
                                    <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                                    <span class="n">text_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;text_dev.txt&#39;</span><span class="p">),</span>
                                    <span class="n">label_file</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">DATA_DIR</span><span class="p">,</span> <span class="s1">&#39;labels_dev.txt&#39;</span><span class="p">),</span>
                                    <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span>
                                    <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span>
                                    <span class="n">punct_label_ids</span><span class="o">=</span><span class="n">punct_label_ids</span><span class="p">,</span>
                                    <span class="n">capit_label_ids</span><span class="o">=</span><span class="n">capit_label_ids</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, create punctuation-capitalization classifier to sit on top of the pretrained BERT model and define the task loss function:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">trainables</span><span class="o">.</span><span class="n">PunctCapitTokenClassifier</span><span class="p">(</span>
      <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
      <span class="n">punct_num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">punct_label_ids</span><span class="p">),</span>
      <span class="n">capit_num_classes</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">capit_label_ids</span><span class="p">),</span>
      <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
      <span class="n">punct_num_layers</span><span class="o">=</span><span class="n">PUNCT_NUM_FC_LAYERS</span>
  <span class="p">)</span>


<span class="c1"># If you don&#39;t want to use weighted loss for Punctuation task, use class_weights=None</span>
<span class="n">punct_label_freqs</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="o">.</span><span class="n">dataset</span><span class="o">.</span><span class="n">punct_label_frequencies</span>
<span class="n">class_weights</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">collections</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">datasets</span><span class="o">.</span><span class="n">datasets_utils</span><span class="o">.</span><span class="n">calc_class_weights</span><span class="p">(</span><span class="n">punct_label_freqs</span><span class="p">)</span>

<span class="c1"># define loss</span>
<span class="n">punct_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLossNM</span><span class="p">(</span><span class="n">logits_ndim</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">class_weights</span><span class="p">)</span>
<span class="n">capit_loss</span> <span class="o">=</span> <span class="n">CrossEntropyLossNM</span><span class="p">(</span><span class="n">logits_ndim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">task_loss</span> <span class="o">=</span> <span class="n">LossAggregatorNM</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Below, we’re passing the output of the datalayers through the pretrained BERT model and to the classifiers:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_type_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">loss_mask</span><span class="p">,</span> <span class="n">subtokens_mask</span><span class="p">,</span> <span class="n">punct_labels</span><span class="p">,</span> <span class="n">capit_labels</span> <span class="o">=</span> <span class="n">train_data_layer</span><span class="p">()</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                      <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_type_ids</span><span class="p">,</span>
                      <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>

<span class="n">punct_logits</span><span class="p">,</span> <span class="n">capit_logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>

<span class="n">punct_loss</span> <span class="o">=</span> <span class="n">punct_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">punct_logits</span><span class="p">,</span>
                        <span class="n">labels</span><span class="o">=</span><span class="n">punct_labels</span><span class="p">,</span>
                        <span class="n">loss_mask</span><span class="o">=</span><span class="n">loss_mask</span><span class="p">)</span>
<span class="n">capit_loss</span> <span class="o">=</span> <span class="n">capit_loss</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">capit_logits</span><span class="p">,</span>
                        <span class="n">labels</span><span class="o">=</span><span class="n">capit_labels</span><span class="p">,</span>
                        <span class="n">loss_mask</span><span class="o">=</span><span class="n">loss_mask</span><span class="p">)</span>
<span class="n">task_loss</span> <span class="o">=</span> <span class="n">task_loss</span><span class="p">(</span><span class="n">loss_1</span><span class="o">=</span><span class="n">punct_loss</span><span class="p">,</span>
                      <span class="n">loss_2</span><span class="o">=</span><span class="n">capit_loss</span><span class="p">)</span>

<span class="n">eval_input_ids</span><span class="p">,</span> <span class="n">eval_input_type_ids</span><span class="p">,</span> <span class="n">eval_input_mask</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">eval_subtokens_mask</span><span class="p">,</span> <span class="n">eval_punct_labels</span><span class="p">,</span> <span class="n">eval_capit_labels</span>\
    <span class="o">=</span> <span class="n">eval_data_layer</span><span class="p">()</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">eval_input_ids</span><span class="p">,</span>
                           <span class="n">token_type_ids</span><span class="o">=</span><span class="n">eval_input_type_ids</span><span class="p">,</span>
                           <span class="n">attention_mask</span><span class="o">=</span><span class="n">eval_input_mask</span><span class="p">)</span>

<span class="n">eval_punct_logits</span><span class="p">,</span> <span class="n">eval_capit_logits</span> <span class="o">=</span> <span class="n">classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>Now, we will set up our callbacks. We will use 3 callbacks:</p>
<ul>
<li><p><cite>SimpleLossLoggerCallback</cite> prints loss values during training;</p></li>
<li><p><cite>EvaluatorCallback</cite> calculates the performance metrics for the dev dataset;</p></li>
<li><p><cite>CheckpointCallback</cite> is used to save and restore checkpoints.</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">callback_train</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">SimpleLossLoggerCallback</span><span class="p">(</span>
<span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">task_loss</span><span class="p">,</span> <span class="n">punct_loss</span><span class="p">,</span> <span class="n">capit_loss</span><span class="p">,</span> <span class="n">punct_logits</span><span class="p">,</span> <span class="n">capit_logits</span><span class="p">],</span>
<span class="n">print_func</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loss: </span><span class="si">{:.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">())),</span>
<span class="n">step_freq</span><span class="o">=</span><span class="n">STEP_FREQ</span><span class="p">)</span>

<span class="n">train_data_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_data_layer</span><span class="p">)</span>

<span class="c1"># If you&#39;re training on multiple GPUs, this should be</span>
<span class="c1"># train_data_size / (batch_size * batches_per_step * num_gpus)</span>
<span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">train_data_size</span> <span class="o">/</span> <span class="p">(</span><span class="n">BATCHES_PER_STEP</span> <span class="o">*</span> <span class="n">BATCH_SIZE</span><span class="p">))</span>

<span class="c1"># Callback to evaluate the model</span>
<span class="n">callback_eval</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">EvaluatorCallback</span><span class="p">(</span>
    <span class="n">eval_tensors</span><span class="o">=</span><span class="p">[</span><span class="n">eval_punct_logits</span><span class="p">,</span>
                  <span class="n">eval_capit_logits</span><span class="p">,</span>
                  <span class="n">eval_punct_labels</span><span class="p">,</span>
                  <span class="n">eval_capit_labels</span><span class="p">,</span>
                  <span class="n">eval_subtokens_mask</span><span class="p">],</span>
    <span class="n">user_iter_callback</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">eval_iter_callback</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span>
    <span class="n">user_epochs_done_callback</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">eval_epochs_done_callback</span><span class="p">(</span><span class="n">x</span><span class="p">,</span>
                                                                  <span class="n">punct_label_ids</span><span class="p">,</span>
                                                                  <span class="n">capit_label_ids</span><span class="p">),</span>
    <span class="n">eval_step</span><span class="o">=</span><span class="n">steps_per_epoch</span><span class="p">)</span>

<span class="c1"># Callback to store checkpoints</span>
<span class="n">ckpt_callback</span> <span class="o">=</span> <span class="n">nemo</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">CheckpointCallback</span><span class="p">(</span><span class="n">folder</span><span class="o">=</span><span class="n">nf</span><span class="o">.</span><span class="n">checkpoint_dir</span><span class="p">,</span>
                                             <span class="n">step_freq</span><span class="o">=</span><span class="n">STEP_FREQ</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</li>
</ul>
<p>Finally, we’ll define our learning rate policy and our optimizer, and start training:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">lr_policy</span> <span class="o">=</span> <span class="n">WarmupAnnealing</span><span class="p">(</span><span class="n">NUM_EPOCHS</span> <span class="o">*</span> <span class="n">steps_per_epoch</span><span class="p">,</span>
                    <span class="n">warmup_ratio</span><span class="o">=</span><span class="n">LR_WARMUP_PROPORTION</span><span class="p">)</span>

<span class="n">nf</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">tensors_to_optimize</span><span class="o">=</span><span class="p">[</span><span class="n">task_loss</span><span class="p">],</span>
         <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">callback_train</span><span class="p">,</span> <span class="n">callback_eval</span><span class="p">,</span> <span class="n">ckpt_callback</span><span class="p">],</span>
         <span class="n">lr_policy</span><span class="o">=</span><span class="n">lr_policy</span><span class="p">,</span>
         <span class="n">batches_per_step</span><span class="o">=</span><span class="n">BATCHES_PER_STEP</span><span class="p">,</span>
         <span class="n">optimizer</span><span class="o">=</span><span class="n">OPTIMIZER</span><span class="p">,</span>
         <span class="n">optimization_params</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;num_epochs&quot;</span><span class="p">:</span> <span class="n">NUM_EPOCHS</span><span class="p">,</span>
                              <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="n">LEARNING_RATE</span><span class="p">})</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>To see how the model performs, let’s run inference on a few samples. We need to define a data layer for inference the same way we created data layers for training and evaluation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">queries</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;can i help you&#39;</span><span class="p">,</span>
           <span class="s1">&#39;yes please&#39;</span><span class="p">,</span>
           <span class="s1">&#39;we bought four shirts from the nvidia gear store in santa clara&#39;</span><span class="p">,</span>
           <span class="s1">&#39;we bought four shirts one mug and ten thousand titan rtx graphics cards&#39;</span><span class="p">,</span>
           <span class="s1">&#39;the more you buy the more you save&#39;</span><span class="p">]</span>
<span class="n">infer_data_layer</span> <span class="o">=</span> <span class="n">nemo_nlp</span><span class="o">.</span><span class="n">nm</span><span class="o">.</span><span class="n">data_layers</span><span class="o">.</span><span class="n">BertTokenClassificationInferDataLayer</span><span class="p">(</span>
                                                        <span class="n">queries</span><span class="o">=</span><span class="n">queries</span><span class="p">,</span>
                                                        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
                                                        <span class="n">max_seq_length</span><span class="o">=</span><span class="n">MAX_SEQ_LENGTH</span><span class="p">,</span>
                                                        <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Run inference, append punctuation and capitalize words based on the generated predictions:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">input_ids</span><span class="p">,</span> <span class="n">input_type_ids</span><span class="p">,</span> <span class="n">input_mask</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">subtokens_mask</span> <span class="o">=</span> <span class="n">infer_data_layer</span><span class="p">()</span>

<span class="n">hidden_states</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">,</span>
                                      <span class="n">token_type_ids</span><span class="o">=</span><span class="n">input_type_ids</span><span class="p">,</span>
                                      <span class="n">attention_mask</span><span class="o">=</span><span class="n">input_mask</span><span class="p">)</span>
<span class="n">punct_logits</span> <span class="o">=</span> <span class="n">punct_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>
<span class="n">capit_logits</span> <span class="o">=</span> <span class="n">capit_classifier</span><span class="p">(</span><span class="n">hidden_states</span><span class="o">=</span><span class="n">hidden_states</span><span class="p">)</span>

<span class="n">evaluated_tensors</span> <span class="o">=</span> <span class="n">nf</span><span class="o">.</span><span class="n">infer</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">[</span><span class="n">punct_logits</span><span class="p">,</span> <span class="n">capit_logits</span><span class="p">,</span> <span class="n">subtokens_mask</span><span class="p">],</span>
                             <span class="n">checkpoint_dir</span><span class="o">=</span><span class="n">WORK_DIR</span> <span class="o">+</span> <span class="s1">&#39;/checkpoints&#39;</span><span class="p">)</span>



<span class="c1"># helper functions</span>
<span class="k">def</span> <span class="nf">concatenate</span><span class="p">(</span><span class="n">lists</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">t</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">lists</span><span class="p">])</span>

<span class="n">punct_ids_to_labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">punct_label_ids</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">punct_label_ids</span><span class="p">}</span>
<span class="n">capit_ids_to_labels</span> <span class="o">=</span> <span class="p">{</span><span class="n">capit_label_ids</span><span class="p">[</span><span class="n">k</span><span class="p">]:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">capit_label_ids</span><span class="p">}</span>

<span class="n">punct_logits</span><span class="p">,</span> <span class="n">capit_logits</span><span class="p">,</span> <span class="n">subtokens_mask</span> <span class="o">=</span> <span class="p">[</span><span class="n">concatenate</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span> <span class="k">for</span> <span class="n">tensors</span> <span class="ow">in</span> <span class="n">evaluated_tensors</span><span class="p">]</span>
<span class="n">punct_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">punct_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">capit_preds</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">capit_logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">query</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">queries</span><span class="p">):</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Query: </span><span class="si">{query}</span><span class="s1">&#39;</span><span class="p">)</span>

    <span class="n">punct_pred</span> <span class="o">=</span> <span class="n">punct_preds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">subtokens_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span>
    <span class="n">capit_pred</span> <span class="o">=</span> <span class="n">capit_preds</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">subtokens_mask</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">]</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">punct_pred</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">capit_pred</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Pred and words must be of the same length&#39;</span><span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
    <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">w</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">words</span><span class="p">):</span>
        <span class="n">punct_label</span> <span class="o">=</span> <span class="n">punct_ids_to_labels</span><span class="p">[</span><span class="n">punct_pred</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span>
        <span class="n">capit_label</span> <span class="o">=</span> <span class="n">capit_ids_to_labels</span><span class="p">[</span><span class="n">capit_pred</span><span class="p">[</span><span class="n">j</span><span class="p">]]</span>

        <span class="k">if</span> <span class="n">capit_label</span> <span class="o">!=</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="n">w</span>
        <span class="k">if</span> <span class="n">punct_label</span> <span class="o">!=</span> <span class="s1">&#39;O&#39;</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">+=</span> <span class="n">punct_label</span>
        <span class="n">output</span> <span class="o">+=</span> <span class="s1">&#39; &#39;</span>
    <span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Combined: {output.strip()}</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Inference results:</p>
<blockquote>
<div><div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Query: can i help you
Combined: Can I help you?

Query: yes please
Combined: Yes, please.

Query: we bought four shirts from the nvidia gear store in santa clara
Combined: We bought four shirts from the Nvidia gear store in Santa Clara.

Query: we bought four shirts one mug and ten thousand titan rtx graphics cards
Combined: We bought four shirts, one mug, and ten thousand Titan Rtx graphics cards.

Query: the more you buy the more you save
Combined: The more you buy, the more you save.
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="training-and-inference-scripts">
<span id="punct-scripts"></span><h2>Training and inference scripts<a class="headerlink" href="#training-and-inference-scripts" title="Permalink to this headline">¶</a></h2>
<p>To run the provided training script:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/nlp/token_classification/punctuation_capitalization.py --data_dir path_to_data --pretrained_model_name<span class="o">=</span>bert-base-uncased --work_dir path_to_output_dir
</pre></div>
</div>
<p>To run inference:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python examples/nlp/token_classification/punctuation_capitalization_infer.py --punct_labels_dict path_to_data/punct_label_ids.csv --capit_labels_dict path_to_data/capit_label_ids.csv --checkpoint_dir path_to_output_dir/checkpoints/
</pre></div>
</div>
<p>Note, punct_label_ids.csv and capit_label_ids.csv files will be generated during training and stored in the data_dir folder.</p>
</div>
<div class="section" id="multi-gpu-training">
<h2>Multi GPU Training<a class="headerlink" href="#multi-gpu-training" title="Permalink to this headline">¶</a></h2>
<p>To run training on multiple GPUs, run</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span> <span class="nv">NUM_GPUS</span><span class="o">=</span><span class="m">2</span>
python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="nv">$NUM_GPUS</span> examples/nlp/token_classification/punctuation_capitalization.py --num_gpus <span class="nv">$NUM_GPUS</span> --data_dir path_to_data
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="question_answering.html" class="btn btn-neutral float-right" title="Tutorial" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="ner.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2020, NVIDIA

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>