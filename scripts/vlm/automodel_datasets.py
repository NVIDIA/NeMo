import json
import random

import torch

from nemo.collections import vlm


def mk_hf_vlm_dataset_rdr(data_path, processor, mbs, gbs):
    skipped_tokens = vlm.HFAutoModelForImageTextToText.extract_skipped_token_ids(processor)
    def collate_fn(examples, processor):
        def fmt(sample):
            instruction = "Describe accurately the given image."
            conversation = [
                {
                    "role": "user",
                    "content": [{"type": "text", "text": instruction}, {"type": "image", "image": sample["image"]}],
                },
                {"role": "assistant", "content": [{"type": "text", "text": sample["text"]}]},
            ]
            return {"conversation": conversation, "images": [sample['image'].convert("RGB")]}

        text = []
        images = []
        for example in map(fmt, examples):

            text.append(
                processor.apply_chat_template(example["conversation"],tokenize=False,add_generation_prompt=False,)
            )
            images += example['images']

        # Tokenize the text and process the images
        batch = processor(text=text,images=images,padding=True,return_tensors="pt",)

        batch["pixel_values"] = batch["pixel_values"].to(torch.bfloat16)
        labels = batch["input_ids"].clone()
        labels[torch.isin(labels, skipped_tokens)] = -100
        batch["labels"] = labels
        return batch

    return vlm.HFDatasetDataModule(
        data_path,
        split="train",
        micro_batch_size=mbs,
        global_batch_size=gbs,
        collate_fn=lambda x: collate_fn(x, processor=processor),
        num_workers=16,
        persistent_workers=True,
    )


def json2token(obj, sort_json_key: bool = True):
    """
    Convert an ordered JSON object into a token sequence
    """
    if type(obj) == dict:
        if len(obj) == 1 and "text_sequence" in obj:
            return obj["text_sequence"]
        else:
            output = ""
            if sort_json_key:
                keys = sorted(obj.keys(), reverse=True)
            else:
                keys = obj.keys()
            for k in keys:
                output += (
                        fr"<s_{k}>"
                        + json2token(obj[k], sort_json_key)
                        + fr"</s_{k}>"
                )
            return output
    elif type(obj) == list:
        return r"<sep/>".join(
            [json2token(item, sort_json_key) for item in obj]
        )
    else:
        obj = str(obj)
        return obj

def mk_hf_vlm_dataset_cord_v2(data_path, processor, mbs, gbs):
    skipped_tokens = vlm.HFAutoModelForImageTextToText.extract_skipped_token_ids(processor)
    def train_collate_fn(examples, processor):
        processed_examples = []
        for example in examples:
            ground_truth = json.loads(example["ground_truth"])
            if "gt_parses" in ground_truth:  # when multiple ground truths are available, e.g., docvqa
                assert isinstance(ground_truth["gt_parses"], list)
                gt_jsons = ground_truth["gt_parses"]
            else:
                assert "gt_parse" in ground_truth and isinstance(ground_truth["gt_parse"], dict)
                gt_jsons = [ground_truth["gt_parse"]]


            text = random.choice([json2token(gt_json,sort_json_key=True) for gt_json in gt_jsons])
            processed_examples.append((example["image"], text))

        examples = processed_examples
        images = []
        texts = []

        for example in examples:
            image, ground_truth = example
            images.append(image)

            conversation = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image"},
                        {"type": "text", "text": "Extract JSON"},
                    ],
                },
                {
                    "role": "assistant",
                    "content": [
                        {"type": "text", "text": ground_truth},
                    ],
                }
            ]
            text_prompt = processor.apply_chat_template(conversation)
            texts.append(text_prompt)

        batch = processor(text=texts, images=images, padding=True, truncation=True,
                          return_tensors="pt")

        labels = batch["input_ids"].clone()
        labels[torch.isin(labels, skipped_tokens)] = -100
        batch["labels"] = labels
        return batch

    return vlm.HFDatasetDataModule(
        data_path,
        split="train",
        micro_batch_size=mbs,
        global_batch_size=gbs,
        num_workers=16,
        persistent_workers=True,
        collate_fn=lambda x: train_collate_fn(x, processor=processor),
    )
