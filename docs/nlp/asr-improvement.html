

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; nemo 0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../_static/jquery.js"></script>
        <script type="text/javascript" src="../_static/underscore.js"></script>
        <script type="text/javascript" src="../_static/doctools.js"></script>
        <script type="text/javascript" src="../_static/language_data.js"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="NeMo Collections API" href="../collections/modules.html" />
    <link rel="prev" title="Tutorial" href="joint_intent_slot_filling.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> nemo
          

          
          </a>

          
            
            
              <div class="version">
                0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/intro.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../training.html">Fast Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../asr/intro.html">Speech Recognition</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="intro.html">Natural Language Processing</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="intro.html#neural-machine-translation-nmt">Neural Machine Translation (NMT)</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#language-modeling-lm">Language Modeling (LM)</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#bert">BERT</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#named-entity-recognition">Named Entity Recognition</a></li>
<li class="toctree-l2"><a class="reference internal" href="intro.html#intent-and-slot-filling">Intent and Slot filling</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="intro.html#improving-speech-recognition-with-bertx2-post-processing-model">Improving speech recognition with BERTx2 post-processing model</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#data">Data</a></li>
<li class="toctree-l4"><a class="reference internal" href="#importing-parameters-from-pretrained-bert">Importing parameters from pretrained BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#neural-modules-overview">Neural modules overview</a></li>
<li class="toctree-l4"><a class="reference internal" href="#model-training">Model training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../collections/modules.html">NeMo Collections API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api-docs/modules.html">NeMo API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions (FAQ)</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">nemo</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="intro.html">Natural Language Processing</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/nlp/asr-improvement.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>In this tutorial we will train an ASR postprocessing model to correct mistakes in
output of end-to-end language model. This model method works similar to translation model
in contrast to traditional ASR language model rescoring. The model architecture is
attention based encoder-decoder where both encoder and decoder are initialized with
pretrained BERT language model. To train this model we collected dataset with typical
ASR errors by using pretrained Jasper ASR model <a class="reference internal" href="#li2019jasper" id="id1">[1]</a>.</p>
<div class="section" id="data">
<h2>Data<a class="headerlink" href="#data" title="Permalink to this headline">¶</a></h2>
<p><strong>Data collection.</strong> We collected dataset for this tutorial with Jasper ASR model
<a class="reference internal" href="#li2019jasper" id="id2">[1]</a> trained on Librispeech dataset <a class="reference internal" href="#panayotov2015librispeech" id="id3">[3]</a>.
Librispeech training dataset consists of three parts – train-clean-100, train-clean-360 and
train-clean-500 which give 281k training examples in total.
To augment this data we used two techniques:</p>
<ul class="simple">
<li><p>We split all training data into 10 folds and trained 10 Jasper models in cross-validation manner: a model was trained on 9 folds and used to make ASR predictions for the remaining fold.</p></li>
<li><p>We took pretrained Jasper model and enabled dropout during inference on training data. This procedure was repeated multiple times with different random seeds.</p></li>
</ul>
<p><strong>Data postprocessing.</strong> The collecred dataset was postprocessed by removing duplicates
and examples with word error rate higher than 0.5.
The resulting training dataset consists of 1.7M pairs of “bad” English-“good” English examples.</p>
<p><strong>Dev and test datasets preparation</strong>. Librispeech contains 2 dev datasets
(dev-clean and dev-other) and 2 test datasets (test-clean and test-other).
For our task we kept the same splits. We fed these datasets to a pretrained
Jasper model with the greedy decoding to get the ASR predictions that are used
for evaluation in our tutorial.</p>
</div>
<div class="section" id="importing-parameters-from-pretrained-bert">
<h2>Importing parameters from pretrained BERT<a class="headerlink" href="#importing-parameters-from-pretrained-bert" title="Permalink to this headline">¶</a></h2>
<p>Both encoder and decoder are initialized with pretrained BERT parameters. Since BERT language
model has the same architecture as transformer encoder, there is no need to do anything
additional. To prepare decoder parameters from pretrained BERT we wrote a script
<code class="docutils literal notranslate"><span class="pre">get_decoder_params_from_bert.py</span></code> that downloads BERT parameters from
pytorch-transformers repository <a class="reference internal" href="#huggingface2019transformers" id="id4">[1]</a> and maps them into a transformer decoder.
Encoder-decoder attention is initialized with self-attention parameters.
The script is located under <code class="docutils literal notranslate"><span class="pre">nemo/scripts</span></code> directory and accepts 2 arguments:
<code class="docutils literal notranslate"><span class="pre">--model_name</span></code> (ex. <code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code>, <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>, etc) and <code class="docutils literal notranslate"><span class="pre">--save_to</span></code>
(a directory where the parameters will be saved):</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python get_decoder_params_from_bert.py --model_name bert-base-uncased
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="neural-modules-overview">
<h2>Neural modules overview<a class="headerlink" href="#neural-modules-overview" title="Permalink to this headline">¶</a></h2>
<p>First we define tokenizer to convert tokens into indices. We will use <code class="docutils literal notranslate"><span class="pre">bert-base-uncased</span></code>
vocabukary, since our dataset only contains lower-case text:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">NemoBertTokenizer</span><span class="p">(</span><span class="n">pretrained_model</span><span class="o">=</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>The encoder block is a neural module corresponding to BERT language model from
<code class="docutils literal notranslate"><span class="pre">nemo.nemo_nlp.huggingface</span></code> collection:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">zeros_transform</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ZerosLikeNM&quot;</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{},</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;nemo_nlp&quot;</span>
<span class="p">)</span>
<span class="n">encoder</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;higgingface.BERT&quot;</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;pretrained_model_name&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">pretrained_model_name</span><span class="p">,</span>
        <span class="s2">&quot;local_rank&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">local_rank</span>
    <span class="p">},</span>
    <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;nemo_nlp&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Making embedding size (as well as all other tensor dimensions) divisible
by 8 will help to get the best GPU utilization and speed-up with mixed precision
training.</p>
</div>
</div></blockquote>
<p>We also pad the matrix of embedding parameters with zeros to have all the dimensions sizes
divisible by 8, which will speed up the computations on GPU with AMP:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">8</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">/</span> <span class="mi">8</span><span class="p">)</span>
<span class="n">tokens_to_add</span> <span class="o">=</span> <span class="n">vocab_size</span> <span class="o">-</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">encoder</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
<span class="n">zeros</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">tokens_to_add</span><span class="p">,</span> <span class="n">args</span><span class="o">.</span><span class="n">d_model</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="n">encoder</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span>
    <span class="p">(</span><span class="n">encoder</span><span class="o">.</span><span class="n">bert</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">word_embeddings</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">zeros</span><span class="p">))</span>
</pre></div>
</div>
</div></blockquote>
<p>Next we construct transformer decoder neural module. Since we will be initializing decoder
with pretrained BERT parameters, we set hidden activation to <code class="docutils literal notranslate"><span class="pre">&quot;hidden_act&quot;:</span> <span class="pre">&quot;gelu&quot;</span></code> and learn
positional encodings <code class="docutils literal notranslate"><span class="pre">&quot;learn_positional_encodings&quot;:</span> <span class="pre">True</span></code>:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">decoder</span> <span class="o">=</span> <span class="n">neural_factory</span><span class="o">.</span><span class="n">get_module</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;TransformerDecoderNM&quot;</span><span class="p">,</span>
    <span class="n">params</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;d_model&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">d_model</span><span class="p">,</span>
        <span class="s2">&quot;d_inner&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">d_inner</span><span class="p">,</span>
        <span class="s2">&quot;num_layers&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span>
        <span class="s2">&quot;num_attn_heads&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="s2">&quot;fully_connected_dropout&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">fully_connected_dropout</span><span class="p">,</span>
        <span class="s2">&quot;vocab_size&quot;</span><span class="p">:</span> <span class="n">vocab_size</span><span class="p">,</span>
        <span class="s2">&quot;max_seq_length&quot;</span><span class="p">:</span> <span class="n">max_sequence_length</span><span class="p">,</span>
        <span class="s2">&quot;embedding_dropout&quot;</span><span class="p">:</span> <span class="n">args</span><span class="o">.</span><span class="n">embedding_dropout</span><span class="p">,</span>
        <span class="s2">&quot;learn_positional_encodings&quot;</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>
        <span class="s2">&quot;hidden_act&quot;</span><span class="p">:</span> <span class="s2">&quot;gelu&quot;</span><span class="p">,</span>
        <span class="o">**</span><span class="n">dec_first_sublayer_params</span>
    <span class="p">},</span>
  <span class="n">collection</span><span class="o">=</span><span class="s2">&quot;nemo_nlp&quot;</span>
  <span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
<p>To load the pretrained parameters into decoder, we use <code class="docutils literal notranslate"><span class="pre">restore_from</span></code> attribute function
of the decoder neural module:</p>
<blockquote>
<div><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">decoder</span><span class="o">.</span><span class="n">restore_from</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">restore_from</span><span class="p">,</span> <span class="n">local_rank</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">local_rank</span><span class="p">)</span>
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="model-training">
<h2>Model training<a class="headerlink" href="#model-training" title="Permalink to this headline">¶</a></h2>
<p>To train the model run <code class="docutils literal notranslate"><span class="pre">bert_asr_improvement.py</span></code> located in <code class="docutils literal notranslate"><span class="pre">nemo\examples\nlp</span></code> directory.
We train with novograd optimizer <a class="reference internal" href="#ginsburg2019stochastic" id="id5">[2]</a>, learning rate <code class="docutils literal notranslate"><span class="pre">lr=0.001</span></code>,
polynomial learning rate decay policy, <code class="docutils literal notranslate"><span class="pre">1000</span></code> warmup steps, per-gpu batch size of <code class="docutils literal notranslate"><span class="pre">4096*8</span></code> tokens,
and <code class="docutils literal notranslate"><span class="pre">0.25</span></code> dropout probability. We trained on 8 GPUS. To launch the training in
multi-gpu mode run the following command:</p>
<blockquote>
<div><div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ python -m torch.distributed.launch --nproc_per_node<span class="o">=</span><span class="m">8</span>  bert_asr_improvement.py --dataset_dir ../../tests/data/pred_real/ --restore_from ../../scripts/bert-base-uncased_decoder.pt
</pre></div>
</div>
</div></blockquote>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p id="bibtex-bibliography-nlp/asr-improvement-0"><dl class="citation">
<dt class="label" id="huggingface2019transformers"><span class="brackets"><a class="fn-backref" href="#id4">1</a></span></dt>
<dd><p>A library of state-of-the-art pretrained models for natural language processing (nlp). <span><a class="reference external" href="#"></a></span>https://github.com/huggingface/pytorch-transformers, Accessed August 23, 2019.</p>
</dd>
<dt class="label" id="ginsburg2019stochastic"><span class="brackets"><a class="fn-backref" href="#id5">2</a></span></dt>
<dd><p>Boris Ginsburg, Patrice Castonguay, Oleksii Hrinchuk, Oleksii Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, Huyen Nguyen, and Jonathan M Cohen. Stochastic gradient methods with layer-wise adaptive moments for training of deep networks. <em>arXiv preprint arXiv:1905.11286</em>, 2019.</p>
</dd>
<dt class="label" id="li2019jasper"><span class="brackets">1</span><span class="fn-backref">(<a href="#id1">1</a>,<a href="#id2">2</a>)</span></dt>
<dd><p>Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii Kuchaiev, Jonathan M Cohen, Huyen Nguyen, and Ravi Teja Gadde. Jasper: an end-to-end convolutional neural acoustic model. <em>arXiv preprint arXiv:1904.03288</em>, 2019.</p>
</dd>
<dt class="label" id="panayotov2015librispeech"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p>Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus based on public domain audio books. In <em>2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 5206–5210. IEEE, 2015.</p>
</dd>
</dl>
</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../collections/modules.html" class="btn btn-neutral float-right" title="NeMo Collections API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="joint_intent_slot_filling.html" class="btn btn-neutral float-left" title="Tutorial" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018-2019, AI Applications team

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>