model:
  scale_factor: 0.13025
  disable_first_stage_autocast: True
  is_legacy: False

  denoiser_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.denoiser.DiscreteDenoiser
    num_idx: 1000

    weighting_config:
      _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.denoiser_weighting.EpsWeighting
    scaling_config:
      _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.denoiser_scaling.EpsScaling
    discretization_config:
      _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.discretizer.LegacyDDPMDiscretization


  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    from_pretrained: /sdxl_ckpts/stable-diffusion-xl-refiner-1.0/unet/diffusion_pytorch_model.safetensors
    from_NeMo: False
    adm_in_channels: 2560
    num_classes: sequential
    use_checkpoint: False
    in_channels: 4
    out_channels: 4
    model_channels: 384
    attention_resolutions: [4, 2]
    num_res_blocks: 2
    channel_mult: [1, 2, 4, 4]
    num_head_channels: 64
    use_spatial_transformer: True
    use_linear_in_transformer: True
    transformer_depth: 4
    context_dim: [1280, 1280, 1280, 1280]  # 1280
    image_size: 64 # unused
    #spatial_transformer_attn_type: softmax-xformers
    legacy: False

  conditioner_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.GeneralConditioner
    emb_models:
      # crossattn and vector cond
      - is_trainable: False
        input_key: txt
        emb_model:
          _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenOpenCLIPEmbedder2
          arch: ViT-bigG-14
          version: laion2b_s39b_b160k
          freeze: True
          layer: penultimate
          always_return_pooled: True
          legacy: False
          # vector cond
      - is_trainable: False
        input_key: original_size_as_tuple
        emb_model:
          _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.ConcatTimestepEmbedderND
          outdim: 256  # multiplied by two
      # vector cond
      - is_trainable: False
        input_key: crop_coords_top_left
        emb_model:
          _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.ConcatTimestepEmbedderND
          outdim: 256  # multiplied by two
      # vector cond
      - is_trainable: False
        input_key: aesthetic_score
        emb_model:
          _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.ConcatTimestepEmbedderND
          outdim: 256  # multiplied by one

  first_stage_config:
    _target_: nemo.collections.multimodal.models.stable_diffusion.ldm.autoencoder.AutoencoderKLInferenceWrapper
    from_pretrained: /sdxl_ckpts/stable-diffusion-xl-refiner-1.0/vae/diffusion_pytorch_model.safetensors
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      attn_type: vanilla
      double_z: true
      z_channels: 4
      resolution: 256
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult: [1, 2, 4, 4]
      num_res_blocks: 2
      attn_resolutions: []
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity
