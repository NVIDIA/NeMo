name: imagen-nemo # The name of your model
allow_tf32: True

trainer:
  devices: 1 # number of GPUs (0 for CPU), or list of the GPUs to use e.g. [0, 1]
  num_nodes: 1
  max_epochs: -1
  max_steps: 2500000 # precedence over max_epochs
  logger: False  # Provided by exp_manager 
  precision: bf16 # Should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  log_every_n_steps: 5  # Interval of logging.
  resume_from_checkpoint: null # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.
  num_sanity_val_steps: 10 # number of steps to perform validation steps for sanity check the validation process before starting the training, setting to 0 disables it
  enable_checkpointing: False # Provided by exp_manager
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True


exp_manager:
  exp_dir: /train/imagen-base64  # exp_dir for your experiment, if None, defaults to "./nemo_experiments"
  name: ${name}
  create_wandb_logger: False
  wandb_logger_kwargs: # Whether you want exp_manger to create a Wandb logger
    name: imagen-base64-nf512
    project: imagen
    group: nemo-imagen
    resume: True
  create_tensorboard_logger: True  # Whether you want exp_manger to create a tb logger
  create_checkpoint_callback: True  # Whether you want exp_manager to create a modelcheckpoint callback
  checkpoint_callback_params:
    monitor: reduced_train_loss
    save_top_k: 5
    every_n_epochs: 0 # Save checkpoint frequency.
    every_n_train_steps: 1000 # Mutually exclusive with every_n_epochs. It is recommended to set this if training on large-scale dataset.
    filename: '${name}--{reduced_train_loss:.2f}-{step}-{consumed_samples}'
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  resume_from_checkpoint: ${model.resume_from_checkpoint}
  ema:
    enable: True
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False


model:
  precision: ${trainer.precision}
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 32 # limited by GPU memory
  global_batch_size: 32 # will use more micro batches to reach global batch size
  inductor: True
  inductor_cudagraphs: False
  unet_type: base
  channels_last: True

  unet:
    embed_dim: 512
    image_size: 64
    channels: 3
    num_res_blocks: 3
    channel_mult: [ 1, 2, 3, 4 ]
    num_attn_heads: 4
    per_head_channels: 64
    cond_dim: 2048
    attention_type: fused
    feature_pooling_type: attention
    learned_sinu_pos_emb_dim: 0
    attention_resolutions: [ 8, 16, 32 ]
    dropout: False
    use_null_token: False
    init_conv_kernel_size: 3
    gradient_checkpointing: False
    scale_shift_norm: True
    stable_attention: False
    flash_attention: True
    resblock_updown: False
    resample_with_conv: True

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)
  ddp_overlap: True # True for using PyTorch default DDP overlap. False for using Megatron's default configuration for async grad allreduce

  preconditioning_type: EDM
  preconditioning:
    loss_type: l2
    sigma_data: 0.5
    p_mean: -1.2
    p_std: 1.2
  # If want to switch to continuous DDPM training,
  # use the following config:
  # preconditioning_type: DDPM
  # preconditioning:
  #   loss_type: l2
  #   pred_objective: noise
  #   noise_schedule: cosine
  #   timesteps: 1000

  conditioning:
    embed_dim: 1024
    token_length: 128
    drop_rate: 0.1
    precached_key: embeddings_t5_xxl
    out_key: t5_text

  data:
    num_workers: 16
    train:
      dataset_path:
        - datasets/laion_aesthetic/wdinfo-selene.pkl #   48,874,000
        - datasets/coyo-700m/wdinfo-selene.pkl       #  627,172,000
      augmentations:
        resize_smallest_side: 64
        center_crop_h_w: 64, 64
        horizontal_flip: False
      filterings: null

    webdataset:
      use_webdataset: True
      object_store: False
      infinite_sampler: False
      local_root_path: /datasets
      verbose: False

  optim:
    # We need weight decay for large-scale odel
    name: fused_adam
    lr: 0.0001
    eps: 1e-8
    betas: [ 0.9, 0.999 ]
    weight_decay: 0.01
    sched:
      name: WarmupPolicy
      warmup_steps: 10000
      warmup_ratio: null