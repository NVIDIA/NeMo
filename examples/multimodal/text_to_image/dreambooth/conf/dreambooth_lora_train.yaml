name: Dreambooth-lora

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  precision: bf16-mixed
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_epochs: -1 # PTL default. In practice, max_steps will be reached first.
  max_steps: 500 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 10
  accumulate_grad_batches: 1 # do not modify, grad acc is automatic for training megatron models
  gradient_clip_val: 1.0
  benchmark: False
  enable_model_summary: True
  limit_val_batches: 0

exp_manager:
  exp_dir: null
  name: ${name}
  create_checkpoint_callback: True
  create_tensorboard_logger: True
  checkpoint_callback_params:
    every_n_train_steps: 200
    every_n_epochs: 0
    monitor: reduced_train_loss
    save_on_train_epoch_end: False
    filename: '${name}-{step}'
    save_top_k: -1
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  resume_from_checkpoint: ${model.resume_from_checkpoint}
  ema:
    enable: False
    decay: 0.9999
    validate_original_weights: False
    every_n_steps: 1
    cpu_offload: False



model:
  precision: ${trainer.precision}
  # specify micro_batch_size, global_batch_size, and model parallelism
  # gradient accumulation will be done automatically based on data_parallel_size
  micro_batch_size: 1 # limited by GPU memory
  global_batch_size: 1 # will use more micro batches to reach global batch size

  with_prior_preservation: False
  use_cached_latents: False
  prior_loss_weight: 0.5
  train_text_encoder: False
  restore_from_path: /ckpts/nemo-v1-5-188000-ema.nemo #This ckpt is only used to generate regularization images, thus .nemo ckpt is needed




  linear_start: 0.00085
  linear_end: 0.012
  num_timesteps_cond: 1
  log_every_t: 200
  timesteps: 1000
  first_stage_key: images
  cond_stage_key: captions
  image_size: 64
  channels: 4
  cond_stage_trainable: false
  conditioning_key: crossattn # check
  monitor: val/loss_simple_ema
  scale_factor: 0.18215
  use_ema: False
  scale_by_std: False
  ckpt_path:
  ignore_keys: [ ]
  parameterization: eps
  clip_denoised: True
  load_only_unet: False
  cosine_s: 8e-3
  given_betas:
  original_elbo_weight: 0
  v_posterior: 0
  l_simple_weight: 1
  use_positional_encodings: False
  learn_logvar: False
  logvar_init: 0
  beta_schedule: linear
  loss_type: l2

  concat_mode: True
  cond_stage_forward:
  text_embedding_dropout_rate: 0.1
  fused_opt: True
  inductor: False
  inductor_cudagraphs: False
  channels_last: False

  unet_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.diffusionmodules.openaimodel.UNetModel
    from_pretrained: /ckpts/unet.bin #load unet weights for finetuning, can use .ckpt ckpts from various sources
    from_NeMo: False #Must be specified when from pretrained is not None, False means loading unet from HF ckpt
    image_size: 32 # unused
    in_channels: 4
    out_channels: 4
    model_channels: 320
    attention_resolutions:
      - 4
      - 2
      - 1
    num_res_blocks: 2
    channel_mult:
      - 1
      - 2
      - 4
      - 4
    num_heads: 8
    use_spatial_transformer: true
    transformer_depth: 1
    context_dim: 768
    use_checkpoint: False
    legacy: False
    use_flash_attention: False
    lora_network_alpha: null

  first_stage_config:
    _target_: nemo.collections.multimodal.models.text_to_image.stable_diffusion.ldm.autoencoder.AutoencoderKL
    from_pretrained: /ckpts/vae.bin
    #ckpt_path: /ckpts/vae.ckpt #to support original opensource weights files, please use ckpt_path to load it.
    embed_dim: 4
    monitor: val/rec_loss
    ddconfig:
      double_z: true
      z_channels: 4
      resolution: 256  #Never used
      in_channels: 3
      out_ch: 3
      ch: 128
      ch_mult:
        - 1
        - 2
        - 4
        - 4
      num_res_blocks: 2
      attn_resolutions: [ ]
      dropout: 0.0
    lossconfig:
      target: torch.nn.Identity

  cond_stage_config:
    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenMegatronCLIPEmbedder
    restore_from_path: /ckpts/openai.nemo
    device: cuda
    freeze: True
    layer: "last"
    enable_lora_finetune: False  #to enable text encoder lora finetune, please enable both this one and "train_text_encoder"
    #    For compatibility of history version that uses HF clip model
    #    _target_: nemo.collections.multimodal.modules.stable_diffusion.encoders.modules.FrozenCLIPEmbedder
    #    version: openai/clip-vit-large-patch14
    #    device: cuda
    #    max_length: 77
    #    enable_lora_finetune: False  #to enable text encoder lora finetune, please enable both this one and "train_text_encoder"

  noise_scheduler:
    _target_: nemo.collections.multimodal.models.text_to_image.dreambooth.util.sd_noise_scheduler
    parameterization: eps
    v_posterior: 0
    given_betas:
    beta_schedule: linear
    timesteps: 1000
    linear_start: 0.00085
    linear_end: 0.012
    cosine_s: 8e-3

  # miscellaneous
  seed: 1234
  resume_from_checkpoint: null # manually set the checkpoint file to load from
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this
  gradient_as_bucket_view: True # PyTorch DDP argument. Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)

  optim:
    name: fused_adam
    lr: 1e-4
    weight_decay: 0.
    betas:
      - 0.9
      - 0.999
    sched:
      name: WarmupHoldPolicy
      warmup_steps: 1
      hold_steps: 10000000000000 # Incredibly large value to hold the lr as constant

  # Nsys profiling options
  nsys_profile:
    enabled: False
    start_step: 10  # Global batch to start profiling
    end_step: 10 # Global batch to end profiling
    ranks: [ 0 ] # Global rank IDs to profile
    gen_shape: False # Generate model and kernel details including input shapes

  data:
    name: pbss
    num_workers: 4
    instance_dir: /home/scratch.zhuoyaow_gpu/workspace/SD_NeMo_EA/launcher_scripts/data/inst_dir
    instance_prompt: a photo of a sks dog
    regularization_dir: /home/scratch.zhuoyaow_gpu/workspace/SD_NeMo_EA/launcher_scripts/data/nemo_dogs
    regularization_prompt: a photo of a dog
    num_reg_images: 10
    num_images_per_prompt: 4
    resolution: 512
    center_crop: True
    cached_instance_dir: #/datasets/instance_dir_cached
    cached_reg_dir: #/datasets/nemo_dogs_cached

  peft:
    peft_scheme: "sdlora"
    restore_from_path: null
    lora_tuning:
      adapter_dim: 32
      network_alpha: 16
      adapter_dropout: 0.0
      column_init_method: 'xavier' # IGNORED if linear_adapter is used, options: xavier, zero or normal
      row_init_method: 'zero' # IGNORED if linear_adapter is used, options: xavier, zero or normal
      layer_selection: null  # selects in which layers to add lora adapters. e.g. [1,12] will add lora to layer 1 (lowest) and 12. null will apply adapters to all layers
      weight_tying: False
      position_embedding_strategy: null # used only when weight_tying is True

##The below infer config is to use inference script generating regularization images
infer:
  unconditional_guidance_scale: 7.5
  num_images_per_prompt: ${model.data.num_images_per_prompt}
  height: 512
  width: 512
  down_factor: 8
  inference_steps: 50
  sampler_type: 'PLMS'
  eta: 0
  output_type: 'pil'
  save_to_file: False
  out_path: ${model.data.regularization_dir}
  prompts: ${model.data.regularization_prompt}