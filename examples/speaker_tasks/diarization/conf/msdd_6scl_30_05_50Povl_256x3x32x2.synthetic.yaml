name: "MultiscaleDiarDecoder" 
sample_rate: 16000
num_workers: 20
batch_size: 7

model:
  diarizer:
    out_dir: null
    oracle_vad: True # If True, uses RTTM files provided in manifest file to get speech activity (VAD) timestamps
    speaker_embeddings:
      model_path: ??? # .nemo local model path or pretrained model name (titanet_large is recommended)
      parameters:
        window_length_in_sec: [3.0,2.5,2.0,1.5,1.0,0.5] # Window length(s) in sec (floating-point number). either a number or a list. ex) 1.5 or [1.5,1.0,0.5]
        shift_length_in_sec: [1.5,1.25,1.0,0.75,0.5,0.25] # Shift length(s) in sec (floating-point number). either a number or a list. ex) 0.75 or [0.75,0.5,0.25]
        multiscale_weights: [1,1,1,1,1,1] # Weight for each scale. should be null (for single scale) or a list matched with window/shift scale count. ex) [0.33,0.33,0.33]
        save_embeddings: True # Save embeddings as pickle file for each audio input.

  num_workers: ${num_workers}
  max_num_of_spks: 2 # Number of speakers per model. This is currently fixed at 2.
  scale_n: 6 # Number of scales for MSDD model and initializing clustering.
  soft_label_thres: 0.5 # Threshold for creating discretized speaker label from continuous speaker label in RTTM files.
  emb_batch_size: 0 # If this value is bigger than 0, corresponding number of embedding vectors are attached to torch graph and trained.

  train_ds:
    manifest_filepath: ???
    emb_dir: ???
    sample_rate: ${sample_rate}
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    labels: null
    batch_size: ${batch_size}
    emb_batch_size: ${model.emb_batch_size}
    shuffle: True
    synthetic: True # if true, simulate data online using data_simulator parameters (refreshed every N epochs, where N is the value of trainer.reload_dataloaders_every_n_epoch)
    include_base_ds: False # if true, use both real data and simulated data generated online

  # Parameters for online data simulation (only performed if train_ds.synthetic is True)
  # The data simulator uses single-speaker utterances and corresponding word alignments to construct 
  # simulated multispeaker audio sessions online. The online data can be refreshed every N epochs.
  data_simulator:
    manifest_filepath: null # Manifest file with paths to librispeech audio files
    sr: 16000 # sampling rate of the audio files
    random_seed: 42

    session_config:
      num_speakers: 4 # number of unique speakers per diarization session
      num_sessions: 120 # number of sessions
      session_length: 2400 # length of each diarization session (seconds)

    session_params:
      sentence_length_params: # k,p values for negative_binomial distribution
      - 0.4
      - 0.05
      dominance_var: 0.11 # variance in speaker dominance
      min_dominance: 0.05 # minimum percentage of speaking time per speaker
      turn_prob: 0.875 # probability of switching speakers
      mean_overlap: 0.19 # mean proportion of overlap to speaking time
      mean_silence: 0.15 # mean proportion of silence to speaking time
      overlap_prob: 0.5 # proportion of overlap occurences versus silence between utterances
      start_window: true #window the start of sentences
      window_type: hamming # type of windowing used when segmenting utterances (hamming, hann, cosine)
      window_size: 0.05 # length of window at end of segmented utterance (seconds)
      start_buffer: 0.1 # buffer of silence before the start of the sentence
      split_buffer: 0.1 # split RTTM if greater than twice this amount of gap
      release_buffer: 0.1 # buffer before window at end of sentence
      normalize: true
      normalization_type: equal # normalizing speakers (equal - same volume per speaker, var - variable volume per speaker)
      normalization_var: 0.1 # variance in speaker volume
      min_volume: 0.75
      max_volume: 1.25
      end_buffer: 0.5 # buffer at the end of the session to leave blank

    outputs:
      output_dir: null # output directory
      output_filename: multispeaker_session # output filename for the wav and rttm files
      overwrite_output: true
      output_precision: 3 # number of decimal places in output files

    background_noise:
      add_bg: true
      background_manifest: null
      snr: 60

    speaker_enforcement:
      enforce_num_speakers: false # enforce that all requested speakers are present in the output wav file
      enforce_time:  # percentage of the way through the audio session that enforcement mode is triggered
      - 0.25
      - 0.75

    segment_manifest:
      window: 0.5
      shift: 0.25
      step_count: 50
      deci: 3

    rir_generation: #whether to generate synthetic RIR
      use_rir: false

  validation_ds:
    manifest_filepath: ???
    emb_dir: ???
    sample_rate: ${sample_rate}
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    labels: null
    batch_size: 2
    emb_batch_size: ${model.emb_batch_size}
    shuffle: False
    synthetic: False

  test_ds:
    manifest_filepath: null
    emb_dir: null
    sample_rate: 16000
    num_spks: ${model.max_num_of_spks}
    soft_label_thres: ${model.soft_label_thres}
    labels: null
    batch_size: 2
    shuffle: False
    seq_eval_mode: False
    synthetic: False

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    normalize: "per_feature"
    window_size: 0.025
    sample_rate: ${sample_rate}
    window_stride: 0.01
    window: "hann"
    features: 80
    n_fft: 512
    frame_splicing: 1
    dither: 0.00001

  msdd_module:
    _target_: nemo.collections.asr.modules.msdd_diarizer.MSDD_module
    num_spks: ${model.max_num_of_spks} # Number of speakers per model. This is currently fixed at 2.
    hidden_size: 256 # Hidden layer size for linear layers in MSDD module
    num_lstm_layers: 3 # Number of stacked LSTM layers
    dropout_rate: 0.5 # Dropout rate
    cnn_output_ch: 32 # Number of filters in a conv-net layer.
    conv_repeat: 2 # Determins the number of conv-net layers. Should be greater or equal to 1.
    emb_dim: 192 # Dimension of the speaker embedding vectors
    scale_n: ${model.scale_n} # Number of scales for multiscale segmentation input
    weighting_scheme: 'conv_scale_weight' # Type of weighting algorithm. Options: ('conv_scale_weight', 'attn_scale_weight')
    context_vector_type: 'cos_sim' # Type of context vector: options. Options: ('cos_sim', 'elem_prod')

  loss:
    _target_: nemo.collections.asr.losses.bce_loss.BCELoss
    weight: null # Weight for binary cross-entropy loss. Either `null` or list type input. (e.g. [0.5,0.5])

  optim:
    name: adam
    lr: .001
    weight_decay: 0.001

    sched:
      name: CosineAnnealing
      min_lr: 0.00001

trainer:
  gpus: 1 # number of gpus
  max_epochs: 200
  max_steps: -1 # computed at runtime if not set
  num_nodes: 1
  strategy: ddp
  reload_dataloaders_every_n_epochs: 1
  accumulate_grad_batches: 1
  deterministic: True
  enable_checkpointing: False
  logger: False
  log_every_n_steps: 1  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  create_wandb_logger: False
  checkpoint_callback_params:
    monitor: "val_loss"
    mode: "min"
    save_top_k: 30
    every_n_epochs: 1
  wandb_logger_kwargs:
    name: null
    project: null

