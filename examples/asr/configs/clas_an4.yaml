# TODO: Update param placement so it is organized per module

model: "CLAS-AN4"

optimization:
    batch_size: 256
    optimizer: "novograd"
    smoothing_coef: 0.0
    warmup_epochs: 0
    params:
        num_epochs: 500
        lr: 0.02
        weight_decay: 0.005
        larc: false
        larc_eta: 1e-3
        luc: false
        luc_eta: 1e-3


input:
    train:
        normalize: "per_feature"
        sample_rate: 16000
        window_size: 0.02
        window_stride: 0.01
        window: "hann"
        features: 64
        n_fft: 512
        frame_splicing: 1
        dither: 0.00001
        feat_type: "logfbank"
        pad_to: 16
        max_duration: 20
    eval:
        normalize: "per_feature"
        sample_rate: 16000
        window_size: 0.02
        window_stride: 0.01
        window: "hann"
        features: 64
        n_fft: 512
        frame_splicing: 1
        dither: 0.00001
        feat_type: "logfbank"


spectr_augment:
    rect_masks: 5
    rect_time: 60
    rect_freq: 25


encoder:
    activation: "relu"
    load: false
    freeze: false

    jasper:
        - filters: 256
          repeat: 1
          kernel: [11]
          stride: [2]
          dilation: [1]
          dropout: 0.2
          residual: false

        - filters: 256
          repeat: 3
          kernel: [11]
          stride: [1]
          dilation: [1]
          dropout: 0.2
          residual: true

        - filters: 512
          repeat: 2
          kernel: [17]
          stride: [1]
          dilation: [1]
          dropout: 0.2
          residual: true

#[[encoder.jasper]]
#filters: 768
#repeat: 1
#kernel: [25]
#stride: [1]
#dilation: [1]
#dropout: 0.4
#residual: true

#[[encoder.jasper]]
#filters: 1024
#repeat: 1
#kernel: [1]
#stride: [1]
#dilation: [1]
#dropout: 0.2
#residual: false

decoder:
    hidden_size: 256
    attention_method: "general"
    attention_type: "post"
    in_dropout: 0.2
    gru_dropout: 0.2
    attn_dropout: 0.2
    teacher_forcing: 0.6
    curriculum_learning: 0.75
    rnn_type: "gru"
    n_layers: 1
    tie_emb_out_weights: true
    load: false
    freeze: false

target:
    labels: [
        " ", "'",
        "a", "b", "c", "d", "e", "f", "g", "h", "i", "j", "k", "l", "m",
        "n", "o", "p", "q", "r", "s", "t", "u", "v", "w", "x", "y", "z"
    ]
    max_len: 57

inference:
    beam_size: 8
