# This config contains the default values for training FastPitch model with aligner on LJSpeech dataset.
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: FastPitch

train_dataset: "/home/pneekhara/NeMo2022/libri_val_formatted.json"
validation_datasets: "/home/pneekhara/NeMo2022/libri_val_formatted.json"

hifi_ckpt_path: "/home/pneekhara/NeMo2022/HiFiCKPTS/hifigan_libritts/HiFiLibriEpoch334.ckpt"

# Default values from librosa.pyin
pitch_fmin: 65.40639132514966
pitch_fmax: 2093.004522404789

# LJSpeech stats (per frame), train (these values depend on pitch_fmin and pitch_fmax)
pitch_mean: 212.35873413085938
pitch_std: 68.52806091308594

# Default values for dataset with sample_rate=22050
sample_rate: 22050
n_mel_channels: 80
n_window_size: 1024
n_window_stride: 256
n_fft: 1024
lowfreq: 0
highfreq: 8000
window: hann

ssl_model_type: "conformer_multitask"
ssl_model_ckpt_path: "/home/pneekhara/NeMo2022/SSLCheckPoints/SSLConformer22050_Epoch23.ckpt"
ssl_content_emb_type: "embedding_and_probs"
n_segments: 18432 #ignored for now
speaker_stats_pitch_fp: "/home/pneekhara/NeMo2022/libri_speaker_stats.json"
pitch_normalization: speaker_wise
use_unique_tokens: false
speaker_conditioning_type: per_sample

model:
  use_encoder: false
  use_duration_predictor: ${use_unique_tokens}
  pitch_conditioning: true
  pitch_loss_scale: 1.0
  learn_alignment: true
  bin_loss_warmup_epochs: 100

  n_speakers: 1
  n_datasets: 1
  max_token_duration: 75
  symbols_embedding_dim: 384
  pitch_embedding_kernel_size: 3

  pitch_fmin: ${pitch_fmin}
  pitch_fmax: ${pitch_fmax}

  pitch_mean: ${pitch_mean}
  pitch_std: ${pitch_std}

  sample_rate: ${sample_rate}
  n_mel_channels: ${n_mel_channels}
  n_window_size: ${n_window_size}
  n_window_stride: ${n_window_stride}
  n_fft: ${n_fft}
  lowfreq: ${lowfreq}
  highfreq: ${highfreq}
  window: ${window}
  
  content_emb_indim: 174
  speaker_emb_indim: 256
  content_emb_outdim: 192
  speaker_emb_outdim: 192
  
  train_ds:
    dataset:
      _target_: nemo.collections.tts.torch.data_ssl_vocoder.SSLVocoderDataset
      manifest_filepath: ${train_dataset}
      sample_rate: ${model.sample_rate}
      ssl_model_type: ${ssl_model_type}
      ssl_model_ckpt_path: ${ssl_model_ckpt_path}
      ssl_content_emb_type: ${ssl_content_emb_type}
      n_segments: ${n_segments}
      pitch_conditioning: true
      pitch_normalization: ${pitch_normalization}
      pitch_mean: 212
      pitch_std: 70
      recache_data: false
      speaker_stats_pitch_fp: ${speaker_stats_pitch_fp}
      min_duration: 0.5
      max_duration: 16.0
      use_unique_tokens: ${use_unique_tokens}
      speaker_conditioning_type: ${speaker_conditioning_type}

    dataloader_params:
      drop_last: false
      shuffle: true
      batch_size: 2
      num_workers: 8
      pin_memory: true

  validation_ds:
    dataset:
      _target_: nemo.collections.tts.torch.data_ssl_vocoder.SSLVocoderDataset
      manifest_filepath: ${validation_datasets}
      sample_rate: ${model.sample_rate}
      ssl_model_type: ${ssl_model_type}
      ssl_model_ckpt_path: ${ssl_model_ckpt_path}
      ssl_content_emb_type: ${ssl_content_emb_type}
      n_segments: ${n_segments}
      pitch_conditioning: true
      pitch_normalization: ${pitch_normalization}
      pitch_mean: 212
      pitch_std: 70
      recache_data: false
      speaker_stats_pitch_fp: ${speaker_stats_pitch_fp}
      min_duration: 0.5
      max_duration: 16.0
      use_unique_tokens: ${use_unique_tokens}
      speaker_conditioning_type: ${speaker_conditioning_type}

    dataloader_params:
      drop_last: false
      shuffle: false
      batch_size: 2
      num_workers: 0
      pin_memory: true

  preprocessor:
    _target_: nemo.collections.asr.modules.AudioToMelSpectrogramPreprocessor
    features: ${model.n_mel_channels}
    lowfreq: ${model.lowfreq}
    highfreq: ${model.highfreq}
    n_fft: ${model.n_fft}
    n_window_size: ${model.n_window_size}
    window_size: false
    n_window_stride: ${model.n_window_stride}
    window_stride: false
    pad_to: 1
    pad_value: 0
    sample_rate: ${model.sample_rate}
    window: ${model.window}
    normalize: null
    preemph: null
    dither: 0.0
    frame_splicing: 1
    log: true
    log_zero_guard_type: add
    log_zero_guard_value: 1e-05
    mag_power: 1.0

  encoder: #n_embed and padding_idx are added by the model
    _target_: nemo.collections.tts.modules.transformer.FFTransformerDecoder
    n_layer: 6
    n_head: 1
    d_model: ${model.symbols_embedding_dim}
    d_head: 64
    d_inner: 1536
    kernel_size: 3
    dropout: 0.1
    dropatt: 0.1
    dropemb: 0.0

  output_fft:
    _target_: nemo.collections.tts.modules.transformer.FFTransformerDecoder
    n_layer: 6
    n_head: 1
    d_model: ${model.symbols_embedding_dim}
    d_head: 64
    d_inner: 1536
    kernel_size: 3
    dropout: 0.1
    dropatt: 0.1
    dropemb: 0.0

  alignment_module:
    _target_: nemo.collections.tts.modules.aligner.AlignmentEncoder
    n_text_channels: ${model.symbols_embedding_dim}

  duration_predictor:
    _target_: nemo.collections.tts.modules.fastpitch.TemporalPredictor
    input_size: ${model.symbols_embedding_dim}
    kernel_size: 3
    filter_size: 256
    dropout: 0.1
    n_layers: 2

  pitch_predictor:
    _target_: nemo.collections.tts.modules.fastpitch.TemporalPredictor
    input_size: ${model.symbols_embedding_dim}
    kernel_size: 3
    filter_size: 256
    dropout: 0.1
    n_layers: 2

  optim:
    _target_: torch.optim.AdamW
    lr: 0.0002
    betas: [0.8, 0.99]
  # optim:
  #   name: adamw
  #   lr: 0.1
  #   betas: [0.9, 0.98]
  #   weight_decay: 1e-6
  #   eps: 1e-6

  #   sched:
  #     name: NoamAnnealing
  #     warmup_steps: 1000
  #     last_epoch: -1
  #     d_model: 1  # Disable scaling based on model dim

trainer:
  num_nodes: 1
  devices: -1
  accelerator: gpu
  strategy: ddp
  precision: 16
  max_epochs: 1000
  accumulate_grad_batches: 1
  gradient_clip_val: 1000.0
  enable_checkpointing: False # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  check_val_every_n_epoch: 5
  benchmark: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  checkpoint_callback_params:
    monitor: v_loss
  resume_if_exists: false
  resume_ignore_no_checkpoint: false
