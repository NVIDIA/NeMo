# This config contains the default values for training a 22.05kHz HiFi-GAN model.
# If you want to train model on other dataset, you can change config values according to your dataset.
# Most dataset-specific arguments are in the head of the config file, see below.

name: "HifiGan"

max_epochs: ???
batch_size: 16
weighted_sampling_steps_per_epoch: null

train_ds_meta: ???
val_ds_meta: ???
log_ds_meta: ???

log_dir: ???

mel_dim: 80
lowfreq: 0
highfreq: null

# Change these values depending on your sampling rate.
sample_rate: 22050
win_length: 1024
hop_length: 256
upsample_rates: [8, 8, 2, 2]
train_n_samples: 8192
val_min_duration_seconds: 3.0
val_n_samples: 66048

model:

  max_epochs: ${max_epochs}
  steps_per_epoch: ${weighted_sampling_steps_per_epoch}
  l1_loss_factor: 60

  preprocessor:
    _target_: nemo.collections.asr.parts.preprocessing.features.FilterbankFeatures
    nfilt: ${mel_dim}
    lowfreq: ${lowfreq}
    highfreq: ${highfreq}
    n_fft: ${win_length}
    n_window_size: ${win_length}
    n_window_stride: ${hop_length}
    pad_to: 0
    pad_value: 0
    exact_pad: true
    sample_rate: ${sample_rate}
    window: hann
    normalize: null
    preemph: null
    dither: 0.0
    frame_splicing: 1
    log: true
    log_zero_guard_type: add
    log_zero_guard_value: 1.0
    mag_power: 1.0
    mel_norm: null
    use_grads: false

  generator:
    _target_: nemo.collections.tts.modules.hifigan_modules.Generator
    resblock: 1
    upsample_rates: ${upsample_rates}
    upsample_kernel_sizes: [16, 16, 4, 4]
    upsample_initial_channel: 512
    resblock_kernel_sizes: [3, 7, 11]
    resblock_dilation_sizes: [[1, 3, 5], [1, 3, 5], [1, 3, 5]]

  train_ds:
    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      weighted_sampling_steps_per_epoch: ${weighted_sampling_steps_per_epoch}
      sample_rate: ${sample_rate}
      n_samples: ${train_n_samples}
      min_duration: 0.4
      max_duration: null
      dataset_meta: ${train_ds_meta}

    dataloader_params:
      batch_size: ${batch_size}
      num_workers: 4

  validation_ds:
    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      sample_rate: ${sample_rate}
      n_samples: ${val_n_samples}
      min_duration: ${val_min_duration_seconds}
      max_duration: null
      dataset_meta: ${val_ds_meta}

    dataloader_params:
      batch_size: ${batch_size}
      num_workers: 2

  log_config:
    log_dir: ${log_dir}
    log_epochs: [10, 50]
    epoch_frequency: 100
    log_tensorboard: false
    log_wandb: false

    generators:
      - _target_: nemo.collections.tts.parts.utils.callbacks.VocoderArtifactGenerator

    dataset:
      _target_: nemo.collections.tts.data.vocoder_dataset.VocoderDataset
      sample_rate: ${sample_rate}
      n_samples: null
      min_duration: null
      max_duration: null
      trunc_duration: 15.0
      dataset_meta: ${log_ds_meta}

    dataloader_params:
      batch_size: 4
      num_workers: 2

  optim:
    _target_: torch.optim.AdamW
    lr: 2e-4
    betas: [0.8, 0.99]
    weight_decay: 1e-6
    sched:
      name: ExponentialLR
      gamma: 0.999

trainer:
  num_nodes: 1
  devices: 1
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  precision: 16
  max_epochs: ${max_epochs}
  accumulate_grad_batches: 1
  enable_checkpointing: False  # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  check_val_every_n_epoch: 10
  benchmark: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_checkpoint_callback: true
  create_wandb_logger: false
  checkpoint_callback_params:
    monitor: val_loss
  resume_if_exists: false
  resume_ignore_no_checkpoint: false
