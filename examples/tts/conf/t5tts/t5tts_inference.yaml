name: T5TTS
mode: test
init_from_ptl_ckpt: ???
max_epochs: 1
# Adjust batch size based on GPU memory
batch_size: 16
# When doing weighted sampling with multiple manifests, this defines how many training steps are in an epoch.
# If null, then weighted sampling is disabled.
weighted_sampling_steps_per_epoch: null

# Dataset metadata for each manifest
# https://github.com/NVIDIA/NeMo/blob/main/nemo/collections/tts/data/vocoder_dataset.py#L39-L41
test_ds_meta: ???

# Modify these values based on your sample rate
sample_rate: 22050

phoneme_dict_path: "scripts/tts_dataset_files/ipa_cmudict-0.7b_nv23.01.txt"
heteronyms_path: "scripts/tts_dataset_files/heteronyms-052722"
model:
  model_type: "multi_encoder_context_tts" # single_encoder_sv_tts, multi_encoder_context_tts, decoder_context_tts or decoder_pretrain_synthesizer
  use_text_conditioning_encoder: false
  transcript_decoder_layers: [3,4,5,6,7] # ONLY used in multi_encoder_context_tts, Transcript goes to these layer ids, context goes to the rest. In single_encoder_sv_tts, all layers are used for transcript.
  context_decoder_layers: [8,9] # ONLY used in multi_encoder_context_tts
  context_duration_min: 3.0
  context_duration_max: 8.0
  use_perceiver: false # For multi_encoder_context_tts, use perceiver after speaker encoder or not
  speaker_emb_dim: 192
  max_decoder_steps: 500
  num_audio_codebooks: 8
  num_audio_tokens_per_codebook: 2048 # Keep atleast 2 extra for eos/bos ids
  codec_model_downsample_factor: 1024
  load_cached_codes_if_available: true
  prior_scaling_factor: null
  prior_end_step: 0
  prior_scaledown_start_step: 0
  alignment_loss_scale: 0.0
  embedding_dim: 768
  codecmodel_path: null
  max_epochs: ${max_epochs}
  steps_per_epoch: ${weighted_sampling_steps_per_epoch}

  sample_rate: ${sample_rate}
  
  text_tokenizer:
    _target_: nemo.collections.common.tokenizers.text_to_speech.tts_tokenizers.IPATokenizer
    punct: true
    apostrophe: true
    pad_with_space: false
    g2p:
      _target_: nemo.collections.tts.g2p.models.i18n_ipa.IpaG2p
      phoneme_dict: ${phoneme_dict_path}
      heteronyms: ${heteronyms_path}
      phoneme_probability: 0.8
      ignore_ambiguous_words: false
      use_chars: true
      use_stresses: true

  test_ds:
    dataset:
      _target_: nemo.collections.tts.data.text_to_speech_dataset.T5TTSDataset
      dataset_meta: ${test_ds_meta}
      sample_rate: ${sample_rate}
      min_duration: 0.5
      max_duration: 20.0
      # speaker_path: ${speaker_path}

    dataloader_params:
      batch_size: ${batch_size}
      num_workers: 2

  t5_encoder:
    is_causal: false
    pos_emb:
      name: "learnable_v2"
      base: 10000
    use_flash_self_attention: false
    use_flash_x_attention: false
    deterministic: false
    d_model: 768
    n_layers: 6
    p_dropout: 0.1
    n_heads: 12
    has_xattn: false
    apply_norm_to_cond: true
    kernel_size: 3
    layer_scale_init: 0.1
    layer_scale_decay: 0.95
    max_length_causal_mask: 2048 # Determinse max size of positional embeddings
    context_xattn: null

  context_encoder: # Only used for multi_encoder_context_tts, ignored otherwise
    is_causal: false
    pos_emb:
      name: "learnable_v2"
      base: 10000
    use_flash_self_attention: false
    use_flash_x_attention: false
    deterministic: false
    d_model: 768
    n_layers: 6
    p_dropout: 0.1
    n_heads: 12
    has_xattn: false
    apply_norm_to_cond: true
    kernel_size: 3
    layer_scale_init: 0.1
    layer_scale_decay: 0.95
    max_length_causal_mask: 2048 # Determinse max size of positional embeddings
    context_xattn: null

  t5_decoder:
    is_causal: true
    pos_emb:
      name: "learnable_v2"
      base: 10000
    use_flash_self_attention: false
    use_flash_x_attention: false
    deterministic: false
    d_model: 768
    n_layers: 12
    p_dropout: 0.1
    n_heads: 12
    has_xattn: true
    apply_norm_to_cond: true
    kernel_size: 3
    layer_scale_init: 0.1
    layer_scale_decay: 0.95
    max_length_causal_mask: 2048 # Determinse max size of positional embeddings
    context_xattn:
        n_heads: 12
        d_heads: 768
        pos_emb:
          name: ""

  optim:
    _target_: torch.optim.Adam
    lr: 2e-4
    betas: [0.8, 0.99]

    sched:
      name: ExponentialLR
      gamma: 0.998

trainer:
  num_nodes: 1
  devices: -1
  accelerator: gpu
  strategy: ddp_find_unused_parameters_true
  precision: 32
  max_epochs: ${max_epochs}
  accumulate_grad_batches: 1
  enable_checkpointing: False # Provided by exp_manager
  logger: false # Provided by exp_manager
  log_every_n_steps: 100
  val_check_interval: 500
  # check_val_every_n_epoch: 10
  benchmark: false

exp_manager:
  exp_dir: null
  name: ${name}
  create_tensorboard_logger: true
  create_wandb_logger: false
  wandb_logger_kwargs:
    name: null
    project: null
  create_checkpoint_callback: true 
  checkpoint_callback_params:
    monitor: val_loss
    mode: min
    save_top_k: 5
    save_best_model: true
    always_save_nemo: true
  resume_if_exists: true
  resume_ignore_no_checkpoint: true
