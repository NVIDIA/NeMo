inference:
  greedy: false # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 0  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 0.9 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 1.0 # sampling temperature
  add_BOS: true # add the bos token at the begining of the prompt
  tokens_to_generate: 30 # The minimum length of the sequence to be generated.
  all_probs: false  # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.2  # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 0  # The minimum length of the sequence to be generated.
  compute_logprob: false  # a flag used to compute logprob of all the input text, a very special case of running inference, default False
  batch_size: 64 # batch size for inference
  max_context_length: 512 # max length of the context, input sequence will be truncated if it is longer than this

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: false # logger provided by exp_manager
  precision: ${export.dtype} # 16, bf16, or 32
  enable_checkpointing: false

model:
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  restore_from_path: llama2-7b-fp16.nemo # Nemo file path
  precision: ${export.dtype} # Model weights data type
  megatron_amp_O2: true # Enable Megatron O2-style half-precision

  ## Activation Checkpoint
  activations_checkpoint_granularity: null # 'selective' or 'full'
  activations_checkpoint_method: null # 'uniform', 'block', not used with 'selective'

quantization:
  decoder_type: ${export.decoder_type} # gpt, llama
  algorithm: fp8 # null, int8_sq, fp8, int4_awq
  calib_dataset: cnn_dailymail # wikitext, cnn_dailymail, or a local dataset
  num_calib_size: 512 # number of samples used for calibration
  awq_block_size: 128 # block size for scaling factors (only used in AWQ algorithms)
  sq_alpha: 1.0 # alpha parameter (only used in SmoothQuant algorithms)
  enable_kv_cache: null # Enable FP8 KV cache quantization. Set to null for automatic selection.

export:
  decoder_type: llama # gpt, llama
  inference_tensor_parallel: 1 # Default using 1 TP for inference
  inference_pipeline_parallel: 1 # Default using 1 PP for inference
  dtype: 16 # Default precision data type for non-quantized layers: 16 or bf16
  save_path: llama2-7b-${quantization.algorithm}.qnemo # Path where the quantized model will be saved
  compress: false # Whether save_path should be a tarball or a directory
  sample_output: true # Whether to run a sample prompt before saving
