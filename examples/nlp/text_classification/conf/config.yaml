defaults:
  - trainer: pl
  - model: text_classification_with_bert

trainer:
  fast_dev_run: true

# Sentence classification with pretrained BERT models
# model:
#   data_dir: ??? # /path/to/data
#   class_balancing: null # or weighted_loss

#   train_ds:
#     prefix: train
#     batch_size: 32
#     shuffle: true
#     num_samples: -1
#     num_workers: 2
#     use_cache: true
#     drop_last: false
#     pin_memory: false

#   validation_ds:
#     prefix: dev
#     batch_size: 32
#     shuffle: false
#     num_samples: -1
#     num_workers: 2
#     use_cache: true
#     drop_last: false
#     pin_memory: false

#   language_model:
#     pretrained_model_name: roberta-base
#     max_seq_length: 36
#     bert_checkpoint: null
#     bert_config: null
#     tokenizer: null # only used if using custom checkpoint
#     tokenizer_model: null # only used if tokenizer is sentencepiece
#     do_lower_case: true # true for uncased models, false for cased models
  
#   head:
#     num_output_layers: 2
#     fc_dropout: 0.1

#   optim:
#     name: adam
#     lr: 2e-5
#     args:
#       name: auto
#       params:
#         weight_decay: 0.01

#     sched:
#       name: WarmupAnnealing
#       iters_per_batch: null # computed at runtime
#       max_steps: null # computed at runtime or explicitly set here

#       # pytorch lightning args
#       monitor: val_loss
#       reduce_on_plateau: false

#       # scheduler config override
#       args:
#         name: auto
#         params:
#           warmup_steps: null
#           warmup_ratio: 0.1
#           last_epoch: -1

#   pl: null # used at runtime
