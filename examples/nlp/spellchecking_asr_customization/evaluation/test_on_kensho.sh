#!/bin/bash

# Copyright (c) 2023, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

NEMO_PATH=/home/aleksandraa/nemo

## Spellchecking model in nemo format, that you get after training. See run_training.sh or run_training_tarred.sh  
PRETRAINED_MODEL=training.nemo

## These two files are generated by dataset_preparation/get_ngram_mappings.sh 
NGRAM_MAPPINGS=replacement_vocab_filt.txt
SUB_MISSPELLS=sub_misspells.txt

## File with IDF (inverse document frequencies) for words and short phrases.
## It is generated during dataset_preparation/build_training_data.sh
IDF=idf.txt

DATA_DIR="/home/aleksandraa/data/kensho"

## To get the dataset, you need to fill the form at
##     https://datasets.kensho.com/datasets/spgispeech
## You will get an email with download links, from which you need only validation part
##    https://datasets.kensho.com/api/v1/download/val.tar.bz2?key=... -o val.tar.bz2
##    https://datasets.kensho.com/api/v1/download/val.csv.bz2?key=... -o val.csv.bz2

## At this moment your ${DATA_DIR} folder structure should look like this:
##  data_folder_example
##   ├── spgispeech
##   │   └── val
##   |      ├── 0018ad922e541b415ae60e175160b976
##   |      |    ├── 118.wav
##   |      |    ├── 120.wav
##   |      |    ├── 148.wav
##   |      |    ├── 21.wav
##   |      |    ├── 34.wav
##   |      |    ├── 3.wav
##   |      |    ├── 72.wav
##   |      |    └── 92.wav
##   |      ├── ...
##   |      └── 002d12258ff802d65f79ae2eef99e4ab
##   └── val.csv

## val.csv contains paths to audio and transcripts
##   wav_filename|wav_filesize|transcript
##   13aa6c0669adb5544a0d62beef677189/12.wav|333164|Daniel, how do we think about the importance of those assets in the total remedy package? I mean, if you look at the last book value of those -- of the remedy assets and then in payments you've taken,
##   13aa6c0669adb5544a0d62beef677189/22.wav|323564|it should be clear that the intention is that the investments in Uttam Galva will be a part of the joint venture and funded by the joint venture.
##   13aa6c0669adb5544a0d62beef677189/75.wav|198764|next year. If you think back to our Q2 results, we did announce at that point that

## Extract text trascripts
awk 'BEGIN {FS="|"} (NR > 1){print $3}' < ${DATA_DIR}/val.csv > ${DATA_DIR}/text.txt

## Convert digits to words etc, but keep original case and punctuation
python ${NEMO_PATH}/nemo_text_processing/text_normalization/normalize.py \
  --input_file ${DATA_DIR}/text.txt \
  --output_file ${DATA_DIR}/norm.txt

## Make manifest file in NeMo format and imitate custom vocabularies.
mkdir ${DATA_DIR}/vocabs
mkdir ${DATA_DIR}/manifests
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/preprocess_kensho_and_create_vocabs.py \
  --input_folder ${DATA_DIR}/spgispeech/val \
  --destination_folder ${DATA_DIR}/vocabs \
  --transcription_file ${DATA_DIR}/val.csv \
  --normalized_file ${DATA_DIR}/norm.txt \
  --output_manifest ${DATA_DIR}/manifests/manifest.json \
  --idf_file ${IDF} \
  --min_idf_uppercase 5.0 \
  --min_idf_lowercase 8.0 \
  --min_len 6

## Transcribe data by NeMo model Conformer-CTC Large
python ${NEMO_PATH}/examples/asr/transcribe_speech.py \
  pretrained_name="stt_en_conformer_ctc_large" \
  dataset_manifest=${DATA_DIR}/manifests/manifest.json \
  output_filename=${DATA_DIR}/manifests/manifest_ctc.json \
  batch_size=16

## Transcribe data by NeMo model Conformer-Transducer Large 
python ${NEMO_PATH}/examples/asr/transcribe_speech.py \
  pretrained_name="stt_en_conformer_transducer_large" \
  dataset_manifest=${DATA_DIR}/manifests/manifest.json \
  output_filename=${DATA_DIR}/manifests/manifest_transducer.json \
  batch_size=16

## Remove all occurences of "um" and "uh" from transcriptions, because they are absent in the reference text.
python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/remove_uh_um_from_manifest.py \
  --input_manifest ${DATA_DIR}/manifests/manifest_ctc.json \
  --output_manifest ${DATA_DIR}/manifests/manifest_ctc_without_uh_um.json

python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/remove_uh_um_from_manifest.py \
  --input_manifest ${DATA_DIR}/manifests/manifest_transducer.json \
  --output_manifest ${DATA_DIR}/manifests/manifest_transducer_without_uh_um.json

## Get CER of baseline CTC model
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${DATA_DIR}/manifests/manifest_ctc_without_uh_um.json \
  use_cer=True \
  only_score_manifest=True

## Get WER of baseline CTC model
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${DATA_DIR}/manifests/manifest_ctc_without_uh_um.json \
  use_cer=False \
  only_score_manifest=True

## Get CER of baseline Transducer model
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${DATA_DIR}/manifests/manifest_transducer_without_uh_um.json \
  use_cer=True \
  only_score_manifest=True

## Get WER of baseline Transducer model
python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
  dataset_manifest=${DATA_DIR}/manifests/manifest_transducer_without_uh_um.json \
  use_cer=False \
  only_score_manifest=True


for ASRTYPE in "ctc_without_uh_um" "transducer_without_uh_um"
do

    ## Split ASR output transcriptions into shorter fragments to serve as ASR hypotheses for spellchecking model
    mkdir ${DATA_DIR}/hypotheses_${ASRTYPE}
    python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/extract_asr_hypotheses.py \
      --manifest ${DATA_DIR}/manifests/manifest_${ASRTYPE}.json \
      --folder ${DATA_DIR}/hypotheses_${ASRTYPE}

    ## Prepare inputs for inference of neural customization spellchecking model
    mkdir ${DATA_DIR}/spellchecker_input_${ASRTYPE}
    mkdir ${DATA_DIR}/spellchecker_output_${ASRTYPE}
    python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/prepare_input_for_spellchecker_inference.py \
      --hypotheses_folder ${DATA_DIR}/hypotheses_${ASRTYPE} \
      --vocabs_folder ${DATA_DIR}/vocabs \
      --output_folder ${DATA_DIR}/spellchecker_input_${ASRTYPE} \
      --ngram_mappings ${NGRAM_MAPPINGS} \
      --sub_misspells_file ${SUB_MISSPELLS}

    ## Create filelist with input filenames
    find ${DATA_DIR}/spellchecker_input_${ASRTYPE}/*.txt | grep -v "info.txt" > ${DATA_DIR}/filelist.txt

    ## Run inference with neural customization spellchecking model
    python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/spellchecking_asr_customization_infer.py \
      pretrained_model=${PRETRAINED_MODEL} \
      model.max_sequence_len=512 \
      +inference.from_filelist=${DATA_DIR}/filelist.txt \
      +inference.output_folder=${DATA_DIR}/spellchecker_output_${ASRTYPE} \
      inference.batch_size=16 \
      lang=en

    ## Postprocess and combine spellchecker results into a single manifest
    python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/update_transcription_with_spellchecker_results.py \
      --asr_hypotheses_folder ${DATA_DIR}/hypotheses_${ASRTYPE} \
      --spellchecker_inputs_folder ${DATA_DIR}/spellchecker_input_${ASRTYPE} \
      --spellchecker_results_folder ${DATA_DIR}/spellchecker_output_${ASRTYPE} \
      --input_manifest ${DATA_DIR}/manifests/manifest_${ASRTYPE}.json \
      --output_manifest ${DATA_DIR}/manifests/manifest_${ASRTYPE}_corrected.json \
      --min_cov 0.4 \
      --min_real_cov 0.8 \
      --min_dp_score_per_symbol -1.5 \
      --ngram_mappings ${NGRAM_MAPPINGS}

    ## Check CER of spellchecker results
    python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
      dataset_manifest=${DATA_DIR}/manifests/manifest_${ASRTYPE}_corrected.json \
      use_cer=True \
      only_score_manifest=True

    ## Check WER of spellchecker results
    python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
      dataset_manifest=${DATA_DIR}/manifests/manifest_${ASRTYPE}_corrected.json \
      use_cer=False \
      only_score_manifest=True

    ## Perform error analysis and create "ideal" spellchecker results for comparison
    python ${NEMO_PATH}/examples/nlp/spellchecking_asr_customization/evaluation/analyze_custom_ref_vs_asr.py \
      --manifest ${DATA_DIR}/manifests/manifest_${ASRTYPE}_corrected.json \
      --vocab_dir ${DATA_DIR}/vocabs \
      --input_dir ${DATA_DIR}/spellchecker_input_${ASRTYPE} \
      --ngram_mappings ${NGRAM_MAPPINGS} \
      --output_name ${DATA_DIR}/${ASRTYPE}_analysis_ref_vs_asr.txt

    ## Check CER of "ideal" spellcheck results
    python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
      dataset_manifest=${DATA_DIR}/${ASRTYPE}_analysis_ref_vs_asr.txt.ideal_spellcheck \
      use_cer=True \
      only_score_manifest=True

    ## Check WER of "ideal" spellcheck results
    python ${NEMO_PATH}/examples/asr/speech_to_text_eval.py \
      dataset_manifest=${DATA_DIR}/${ASRTYPE}_analysis_ref_vs_asr.txt.ideal_spellcheck \
      use_cer=False \
      only_score_manifest=True
done
