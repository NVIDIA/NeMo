defaults:
  - ../../language_modeling/conf@model.encoder: megatron_model_base_config
  - ../../language_modeling/conf@model.decoder: megatron_model_base_config

name: megatron_nmt
restore_from_path: null # used when starting from a .nemo file

trainer:
  devices: 1
  num_nodes: 1
  precision: 16 # Should be set to 16 for O1 and O2, default is 16 as PT ignores it when am_level is O0
  accelerator: gpu
  logger: False # logger provided by exp_manager
  enable_checkpointing: False
  use_distributed_sampler: False
  max_epochs: 1000 # PTL default. In practice, max_steps will be reached first. 
  max_steps: 400000 # consumed_samples = global_step * micro_batch_size * data_parallel_size * accumulate_grad_batches
  log_every_n_steps: 10
  val_check_interval: 1000
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0

exp_manager:
  explicit_log_dir: null
  exp_dir: null
  name: ${name}
  create_wandb_logger: False
  wandb_logger_kwargs:
    project: null
    name: null
  resume_if_exists: True
  resume_ignore_no_checkpoint: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    monitor: val_loss
    save_top_k: 10
    mode: min
    always_save_nemo: False # saves nemo file during validation, not implemented for model parallel
    filename: '${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}'
    model_parallel_size: ${multiply:${model.tensor_model_parallel_size}, ${model.pipeline_model_parallel_size}}

model:
  # NMT Params
  multilingual: False
  label_smoothing: 0.1 # TODO: Implement this.
  shared_tokenizer: True # train tokenizer model across src and tgt train data
  preproc_out_dir: null # path to store data preprocessing outputs
  src_language: 'en'
  tgt_language: 'de'
  max_generation_delta: 20 # Maximum decoder sequence length is encoder sequence length + this parameter.
  pretrained_model_path: null # Path to a pretrained model 
  pretrained_model_type: T5

  # model parallelism 
  micro_batch_size: 32
  global_batch_size: 512 # will use more micro batches to reach global batch size
  tensor_model_parallel_size: 1
  pipeline_model_parallel_size: 1
  pipeline_model_parallel_split_rank: 0 # rank at which decoder starts.
  resume_from_checkpoint: null # manually set the checkpoint file to load from

  # model architecture
  make_vocab_size_divisible_by: 128 # Pad the vocab size to be divisible by this value for computation efficiency.

  megatron_amp_O2: False # use AMP with O2 style mixed precision instead of native amp on-the-fly weight autocasting.
  grad_allreduce_chunk_size_mb: 125
  grad_div_ar_fusion: True # Fuse grad division into torch.distributed.all_reduce
  gradient_as_bucket_view: True # Allocate gradients in a contiguous bucket to save memory (less fragmentation and buffer memory)

  seq_length: 512
  max_position_embeddings: ${.seq_length}

  # weight init
  embedding_init_method_std: 0.02 # Standard deviation of the zero mean normal distribution used for weight initialization.')

  # embedding dropout
  embedding_dropout: 0.1

  # embedding sharing
  share_token_embeddings: True # If True share encoder/decoder embeddings
  share_decoder_tokens_head_embeddings: True # If True share decoder embeddings and decoder projection to logits

  # token head
  tokens_head_bias: True

  # precision
  native_amp_init_scale: 4294967296 # 2 ** 32
  native_amp_growth_interval: 1000
  fp16_lm_cross_entropy: False # Move the cross entropy unreduced loss calculation for lm head to fp16

  # miscellaneous
  seed: 1234
  use_cpu_initialization: False # Init weights on the CPU (slow for large models)
  apex_transformer_log_level: 30 # Python logging level displays logs with severity greater than or equal to this

  train_ds:
    src_file_name: null
    tgt_file_name: null
    dataset_type: 'text_memmap' # Options ['bin_memmap', 'text_memmap']
    sampler: 'megatron' # Options ['megatron']. Note megatron samplers do not shuffle across epochs.
    objective: 'nmt' # Options ['nmt', 'nmt-xlm']
    # NOTE: These ratios are used only when the objective is `nmt-xlm`
    sampling_ratios:
      x-masking: 0.17 # Extreme span masking task selection probability (either large spans or large masking prob)
      r-masking: 0.17 # T5-style random span masking task selection probability
      s-masking: 0.16 # Prefix-LM task selection probability
      nmt: 0.5 # NMT selection probability
    micro_batch_size: ${model.micro_batch_size}
    global_batch_size: ${model.global_batch_size}
    # config for preprocessing training data and creating a tarred datset automatically
    max_seq_length: 512
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8
    concat_sampling_probabilities: null # only used with ConcatTranslationDataset 

  validation_ds:
    src_file_name: ???
    tgt_file_name: ???
    dataset_type: 'text' # Options: ['text']. Validation data needs to be raw text.
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8

  test_ds:
    src_file_name: ???
    tgt_file_name: ???
    dataset_type: 'text' # Options: ['text']. Validation data needs to be raw text.
    tokens_in_batch: 512
    clean: false
    max_seq_length: 512
    shuffle: false
    num_samples: -1
    drop_last: false
    pin_memory: false
    num_workers: 8

  optim:
    name: fused_adam
    lr: 0.0004
    betas:
      - 0.9
      - 0.98
    weight_decay: 0.0
    sched:
      name: InverseSquareRootAnnealing
      min_lr: 0.0
      last_epoch: -1
      warmup_ratio: 0.1

  encoder_tokenizer:
    library: yttm
    model: null
    vocab_size: null # vocab size for training bpe
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null # valid for sentencepiece tokenizer
    r2l: false
    sentencepiece_legacy: True # Legacy=True allows you to add special tokens to sentencepiece tokenizers.
    num_sentinel_tokens: 0

  decoder_tokenizer:
    library: yttm
    model: null
    vocab_size: null # vocab size for training bpe
    bpe_dropout: null
    vocab_file: null
    special_tokens: null
    training_sample_size: null # valid for sentencepiece tokenizer
    r2l: false
    sentencepiece_legacy: True
    num_sentinel_tokens: 0
