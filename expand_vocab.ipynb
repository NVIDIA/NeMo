{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66c33286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE! Installing ujson may make loading annotations faster.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2023-08-07 23:31:58 experimental:27] Module <class 'nemo.collections.nlp.models.text_normalization_as_tagging.thutmose_tagger.ThutmoseTaggerModel'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2023-08-07 23:31:58 nemo_logging:349] /usr/local/lib/python3.8/dist-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "      warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "    \n",
      "[NeMo W 2023-08-07 23:31:58 experimental:27] Module <class 'nemo.collections.asr.modules.audio_modules.SpectrogramToMultichannelFeatures'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning.trainer.trainer import Trainer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from omegaconf import OmegaConf, open_dict\n",
    "\n",
    "from nemo.collections.nlp.models.language_modeling.megatron_base_model import MegatronBaseModel\n",
    "from nemo.collections.nlp.modules.common.tokenizer_utils import get_nmt_tokenizer\n",
    "from nemo.collections.nlp.parts.nlp_overrides import NLPSaveRestoreConnector\n",
    "from nemo.utils import AppState, model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21649661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba05b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "connector = NLPSaveRestoreConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5111db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nemo_file = \"path to 1TP nemo file\"\n",
    "nemo_file = \"/data/megatron_3b_1TP/megatron_t5_expanded_vocab_posemb.nemo\"\n",
    "# nemo_file = \"path to 1TP output nemo file to be saved after expansion\" If this is none then out_file = nemo_file + \"_expanded_vocab.nemo\"\n",
    "out_file = None #  \"/data/megatron_3b_1TP/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba309aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand_vocab(key, state_dict, increase_by=9984):\n",
    "    original_shape = state_dict[key].shape\n",
    "    shape_len = len(original_shape)\n",
    "    print(\"Original shape :\", original_shape, len(original_shape))\n",
    "\n",
    "    # Use vocab size as final expansion dim, we want the vocab to be divisible by 128\n",
    "    new_vocab_size = original_shape[0] + increase_by\n",
    "    print(\"New vocab size :\", new_vocab_size)\n",
    "\n",
    "    # Add buffer of dummy tokens for divisibility of vocab tokens\n",
    "    divisible_by_val = 1 # model_cfg.get('make_vocab_size_divisible_by', 1)\n",
    "    if new_vocab_size % divisible_by_val != 0:\n",
    "        dummy_tokens = divisible_by_val - (new_vocab_size % divisible_by_val)\n",
    "        final_vocab_size = new_vocab_size + dummy_tokens\n",
    "        print(\"Adding Dummy Tokens :\", dummy_tokens)\n",
    "    else:\n",
    "        final_vocab_size = new_vocab_size\n",
    "\n",
    "    # Final expanded shape\n",
    "    if shape_len > 1:\n",
    "        final_vocab_shape = [final_vocab_size, original_shape[1]]\n",
    "    else:\n",
    "        final_vocab_shape = [final_vocab_size]\n",
    "    new_shape = torch.Size(final_vocab_shape)\n",
    "\n",
    "    # Expand vocab\n",
    "    new_output_layer = torch.zeros(new_shape, dtype=state_dict[key].dtype)\n",
    "    if shape_len > 1:\n",
    "        new_output_layer[: original_shape[0], :] = state_dict[key]\n",
    "    else:\n",
    "        new_output_layer[: original_shape[0]] = state_dict[key]\n",
    "    print(new_output_layer.size())\n",
    "\n",
    "    # Update new tokens\n",
    "    if shape_len > 1:\n",
    "        new_output_layer[original_shape[0]:, :] = 1e-6  # small constant init is sufficient for new tokens\n",
    "    else:\n",
    "        new_output_layer[original_shape[0]:] = 1e-6\n",
    "\n",
    "    # Update dummy tokens\n",
    "    if shape_len > 1:\n",
    "        new_output_layer[new_vocab_size:, :] = 0.0\n",
    "    else:\n",
    "        new_output_layer[new_vocab_size:] = 0.0\n",
    "#     print(new_output_layer)\n",
    "\n",
    "    state_dict[key] = new_output_layer\n",
    "    print(f\"FINAL state_dict[{key}].shape {state_dict[key].shape}\")\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbb5c5eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "Parsing checkpoint at location:  /tmp/tmp18edgfls/model_weights.ckpt\n",
      "Before enc_dec_model.encoder_embedding.word_embeddings.weight --> torch.Size([39168, 1024])\n",
      "After enc_dec_model.encoder_embedding.word_embeddings.weight --> torch.Size([39168, 1024])\n",
      "Before enc_dec_model.decoder_embedding.word_embeddings.weight --> torch.Size([39168, 1024])\n",
      "After enc_dec_model.decoder_embedding.word_embeddings.weight --> torch.Size([39168, 1024])\n",
      "Before enc_dec_model.tokens_head.bias --> torch.Size([39168])\n",
      "After enc_dec_model.tokens_head.bias --> torch.Size([39168])\n",
      "Before enc_dec_model.encoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n",
      "After enc_dec_model.encoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n",
      "Before enc_dec_model.decoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n",
      "After enc_dec_model.decoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n",
      "Saving state dict ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "keys = [\n",
    "    'enc_dec_model.encoder_embedding.word_embeddings.weight',\n",
    "    'enc_dec_model.decoder_embedding.word_embeddings.weight',\n",
    "    'enc_dec_model.tokens_head.bias',\n",
    "    'enc_dec_model.encoder_embedding.position_embeddings.weight',\n",
    "    'enc_dec_model.decoder_embedding.position_embeddings.weight',\n",
    "]\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "        # Extract the model from the checkpoint\n",
    "        connector._unpack_nemo_file(nemo_file, tmpdir)\n",
    "\n",
    "        # Load the model config\n",
    "        config_path = os.path.join(tmpdir, connector.model_config_yaml)\n",
    "        config = OmegaConf.load(config_path)\n",
    "\n",
    "        tp_size = config.get('tensor_model_parallel_size', 1)\n",
    "        pp_size = config.get('pipeline_model_parallel_size', 1)\n",
    "\n",
    "        if tp_size > 1:\n",
    "            raise RuntimeError(\n",
    "                \"GPT model's vocab and output embeddings cannot be expanded for tensor parallelism > 1.\\n\"\n",
    "                \"Please use the `megatron_change_num_partitions.py` script to change the number of partitions.\\n\"\n",
    "                \"Then run this script to expand the vocabulary and output embeddings.\\n\"\n",
    "                \"Finally run the `megatron_change_num_partitions.py` script again to restore the number of partitions.\"\n",
    "            )\n",
    "\n",
    "        appstate = AppState()\n",
    "        appstate.tensor_model_parallel_rank = 1\n",
    "        appstate.pipeline_model_parallel_rank = 1\n",
    "        appstate.tensor_model_parallel_size = tp_size\n",
    "        appstate.pipeline_model_parallel_size = pp_size\n",
    "        appstate.model_parallel_size = tp_size * pp_size\n",
    "\n",
    "        input_embedding_size = None\n",
    "        input_embed_processed = False\n",
    "\n",
    "        output_layer_size = None\n",
    "        output_layer_processed = False\n",
    "\n",
    "        # Load the TP 1 PP Y model checkpoint\n",
    "\n",
    "        # Only need to update 2 PP - PP 0 for the input embedding and PP -1 for the output layer.\n",
    "        pp_checks = [0]\n",
    "        if pp_size > 1:\n",
    "            pp_checks = [pp_size - 1]\n",
    "        print(pp_checks)\n",
    "        \n",
    "        for pp in pp_checks:\n",
    "            for tp in range(1):  # tp size\n",
    "                appstate.tensor_model_parallel_rank = tp\n",
    "                appstate.pipeline_model_parallel_rank = pp\n",
    "\n",
    "                checkpoint_path = os.path.join(tmpdir, connector.model_weights_ckpt)\n",
    "                checkpoint_path = model_utils.inject_model_parallel_rank(checkpoint_path)\n",
    "\n",
    "                print(\"Parsing checkpoint at location: \", checkpoint_path)\n",
    "                state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "                \n",
    "                \n",
    "                for k in keys:\n",
    "                    print(f\"Before {k} --> {state_dict[k].shape}\")\n",
    "#                     if \"position\" in k:\n",
    "#                         state_dict = expand_vocab(k, state_dict, 1536)\n",
    "#                     else:\n",
    "#                         state_dict = expand_vocab(k, state_dict)\n",
    "                    print(f\"After {k} --> {state_dict[k].shape}\")\n",
    "                    \n",
    "                print(\"Saving state dict ...\")\n",
    "#                 torch.save(state_dict, checkpoint_path)\n",
    "\n",
    "#         # Save the full nemo file\n",
    "#         save_filepath = out_file\n",
    "#         if save_filepath is None:\n",
    "#             save_filepath = os.path.splitext(nemo_file)[0] + '_expanded_vocab_posemb.nemo'\n",
    "#             print(save_filepath)\n",
    "\n",
    "#         connector._make_nemo_file_from_folder(save_filepath, tmpdir)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c6eb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39168, 1024]) torch.Size([2048, 1024]) torch.Size([39168, 1024]) torch.Size([2048, 1024]) torch.Size([39168])\n"
     ]
    }
   ],
   "source": [
    "print(state_dict['enc_dec_model.encoder_embedding.word_embeddings.weight'].size(),\n",
    "state_dict['enc_dec_model.encoder_embedding.position_embeddings.weight'].size(),\n",
    "state_dict['enc_dec_model.decoder_embedding.word_embeddings.weight'].size(),\n",
    "state_dict['enc_dec_model.decoder_embedding.position_embeddings.weight'].size(),\n",
    "state_dict['enc_dec_model.tokens_head.bias'].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffd9dc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_dec_model.encoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n",
      "enc_dec_model.decoder_embedding.position_embeddings.weight --> torch.Size([2048, 1024])\n"
     ]
    }
   ],
   "source": [
    "for k in list(state_dict.keys()):\n",
    "    if \"position\" in k:\n",
    "        print(f\"{k} --> {state_dict[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fb1fb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enc_dec_model.encoder_embedding.word_embeddings.weight --> torch.Size([29184, 768])\n",
      "enc_dec_model.encoder_embedding.position_embeddings.weight --> torch.Size([512, 768])\n"
     ]
    }
   ],
   "source": [
    "for k in list(state_dict.keys()):\n",
    "    if \"encoder_embedding\" in k:\n",
    "        print(f\"{k} --> {state_dict[k].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe471770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
